{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation and Bias-Variance decomposition\n",
    "## Cross-Validation\n",
    "Implementing 4-fold cross-validation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import load_data\n",
    "\n",
    "# load dataset\n",
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    '''\n",
    "    Build k indices for k-fold.\n",
    "    '''\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    '''\n",
    "    Standardize the original data set.\n",
    "    '''\n",
    "    mean_x = np.mean(x, axis = 0)\n",
    "    x = x - mean_x\n",
    "    stddev_x = np.std(x, axis = 0)\n",
    "    x = x / stddev_x\n",
    "    return x, mean_x, stddev_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_k_indices(y, x, k_indices, k):\n",
    "    '''\n",
    "    Splits the data into test and training data.\n",
    "    Samples get randomized through 'k_indices' and\n",
    "    selected through 'k'.\n",
    "    '''\n",
    "    mask = np.ones(k_indices.shape, dtype=bool)\n",
    "    mask[k] = False\n",
    "    \n",
    "    # Test data\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    # Train data\n",
    "    x_train = x[k_indices[mask]]\n",
    "    y_train = y[k_indices[mask]]\n",
    "    \n",
    "    return x_test, y_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing splitting method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "['x3' 'x4']\n",
      "['y3' 'y4']\n",
      "\n",
      "Train data:\n",
      "['x1' 'x2' 'x5' 'x6']\n",
      "['y1' 'y2' 'y5' 'y6']\n"
     ]
    }
   ],
   "source": [
    "def test_split_data_k_indices():\n",
    "    # Example data for x and y\n",
    "    example_x = np.array(['x1', 'x2', 'x3', 'x4', 'x5', 'x6'])\n",
    "    example_y = np.array(['y1', 'y2', 'y3', 'y4', 'y5', 'y6'])\n",
    "\n",
    "    # 'example_k_indices' remained sorted for output clarity\n",
    "    example_k_indices = np.array([[0,1], [2,3], [4,5]])\n",
    "\n",
    "    # With 'example_k >= 0' AND 'example_k < example_k_indices.shape[0]'\n",
    "    example_k = 1\n",
    "\n",
    "    # Split data output\n",
    "    example_x_test, example_y_test, example_x_train, example_y_train = split_data_k_indices(example_y, example_x, example_k_indices, example_k)\n",
    "\n",
    "    # Debug prints\n",
    "    print(\"Test data:\")\n",
    "    print(example_x_test)\n",
    "    print(example_y_test)\n",
    "    print(\"\\nTrain data:\")\n",
    "    print(example_x_train)\n",
    "    print(example_y_train)\n",
    "    \n",
    "test_split_data_k_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import compute_mse\n",
    "from ridge_regression import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "    \n",
    "    # Standardizing testing and training data.\n",
    "    # Done independently for 'k_fold' value validation (a good value\n",
    "    # corresponds to lower difference between the mean and stddev \n",
    "    # of both sets).\n",
    "    x_test, x_test_mean, x_test_stddev = standardize(x_test)\n",
    "    x_train, x_train_mean, x_train_stddev = standardize(x_train)\n",
    "    \n",
    "    # Form data with polynomial degree\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    \n",
    "    print(tx_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print(tx_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = ridge_regression(y, tx_train, lambda_)\n",
    "    rmse_test = np.sqrt(compute_mse(y, tx_test, w_opt))\n",
    "    \n",
    "    # Return loss for train and test data\n",
    "    return rmse_train, rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 8)\n",
      "(12,)\n",
      "(36, 8)\n",
      "(36,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (8,36) and (50,) not aligned: 36 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6d2a82a30506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mcross_validation_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-6d2a82a30506>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# them (for a gicen lambda).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mrmse_cur_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_cur_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mrmse_cur_dif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse_cur_tr\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrmse_cur_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrmse_cur_dif\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrmse_min_dif\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c15953d432d5>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Apply ridge regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mw_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mrmse_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\User\\Documents\\Notebooks\\ml2017-proj\\ex04\\template\\ridge_regression.py\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(y, tx, lambda_)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0minv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mw_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (8,36) and (50,) not aligned: 36 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "\n",
    "    # K-fold cross validation\n",
    "    for k in range(0, k_fold):\n",
    "        \n",
    "        rmse_min_dif = float(\"inf\")\n",
    "        rmse_min_dif_tr = 0\n",
    "        rmse_min_dif_te = 0\n",
    "        \n",
    "        # Apply ridge regression and pick the rmse 'test' and 'train'\n",
    "        # errors that represent the least absolute difference between\n",
    "        # them (for a given lambda).\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "            rmse_cur_tr, rmse_cur_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            rmse_cur_dif = abs(rmse_cur_tr - rmse_cur_te)\n",
    "            if (rmse_cur_dif < rmse_min_dif):\n",
    "                rmse_min_dif = rmse_cur_dif\n",
    "                rmse_min_dif_tr = rmse_cur_tr\n",
    "                rmse_min_dif_te = rmse_cur_te\n",
    "                \n",
    "        rmse_tr.append(rmse_min_dif_tr)\n",
    "        rmse_te.append(rmse_min_dif_te)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "Visualize bias-variance trade-off by implementing the function `bias_variance_demo()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from least_squares import least_squares\n",
    "from split_data import split_data\n",
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        x = np.linspace(0.1, 2 * np.pi, num_data)\n",
    "        y = np.sin(x) + 0.3 * np.random.randn(num_data).T\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # split data with a specific seed: TODO\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # bias_variance_decomposition: TODO\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "bias_variance_demo()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
