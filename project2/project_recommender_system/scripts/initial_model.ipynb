{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General ideas:\n",
    "-----------------------------\n",
    "* Baselines\n",
    "    * Simpler ones\n",
    "        * Global mean :)\n",
    "        * User mean   :)\n",
    "        * Item mean   :)\n",
    "    * Aditional\n",
    "        * User rating depends on number of ratings\n",
    "        * User rating depends on overall rating for the item\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "From here on, use regularization! Better Ridge, maybe Lasso.\n",
    "\n",
    "* Neighborhood models\n",
    "    * Find sets of similar users\n",
    "    * Find sets of similar items\n",
    "    * Correlation/Cosine similarity suggested in post (one for users, one for items)\n",
    "* Matrix factorization. For sparse matrices. Non-negative elements. Missing elements are not the same as elements equal to 0. (!!!)\n",
    "    * Standard SVD\n",
    "    * Asymmetric SVD\n",
    "    * SVD++\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Put all together...\n",
    "* Ensemble methods\n",
    "    * Linear regression\n",
    "    * Gradient boosted decision trees - can apply different methods to different slices of data! We can cluster by: (!!!)\n",
    "        * Number of items rated\n",
    "        * Number of users that rated the item\n",
    "        * Factor vectors of users and items (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 1682, number of users: 943\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "#ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))\n",
    "ratings = load_data('{dp}movielens100k.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f720d421518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 20, min # of users per item = 1.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in original data:99999\n",
      "Total number of nonzero elements in train data:88157\n",
      "Total number of nonzero elements in test data:9795\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex10 functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    train_mean = train.sum() / train.count_nonzero()\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test.data, train_mean)\n",
    "    test_rmse = np.sqrt(test_mse / len(test.data))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using global mean: {e}\".format(e=test_rmse))\n",
    "\n",
    "\n",
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the user mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'user')\n",
    "\n",
    "    \n",
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the item mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'item')\n",
    "\n",
    "    \n",
    "def baseline_user_item_mean(train, test, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "    num = train.shape[flag]\n",
    "    \n",
    "    # Compute means using training data\n",
    "    train_ = sp.find(train)\n",
    "    counts = np.bincount(train_[flag], minlength=num)\n",
    "    sums = np.bincount(train_[flag], weights=train_[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    # Do predictions\n",
    "    test_ = sp.find(test)\n",
    "    pred_test = test_[2].copy()\n",
    "    pred_test = 1.0 * pred_test\n",
    "    for x in range(num):\n",
    "        ys = test_[inv_flag][test_[flag]==x]\n",
    "        for y in ys:\n",
    "            pred_test[(test_[flag]==x) & (test_[inv_flag]==y)] = means[x]\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test_[2], pred_test)\n",
    "    test_rmse = np.sqrt(test_mse / len(test_[2]))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using {m} mean: {e}\".format(m=mean, e=test_rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "baseline_global_mean(train, test)\n",
    "baseline_user_mean(train, test)\n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    data_mean = data.sum() / data.count_nonzero()\n",
    "    return data_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = data.shape[flag]\n",
    "    \n",
    "    # Obtain data_deviations, which are the ratings minus global avg\n",
    "    global_mean = baseline_rating(data)\n",
    "    data_deviations = data.copy()\n",
    "    data_deviations.data -= global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in data_deviations\n",
    "    data_rcv = sp.find(data_deviations)\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from helpers import create_csv_submission\n",
    "\n",
    "global_mean = baseline_rating(ratings)\n",
    "user_means = baseline_user_item_specific(ratings, 'user')\n",
    "item_means = baseline_user_item_specific(ratings, 'item')\n",
    "\n",
    "sample_submission = np.genfromtxt('{dp}sample_submission.csv'.format(dp=DATA_PATH), delimiter=\",\", skip_header=1, dtype=str)\n",
    "ids = sample_submission[:,0]\n",
    "y_pred = np.full(len(ids), global_mean)\n",
    "\n",
    "create_csv_submission(ids, y_pred, '{pp}global_mean.csv'.format{pp=PREDICTION_PATH}) # Achieves 1.11785 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global, user and item means (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the RMSE for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_baseline(data, test_data, test_flag, sub_flag=False,\n",
    "    sub_filename=\"new_submission\"):\n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "\n",
    "    print('----- Finished computing means -----')  \n",
    "    \n",
    "    (rows, cols, vals) = sp.find(test_data)\n",
    "    \n",
    "    if test_flag:        \n",
    "        # Do predictions\n",
    "        pred_test = vals.copy()\n",
    "        pred_test = 1.0 * pred_test\n",
    "\n",
    "        for (i, u) in zip(rows, cols):\n",
    "            pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "            pred_test[(rows==i) & (cols==u)] = pred_i_u\n",
    "\n",
    "        print('----- Finished predicting -----')\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(vals))\n",
    "        print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse)) \n",
    "        return pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            print('----- Starting to write file -----')\n",
    "            for (i, u) in zip(rows, cols):\n",
    "                pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_test = train_model_baseline(train, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the submission file training on all data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()\n",
    "\n",
    "train_model_baseline(ratings_csr, sample_submission_csr, False, True, \"baselines\")\n",
    "# Achieves 1.00386 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_matrix_factorization(data, test_data,\n",
    "    test_flag, sub_flag=False, sub_filename=\"new_submission\"):\n",
    "    \n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    # assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    # Compute global, user and item means \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    print('----- Finished computing means -----')  \n",
    "    \n",
    "    # Substract the baseline of each element in data\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "\n",
    "    for (i, u) in zip(rows, cols):\n",
    "        baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "        train_vals[(rows==i) & (cols==u)] += (5 - baseline_i_u)\n",
    "    \n",
    "    print('----- Finished substracting baseline from train data -----')\n",
    "    \n",
    "    # Substract the baseline of each element in test_data\n",
    "    (test_rows, test_cols, ts_vals) = sp.find(test_data)\n",
    "    test_vals = ts_vals.copy()\n",
    "    test_vals = 1.0 * test_vals\n",
    "\n",
    "    for (i, u) in zip(test_rows, test_cols):\n",
    "        baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "        test_vals[(test_rows==i) & (test_cols==u)] += (5 - baseline_i_u)\n",
    "        \n",
    "    print('----- Finished substracting baseline from test data -----')\n",
    "\n",
    "    # Get matrix\n",
    "    train_matrix = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "\n",
    "    return train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Finished computing means -----\n",
      "----- Finished substracting baseline from train data -----\n",
      "----- Finished substracting baseline from test data -----\n"
     ]
    }
   ],
   "source": [
    "train_matrix = train_model_matrix_factorization(train, test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(alpha=0, l1_ratio=0, shuffle=False)\n",
    "W = nmf_model.fit_transform(train_matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "W_csr = sp.csr_matrix(W)\n",
    "H_csr = sp.csr_matrix(H)\n",
    "\n",
    "(test_rows, test_cols, test_vals) = sp.find(test)\n",
    "\n",
    "# Do predictions\n",
    "pred_test = test_vals.copy()\n",
    "pred_test = 1.0 * pred_test\n",
    "\n",
    "pred_matrix_factor = W_csr.dot(H_csr)\n",
    "\n",
    "\n",
    "# Compute global, user and item means \n",
    "global_mean = baseline_rating(train)\n",
    "user_means = baseline_user_item_specific(train, 'user')\n",
    "item_means = baseline_user_item_specific(train, 'item')\n",
    "print('----- Finished computing means -----')\n",
    "\n",
    "\n",
    "for (i, u) in zip(test_rows, test_cols):\n",
    "    pred_i_u = pred_matrix_factor[i,u]\n",
    "    baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "    pred_test[(test_rows==i) & (test_cols==u)] = pred_i_u + baseline_i_u\n",
    "\n",
    "print('----- Finished predicting -----')\n",
    "\n",
    "# Compute and print test error\n",
    "test_mse = calculate_mse(test_vals, pred_test)\n",
    "test_rmse = np.sqrt(test_mse / len(test_vals))\n",
    "print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse)) \n",
    "\n",
    "print(sp.find(test_vals))\n",
    "\n",
    "pred_matrix_factor = W_csr.dot(H_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model_2 = NMF(alpha=1, l1_ratio=0, shuffle=True)\n",
    "W_2 = nmf_model_2.fit_transform(train_matrix)\n",
    "H_2 = nmf_model_2.components_\n",
    "\n",
    "W_csr_2 = sp.csr_matrix(W_2)\n",
    "H_csr_2 = sp.csr_matrix(H_2)\n",
    "\n",
    "# Do predictions\n",
    "pred_test_2 = test_vals.copy()\n",
    "pred_test_2 = 1.0 * pred_test_2\n",
    "\n",
    "pred_matrix_factor_2 = W_csr_2.dot(H_csr_2)\n",
    "\n",
    "for (i, u) in zip(test_rows, test_cols):\n",
    "    pred_i_u = pred_matrix_factor_2[i,u]\n",
    "    baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "    pred_test_2[(test_rows==i) & (test_cols==u)] = pred_i_u + baseline_i_u\n",
    "\n",
    "print('----- Finished predicting -----')\n",
    "\n",
    "# Compute and print test error\n",
    "test_mse_2 = calculate_mse(test_vals, pred_test_2)\n",
    "test_rmse_2 = np.sqrt(test_mse / len(test_vals))\n",
    "print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse_2)) \n",
    "\n",
    "print(sp.find(test_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization for Ex10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codereview.stackexchange.com/questions/35727/optimize-scipy-sparse-matrix-factorization-code-for-sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = self.model.shape[0] #no of users\n",
    "M = self.model.shape[1] #no of items\n",
    "self.p = np.random.rand(N, K)\n",
    "self.q = np.random.rand(M, K)\n",
    "rows,cols = self.model.nonzero()        \n",
    "for step in xrange(steps):\n",
    "    for u, i in zip(rows,cols):\n",
    "        e = self.model[u, i] - np.dot(self.p[u, :], self.q[i, :]) #calculate error for gradient\n",
    "        p_temp = learning_rate * ( e * self.q[i,:] - regularization * self.p[u,:])\n",
    "        self.q[i,:]+= learning_rate * ( e * self.p[u,:] - regularization * self.q[i,:])\n",
    "        self.p[u,:] += p_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # you should return:\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "    # ***************************************************\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO\n",
    "        # do matrix factorization.\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "    # ***************************************************\n",
    "    # TODO\n",
    "    # evaluate the test error.\n",
    "    # ***************************************************\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    raise NotImplementedError\n",
    "\n",
    "matrix_factorization_SGD(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return user feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return item feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS(train, test):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # start you ALS-WR algorithm.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "ALS(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
