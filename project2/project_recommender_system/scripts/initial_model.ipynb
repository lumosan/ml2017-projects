{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General ideas:\n",
    "-----------------------------\n",
    "\n",
    "Basically, use http://surpriselib.com/\n",
    "\n",
    "* Baselines\n",
    "    * Simpler ones -> Need to try with `surprise.prediction_algorithms.baseline_only.BaselineOnly(bsl_options={})`\n",
    "        * Global mean :)\n",
    "        * User mean   :)\n",
    "        * Item mean   :)\n",
    "    * Aditional\n",
    "        * User rating depends on number of ratings\n",
    "        * User rating depends on overall rating for the item\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "From here on, use regularization! Better Ridge, maybe Lasso.\n",
    "\n",
    "* Neighborhood models (http://surprise.readthedocs.io/en/stable/knn_inspired.html)\n",
    "    * Find sets of similar users\n",
    "    * Find sets of similar items\n",
    "    * Correlation/Cosine similarity suggested in post (one for users, one for items)\n",
    "* Matrix factorization. For sparse matrices. Non-negative elements. Missing elements are not the same as elements equal to 0. (!!!)\n",
    "    * Standard SVD `surprise.prediction_algorithms.matrix_factorization.SVD`\n",
    "    * Asymmetric SVD\n",
    "    * SVD++ `surprise.prediction_algorithms.matrix_factorization.SVDpp`\n",
    "    * NMF (already implemented, but not properly?) `surprise.prediction_algorithms.matrix_factorization.NMF`\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Put all together...\n",
    "\n",
    "Ensemble methods (http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "* Linear regression\n",
    "* Gradient boosted decision trees - can apply different methods to different slices of data! We can cluster by: (!!!)\n",
    "    * Number of items rated\n",
    "    * Number of users that rated the item\n",
    "    * Factor vectors of users and items (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))\n",
    "#ratings = load_data('{dp}movielens100k.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQBljjIlJVqCMMcbEpP8PbC+qj1CYuYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f017c636630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1, verbose=False):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "        print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "        print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex10 functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    train_mean = train.sum() / train.count_nonzero()\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test.data, train_mean)\n",
    "    test_rmse = np.sqrt(test_mse / len(test.data))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using global mean: {e}\".format(e=test_rmse))\n",
    "\n",
    "\n",
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the user mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'user')\n",
    "\n",
    "    \n",
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the item mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'item')\n",
    "\n",
    "    \n",
    "def baseline_user_item_mean(train, test, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "    num = train.shape[flag]\n",
    "    \n",
    "    # Compute means using training data\n",
    "    train_ = sp.find(train)\n",
    "    counts = np.bincount(train_[flag], minlength=num)\n",
    "    sums = np.bincount(train_[flag], weights=train_[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    # Do predictions\n",
    "    test_ = sp.find(test)\n",
    "    pred_test = test_[2].copy()\n",
    "    pred_test = 1.0 * pred_test\n",
    "    for x in range(num):\n",
    "        ys = test_[inv_flag][test_[flag]==x]\n",
    "        for y in ys:\n",
    "            pred_test[(test_[flag]==x) & (test_[inv_flag]==y)] = means[x]\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test_[2], pred_test)\n",
    "    test_rmse = np.sqrt(test_mse / len(test_[2]))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using {m} mean: {e}\".format(m=mean, e=test_rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "baseline_global_mean(train, test)\n",
    "baseline_user_mean(train, test)\n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    data_mean = data.sum() / data.count_nonzero()\n",
    "    return data_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = data.shape[flag]\n",
    "    \n",
    "    # Obtain data_deviations, which are the ratings minus global avg\n",
    "    global_mean = baseline_rating(data)\n",
    "    data_deviations = data.copy()\n",
    "    data_deviations.data = 1.0 * data_deviations.data\n",
    "    data_deviations.data -= global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in data_deviations\n",
    "    data_rcv = sp.find(data_deviations)\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from helpers import create_csv_submission\n",
    "\n",
    "global_mean = baseline_rating(ratings)\n",
    "user_means = baseline_user_item_specific(ratings, 'user')\n",
    "item_means = baseline_user_item_specific(ratings, 'item')\n",
    "\n",
    "sample_submission = np.genfromtxt('{dp}sample_submission.csv'.format(dp=DATA_PATH), delimiter=\",\", skip_header=1, dtype=str)\n",
    "ids = sample_submission[:,0]\n",
    "y_pred = np.full(len(ids), global_mean)\n",
    "\n",
    "create_csv_submission(ids, y_pred, '{pp}global_mean.csv'.format{pp=PREDICTION_PATH}) # Achieves 1.11785 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global, user and item means (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the RMSE for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_baseline(data, test_data, test_flag, sub_flag=False,\n",
    "    sub_filename=\"new_submission\", verbose=False):\n",
    "\n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    \n",
    "    (rows, cols, vals) = sp.find(test_data)\n",
    "    \n",
    "    if test_flag:        \n",
    "        # Do predictions\n",
    "        pred_test = vals.copy()\n",
    "        pred_test = 1.0 * pred_test\n",
    "\n",
    "        for (i, u) in zip(rows, cols):\n",
    "            pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "            pred_test[(rows==i) & (cols==u)] = pred_i_u\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(vals))\n",
    "        if verbose:\n",
    "            print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse)) \n",
    "        return test_rmse, pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for (i, u) in zip(rows, cols):\n",
    "                pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of baseline using baseline: 1.0057078177840963\n"
     ]
    }
   ],
   "source": [
    "test_rmse, pred_test = train_model_baseline(train, test, True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the submission file training on all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()\n",
    "\n",
    "train_model_baseline(ratings_csr, sample_submission_csr, False, True, \"baselines_2\")\n",
    "# Achieves 1.00386 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbaseline_matrix(data, add_constant, verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Compute global, user and item means \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item') \n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "\n",
    "    for (i, u) in zip(rows, cols):\n",
    "        baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "        train_vals[(rows==i) & (cols==u)] += (- baseline_i_u)\n",
    "\n",
    "    # Get matrix\n",
    "    train_matrix = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "    \n",
    "    train_matrix = train_matrix.todense() + add_constant\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed unbaseline_matrix!       ')\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    return train_matrix, global_mean, user_means, item_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is stupid, keeping it for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_matrix_factorization(data, test_data, test_flag, sub_flag=False,\n",
    "    sub_filename=\"new_submission\", alpha=0, l1_ratio=0, shuffle=True, random_state=42,\n",
    "    verbose=False):\n",
    "\n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "\n",
    "    # Get matrix\n",
    "    add_constant = 5\n",
    "    train_matrix, global_mean, user_means, item_means = unbaseline_matrix(data, add_constant, verbose=verbose)\n",
    "    \n",
    "    # Create and fit model   \n",
    "    nmf_model = NMF(alpha=alpha, l1_ratio=l1_ratio, shuffle=shuffle, random_state=random_state)\n",
    "    W = nmf_model.fit_transform(train_matrix)\n",
    "    H = nmf_model.components_\n",
    "    W_csr = sp.csr_matrix(W)\n",
    "    H_csr = sp.csr_matrix(H)\n",
    "    prediction_matrix = W_csr.dot(H_csr)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Finished fitting model')\n",
    "\n",
    "    (test_rows, test_cols, test_vals) = sp.find(test)\n",
    "\n",
    "    if test_flag:        \n",
    "        # Do predictions\n",
    "        pred_test = test_vals.copy()\n",
    "        pred_test = 1.0 * pred_test\n",
    "        \n",
    "        for (i, u) in zip(test_rows, test_cols):\n",
    "            mat_fact_i_u = prediction_matrix[i,u]\n",
    "            baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "            pred_i_u = mat_fact_i_u + baseline_i_u - add_constant\n",
    "            \n",
    "            pred_test[(test_rows==i) & (test_cols==u)] = pred_i_u\n",
    "        \n",
    "        if verbose:\n",
    "            print('Finished predicting')\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(test_vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(vals))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Test RMSE using baseline and matrix factorization: {e}\".format(e=test_rmse)) \n",
    "            print()\n",
    "            print('---------------------------------------------')\n",
    "            print(' Completed train_model_matrix_factorization! ')    \n",
    "            print('---------------------------------------------')\n",
    "            \n",
    "        return test_rmse, pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for (i, u) in zip(test_rows, test_cols):  \n",
    "                pred_i_u = prediction_matrix[i,u]\n",
    "                baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "                pred_i_u = mat_fact_i_u + baseline_i_u - add_constant\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "                \n",
    "        if verbose:\n",
    "            print('---------------------------------------------')\n",
    "            print(' Completed train_model_matrix_factorization! ')    \n",
    "            print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple example to try new functions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 5]\n",
      " [0 0 0]\n",
      " [5 5 5]\n",
      " [0 0 0]]\n",
      "\n",
      "[[ 5.   4.2  6.2]\n",
      " [ 5.   5.   5. ]\n",
      " [ 4.2  6.2  4.2]\n",
      " [ 5.   5.   5. ]]\n",
      "\n",
      "\n",
      "[[ 0.03641587  1.51648764  1.74237048]\n",
      " [ 0.96930122  1.0870609   1.41196685]\n",
      " [ 2.14769667  0.85292491  0.57534788]\n",
      " [ 0.96875635  1.08721456  1.41241006]]\n",
      "[[ 1.09664948  1.96432528  0.46772394]\n",
      " [ 0.58789681  1.75436479  3.27679007]\n",
      " [ 2.33525906  0.84236006  0.69682997]]\n",
      "\n",
      "[[  3.60139476e-04  -8.00291573e-01   1.20038018e+00]\n",
      " [ -6.28316951e-04   5.08731207e-04  -6.63440377e-04]\n",
      " [ -7.99711403e-01   1.19976635e+00  -7.99695346e-01]\n",
      " [ -1.00513528e-04   8.13373699e-05  -1.05940336e-04]]\n"
     ]
    }
   ],
   "source": [
    "nrows, ncols = (4,3)\n",
    "a=np.random.randint(0,7,(nrows,ncols))\n",
    "a[a>5]=0\n",
    "a=sp.csr_matrix(a)\n",
    "print(a.todense())\n",
    "print()\n",
    "a_unbiased, gm, um, im = unbaseline_matrix(a, 5)\n",
    "print(a_unbiased)\n",
    "print()\n",
    "\n",
    "nmf_model = NMF(alpha=0, l1_ratio=0, shuffle=True, random_state=1)\n",
    "W = nmf_model.fit_transform(a_unbiased)\n",
    "H = nmf_model.components_\n",
    "\n",
    "prediction_matrix=W.dot(H)\n",
    "print()\n",
    "print(W)\n",
    "print(H)\n",
    "\n",
    "pred_sol = np.full((nrows,ncols),1.0)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for u in range(ncols):\n",
    "        mat_fact_i_u = prediction_matrix[i,u]\n",
    "        baseline_i_u = gm + um[u] + im[i]\n",
    "        pred_i_u = mat_fact_i_u - 5 #+ baseline_i_u\n",
    "\n",
    "        pred_sol[i,u] = pred_i_u\n",
    "        #print('i{} u{} mf{} bl{} p{}'.format(i,u,mat_fact_i_u,baseline_i_u,pred_i_u))\n",
    "\n",
    "print()\n",
    "print(pred_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train some real models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(no models to train, sadly...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization for Ex10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some guy wanted to optimize running time, this is his code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codereview.stackexchange.com/questions/35727/optimize-scipy-sparse-matrix-factorization-code-for-sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = self.model.shape[0] #no of users\n",
    "M = self.model.shape[1] #no of items\n",
    "self.p = np.random.rand(N, K)\n",
    "self.q = np.random.rand(M, K)\n",
    "rows,cols = self.model.nonzero()        \n",
    "for step in xrange(steps):\n",
    "    for u, i in zip(rows,cols):\n",
    "        e = self.model[u, i] - np.dot(self.p[u, :], self.q[i, :]) #calculate error for gradient\n",
    "        p_temp = learning_rate * ( e * self.q[i,:] - regularization * self.p[u,:])\n",
    "        self.q[i,:]+= learning_rate * ( e * self.p[u,:] - regularization * self.q[i,:])\n",
    "        self.p[u,:] += p_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # you should return:\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "    # ***************************************************\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO\n",
    "        # do matrix factorization.\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "    # ***************************************************\n",
    "    # TODO\n",
    "    # evaluate the test error.\n",
    "    # ***************************************************\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    raise NotImplementedError\n",
    "\n",
    "matrix_factorization_SGD(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return user feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return item feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS(train, test):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # start you ALS-WR algorithm.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "ALS(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
