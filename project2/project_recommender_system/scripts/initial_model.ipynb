{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General ideas:\n",
    "-----------------------------\n",
    "* Baselines\n",
    "    * Simpler ones\n",
    "        * Global mean :)\n",
    "        * User mean   :)\n",
    "        * Item mean   :)\n",
    "    * Aditional\n",
    "        * User rating depends on number of ratings\n",
    "        * User rating depends on overall rating for the item\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "From here on, use regularization! Better Ridge, maybe Lasso.\n",
    "\n",
    "* Neighborhood models\n",
    "    * Find sets of similar users\n",
    "    * Find sets of similar items\n",
    "    * Correlation/Cosine similarity suggested in post (one for users, one for items)\n",
    "* Matrix factorization. For sparse matrices. Non-negative elements. Missing elements are not the same as elements equal to 0. (!!!)\n",
    "    * Standard SVD\n",
    "    * Asymmetric SVD\n",
    "    * SVD++\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Put all together...\n",
    "* Ensemble methods\n",
    "    * Linear regression\n",
    "    * Gradient boosted decision trees - can apply different methods to different slices of data! We can cluster by: (!!!)\n",
    "        * Number of items rated\n",
    "        * Number of users that rated the item\n",
    "        * Factor vectors of users and items (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 1682, number of users: 943\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "#ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))\n",
    "ratings = load_data('{dp}movielens100k.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa97001a4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 20, min # of users per item = 1.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in original data:99999\n",
      "Total number of nonzero elements in train data:88157\n",
      "Total number of nonzero elements in test data:9795\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex10 functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    train_mean = train.sum() / train.count_nonzero()\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test.data, train_mean)\n",
    "    test_rmse = np.sqrt(test_mse / len(test.data))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using global mean: {e}\".format(e=test_rmse))\n",
    "\n",
    "\n",
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the user mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'user')\n",
    "\n",
    "    \n",
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"Implements baseline method using the item mean.\"\"\"\n",
    "    baseline_user_item_mean(train, test, 'item')\n",
    "\n",
    "    \n",
    "def baseline_user_item_mean(train, test, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "    num = train.shape[flag]\n",
    "    \n",
    "    # Compute means using training data\n",
    "    train_ = sp.find(train)\n",
    "    counts = np.bincount(train_[flag], minlength=num)\n",
    "    sums = np.bincount(train_[flag], weights=train_[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    # Do predictions\n",
    "    test_ = sp.find(test)\n",
    "    pred_test = test_[2].copy()\n",
    "    pred_test = 1.0 * pred_test\n",
    "    for x in range(num):\n",
    "        ys = test_[inv_flag][test_[flag]==x]\n",
    "        for y in ys:\n",
    "            pred_test[(test_[flag]==x) & (test_[inv_flag]==y)] = means[x]\n",
    "    \n",
    "    # Compute test error\n",
    "    test_mse = calculate_mse(test_[2], pred_test)\n",
    "    test_rmse = np.sqrt(test_mse / len(test_[2]))\n",
    "    \n",
    "    print(\"Test RMSE of baseline using {m} mean: {e}\".format(m=mean, e=test_rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "baseline_global_mean(train, test)\n",
    "baseline_user_mean(train, test)\n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method using the global mean.\"\"\"\n",
    "    # Compute global mean using training data\n",
    "    data_mean = data.sum() / data.count_nonzero()\n",
    "    return data_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean):\n",
    "    \"\"\"Implements baseline method using either the user\n",
    "    or the item mean, as indicated in parameter mean.\"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = data.shape[flag]\n",
    "    \n",
    "    # Obtain data_deviations, which are the ratings minus global avg\n",
    "    global_mean = baseline_rating(data)\n",
    "    data_deviations = data.copy()\n",
    "    data_deviations.data -= global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in data_deviations\n",
    "    data_rcv = sp.find(data_deviations)\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from helpers import create_csv_submission\n",
    "\n",
    "global_mean = baseline_rating(ratings)\n",
    "user_means = baseline_user_item_specific(ratings, 'user')\n",
    "item_means = baseline_user_item_specific(ratings, 'item')\n",
    "\n",
    "sample_submission = np.genfromtxt('{dp}sample_submission.csv'.format(dp=DATA_PATH), delimiter=\",\", skip_header=1, dtype=str)\n",
    "ids = sample_submission[:,0]\n",
    "y_pred = np.full(len(ids), global_mean)\n",
    "\n",
    "create_csv_submission(ids, y_pred, '{pp}global_mean.csv'.format{pp=PREDICTION_PATH}) # Achieves 1.11785 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using global, user and item means (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the RMSE for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_baseline(data, test_data, test_flag, sub_flag=False,\n",
    "    sub_filename=\"new_submission\"):\n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "\n",
    "    print('----- Finished computing means -----')  \n",
    "    \n",
    "    (rows, cols, vals) = sp.find(test_data)\n",
    "    \n",
    "    if test_flag:        \n",
    "        # Do predictions\n",
    "        pred_test = vals.copy()\n",
    "        pred_test = 1.0 * pred_test\n",
    "\n",
    "        for (i, u) in zip(rows, cols):\n",
    "            pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "            pred_test[(rows==i) & (cols==u)] = pred_i_u\n",
    "\n",
    "        print('----- Finished predicting -----')\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(vals))\n",
    "        print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse)) \n",
    "        return pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            print('----- Starting to write file -----')\n",
    "            for (i, u) in zip(rows, cols):\n",
    "                pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_test = train_model_baseline(train, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the submission file training on all data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()\n",
    "\n",
    "train_model_baseline(ratings_csr, sample_submission_csr, False, True, \"baselines\")\n",
    "# Achieves 1.00386 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_matrix_factorization(data, test_data,\n",
    "    test_flag, sub_flag=False, sub_filename=\"new_submission\"):\n",
    "    \n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    # assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    # Compute global, user and item means \n",
    "    global_mean = baseline_rating(data)\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    print('----- Finished computing means -----')  \n",
    "    \n",
    "    # Substract the baseline of each element in data\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "\n",
    "    for (i, u) in zip(rows, cols):\n",
    "        baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "        train_vals[(rows==i) & (cols==u)] += (5 - baseline_i_u)\n",
    "    \n",
    "    print('----- Finished substracting baseline from train data -----')\n",
    "    \n",
    "    # Substract the baseline of each element in test_data\n",
    "    (test_rows, test_cols, ts_vals) = sp.find(test_data)\n",
    "    test_vals = ts_vals.copy()\n",
    "    test_vals = 1.0 * test_vals\n",
    "\n",
    "    for (i, u) in zip(test_rows, test_cols):\n",
    "        baseline_i_u = global_mean + user_means[u] + item_means[i]\n",
    "        test_vals[(test_rows==i) & (test_cols==u)] += (5 - baseline_i_u)\n",
    "        \n",
    "    print('----- Finished substracting baseline from test data -----')\n",
    "\n",
    "    # Get matrix\n",
    "    train_matrix = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "\n",
    "    return train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Finished computing means -----\n",
      "----- Finished substracting baseline from train data -----\n",
      "----- Finished substracting baseline from test data -----\n"
     ]
    }
   ],
   "source": [
    "train_matrix = train_model_matrix_factorization(train, test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4733e577558b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnmf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                                                random_state=random_state)\n\u001b[0m\u001b[1;32m   1024\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         W, H, n_iter = _fit_multiplicative_update(X, W, H, beta_loss, max_iter,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_fit_coordinate_descent\u001b[0;34m(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Update W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         violation += _update_coordinate_descent(X, W, Ht, l1_reg_W,\n\u001b[0;32m--> 485\u001b[0;31m                                                 l2_reg_W, shuffle, rng)\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;31m# Update H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_update_coordinate_descent\u001b[0;34m(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# The following seems to be required on 64-bit Windows w/ Python 3.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_update_cdnmf_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHHt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXHt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(alpha=0, l1_ratio=0, shuffle=False)\n",
    "W = nmf_model.fit_transform(train_matrix)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization for Ex10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codereview.stackexchange.com/questions/35727/optimize-scipy-sparse-matrix-factorization-code-for-sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = self.model.shape[0] #no of users\n",
    "M = self.model.shape[1] #no of items\n",
    "self.p = np.random.rand(N, K)\n",
    "self.q = np.random.rand(M, K)\n",
    "rows,cols = self.model.nonzero()        \n",
    "for step in xrange(steps):\n",
    "    for u, i in zip(rows,cols):\n",
    "        e = self.model[u, i] - np.dot(self.p[u, :], self.q[i, :]) #calculate error for gradient\n",
    "        p_temp = learning_rate * ( e * self.q[i,:] - regularization * self.p[u,:])\n",
    "        self.q[i,:]+= learning_rate * ( e * self.p[u,:] - regularization * self.q[i,:])\n",
    "        self.p[u,:] += p_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # you should return:\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "    # ***************************************************\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO\n",
    "        # do matrix factorization.\n",
    "        # ***************************************************\n",
    "        raise NotImplementedError\n",
    "\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "    # ***************************************************\n",
    "    # TODO\n",
    "    # evaluate the test error.\n",
    "    # ***************************************************\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    raise NotImplementedError\n",
    "\n",
    "matrix_factorization_SGD(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return user feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # update and return item feature.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS(train, test):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 20   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # start you ALS-WR algorithm.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "ALS(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
