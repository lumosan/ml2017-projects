{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))\n",
    "#ratings = load_data('{dp}movielens100k.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f78812390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1, verbose=False):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "        print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "        print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in original data:1176952\n",
      "Total number of nonzero elements in train data:1059186\n",
      "Total number of nonzero elements in test data:117687\n"
     ]
    }
   ],
   "source": [
    "valid_ratings, train, test = split_data(ratings, num_items_per_user,\n",
    "    num_users_per_item, min_num_ratings=10, p_test=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read submission creation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using the global mean.\n",
    "    \"\"\"\n",
    "    # Compute global mean using training data\n",
    "    r_mean = data.sum() / data.getnnz()\n",
    "    return r_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean, set_num=0):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using either the user or the item mean,\n",
    "    as indicated in parameter mean.\n",
    "    \"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = max(set_num, data.shape[flag])\n",
    "    \n",
    "    # Obtain r_demeaned (ratings minus global avg)\n",
    "    global_mean = baseline_rating(data)\n",
    "    r_demeaned = data.copy()\n",
    "    r_demeaned.data = (1.0 * r_demeaned.data) - global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in r_demeaned\n",
    "    data_rcv = sp.find(r_demeaned)\n",
    "    # compute means\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_matrix(data, verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Compute global, user and item means    \n",
    "    global_mean = baseline_rating(data)\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals = train_vals - baselines\n",
    "    \n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    return r_demeaned, global_mean, user_means, item_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_test_matrix(data, global_mean, item_means, user_means,\n",
    "    verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_items, num_users = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals -= baselines\n",
    "\n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_items, num_users))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    return r_demeaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(data, k):\n",
    "    \"\"\"Initializes parameters for Matrix Factorization.\n",
    "    Assumes 'data' matrix is already demeaned.\n",
    "    \"\"\"      \n",
    "    np.random.seed(988)\n",
    "    num_items, num_users = data.shape\n",
    "    u_features = np.random.rand(k, num_users)\n",
    "    i_features = np.random.rand(k, num_items)\n",
    "    return u_features, i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, u_features, i_features, nz):\n",
    "    \"\"\"Compute RMSE for prediction of nonzero elements.\"\"\"\n",
    "    preds = np.array([(u_features[:,u].dot(i_features[:,i]))\n",
    "        for (i, u) in nz])\n",
    "    vals = np.array([data[i,u] for (i,u) in nz])\n",
    "    mse = calculate_mse(vals, preds)  \n",
    "    rmse = np.sqrt(mse / len(vals))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_features(train, i_features, lambda_u,\n",
    "    n_i_per_user, nz_i_per_user):\n",
    "    \"\"\"Updates user feature matrix.\"\"\"\n",
    "    n_u = len(nz_i_per_user)\n",
    "    k = i_features.shape[0]\n",
    "    lambda_u_I = lambda_u * sp.eye(k)\n",
    "    new_u_features = np.zeros((k, n_u))\n",
    "    for u, i in nz_i_per_user:\n",
    "        M = i_features[:,i]\n",
    "        V = train[i,u].T.dot(M.T)\n",
    "        A = M.dot(M.T) + n_i_per_user[u] * lambda_u_I\n",
    "        new_u_features[:,u] = np.linalg.solve(A, V.T).T\n",
    "    return new_u_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_item_features(train, u_features, lambda_i,\n",
    "    n_u_per_item, nz_u_per_item):\n",
    "    \"\"\"Updates item feature matrix.\"\"\"\n",
    "    n_i = len(nz_u_per_item)\n",
    "    k = u_features.shape[0]\n",
    "    lambda_i_I = lambda_i * sp.eye(k)\n",
    "    new_i_features = np.zeros((k, n_i))\n",
    "    for i, u in nz_u_per_item:\n",
    "        M = u_features[:,u]\n",
    "        V = train[i,u].dot(M.T)\n",
    "        A = M.dot(M.T) + n_u_per_item[i] * lambda_i_I\n",
    "        new_i_features[:,i] = np.linalg.solve(A, V.T).T\n",
    "    return new_i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_ALS(data_demeaned, test_demeaned, \n",
    "    k=20, lambda_u=.1, lambda_i=.7, tol=1e-4, max_iter=50,\n",
    "    init_u_features=None, init_i_features=None):\n",
    "    \"\"\"Matrix factorization by ALS\"\"\"\n",
    "    assert k <= min(data_demeaned.shape), \"k must be smaller than the min dimension of 'data'\"\n",
    "    \n",
    "    # Get non-zero elements\n",
    "    (rows, cols, vals) = sp.find(data_demeaned)\n",
    "    (test_rows, test_cols, test_vals) = sp.find(test_demeaned)\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # Initialize feature vectors for users and items\n",
    "    rand_u_features, rand_i_features = init_MF(data_demeaned, k)\n",
    "    if init_u_features is None:\n",
    "        u_features = rand_u_features\n",
    "    else:\n",
    "        u_features = init_u_features\n",
    "\n",
    "    if init_i_features is None:\n",
    "        i_features = rand_i_features\n",
    "    else:\n",
    "        i_features = init_i_features\n",
    "\n",
    "    # Get number of non-zero ratings per user and item\n",
    "    n_i_per_user = data_demeaned.getnnz(axis=0)\n",
    "    n_u_per_item = data_demeaned.getnnz(axis=1)\n",
    "    \n",
    "    # Get non-zero ratings per user and item\n",
    "    nz_train, nz_u_per_item, nz_i_per_user = build_index_groups(data_demeaned)\n",
    "\n",
    "    e = 1000\n",
    "    \n",
    "    # ALS-WR algorithm\n",
    "    for it in range(max_iter):\n",
    "        u_features = update_user_features(data_demeaned, i_features, lambda_u,\n",
    "            n_i_per_user, nz_i_per_user)\n",
    "        i_features = update_item_features(data_demeaned, u_features, lambda_i,\n",
    "            n_u_per_item, nz_u_per_item)\n",
    "        # compute and print new training error\n",
    "        old_e = e\n",
    "        e = compute_error(data_demeaned, u_features, i_features, nz_train)\n",
    "        print(\"training RMSE: {}.\".format(e))\n",
    "        if(abs(old_e - e) < tol):\n",
    "            print('Finished estimating features')\n",
    "            break\n",
    "        if(old_e - e < -tol):\n",
    "            print('Whoops!')\n",
    "            break\n",
    "    # Do predictions        \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    interactions = np.array([u_features[:,u].dot(i_features[:,i].T)\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    pred_test = baselines + interactions\n",
    "\n",
    "    # Compute and print test error\n",
    "    nnz_row, nnz_col = test.nonzero()\n",
    "    nnz_test = list(zip(nnz_row, nnz_col))\n",
    "    rmse = compute_error(test_demeaned, u_features, i_features, nnz_test)\n",
    "    print(\"test RMSE after running ALS: {v}.\".format(v=rmse))\n",
    "\n",
    "    return u_features, i_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demean matrix and get feature vectors\n",
    "r_demeaned, global_mean, user_means, item_means = demean_matrix(train, verbose=False)\n",
    "test_r_demeaned = demean_test_matrix(test, global_mean, item_means,\n",
    "    user_means, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8777862258178835.\n",
      "training RMSE: 0.8609852732733557.\n",
      "training RMSE: 0.8494242541625361.\n",
      "training RMSE: 0.8414352305467109.\n",
      "training RMSE: 0.8355475624432326.\n",
      "training RMSE: 0.8309952836855966.\n",
      "training RMSE: 0.8273675851466326.\n",
      "training RMSE: 0.8244777626958406.\n",
      "training RMSE: 0.8221390154538166.\n",
      "training RMSE: 0.8202211401359562.\n",
      "training RMSE: 0.8185933833674747.\n",
      "training RMSE: 0.8172179375038617.\n",
      "training RMSE: 0.8160185127237288.\n",
      "training RMSE: 0.8149958845960243.\n",
      "training RMSE: 0.8140796074315819.\n",
      "training RMSE: 0.813272852105327.\n",
      "training RMSE: 0.8125509141310915.\n",
      "training RMSE: 0.8119033624186827.\n",
      "training RMSE: 0.811321271454323.\n",
      "training RMSE: 0.8107961328573489.\n",
      "training RMSE: 0.8103140591436243.\n",
      "training RMSE: 0.8098739035279681.\n",
      "training RMSE: 0.8094649221133756.\n",
      "training RMSE: 0.8090866257653035.\n",
      "training RMSE: 0.808732652574688.\n",
      "training RMSE: 0.808408715765256.\n",
      "training RMSE: 0.8081055632920267.\n",
      "training RMSE: 0.8078251981609506.\n",
      "training RMSE: 0.8075565231516673.\n",
      "training RMSE: 0.8072977871617737.\n",
      "training RMSE: 0.8070587278820082.\n",
      "training RMSE: 0.8068314783943424.\n",
      "training RMSE: 0.8066162223369489.\n",
      "training RMSE: 0.8064150784389239.\n",
      "training RMSE: 0.8062223535417181.\n",
      "training RMSE: 0.8060433275158122.\n",
      "training RMSE: 0.8058693172828874.\n",
      "training RMSE: 0.8057011453387364.\n",
      "training RMSE: 0.8055424157999247.\n",
      "training RMSE: 0.805391519064551.\n",
      "training RMSE: 0.8052495203311993.\n",
      "training RMSE: 0.8051116972052619.\n",
      "training RMSE: 0.8049793981301401.\n",
      "training RMSE: 0.8048495535414537.\n",
      "training RMSE: 0.804725951664903.\n",
      "training RMSE: 0.8046069922771268.\n",
      "training RMSE: 0.8044906316451488.\n",
      "training RMSE: 0.8043798653400459.\n",
      "training RMSE: 0.8042716449081555.\n",
      "training RMSE: 0.804168170723629.\n",
      "test RMSE after running ALS: 15.24487597856167.\n"
     ]
    }
   ],
   "source": [
    "u_features_0, i_features_0 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=0, lambda_i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8040678715167391.\n",
      "training RMSE: 0.8039719579781739.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 15.542746325402772.\n"
     ]
    }
   ],
   "source": [
    "u_features_1, i_features_1 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=0, lambda_i=0, init_u_features=u_features_0,\n",
    "    init_i_features=i_features_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9237322846626974.\n",
      "training RMSE: 0.8377009792170633.\n",
      "training RMSE: 0.8294620803704968.\n",
      "training RMSE: 0.8268916018781186.\n",
      "training RMSE: 0.8256684789034854.\n",
      "training RMSE: 0.8249008040169914.\n",
      "training RMSE: 0.8243289815749132.\n",
      "training RMSE: 0.8238632685956975.\n",
      "training RMSE: 0.8234666760808166.\n",
      "training RMSE: 0.8231206627287084.\n",
      "training RMSE: 0.8228140956003854.\n",
      "training RMSE: 0.8225393928546721.\n",
      "training RMSE: 0.8222910263880261.\n",
      "training RMSE: 0.8220648361798045.\n",
      "training RMSE: 0.82185764338269.\n",
      "training RMSE: 0.8216669915377129.\n",
      "training RMSE: 0.8214909578679074.\n",
      "training RMSE: 0.8213280120437985.\n",
      "training RMSE: 0.8211769105248491.\n",
      "training RMSE: 0.8210366180697286.\n",
      "training RMSE: 0.820906249788166.\n",
      "training RMSE: 0.8207850286920341.\n",
      "training RMSE: 0.8206722551758153.\n",
      "training RMSE: 0.8205672859272846.\n",
      "training RMSE: 0.8204695203388169.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 1.040108102927818.\n"
     ]
    }
   ],
   "source": [
    "u_features_003, i_features_003 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.03, lambda_i=.03, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9440056863643365.\n",
      "training RMSE: 0.8674954695546094.\n",
      "training RMSE: 0.8527756072753845.\n",
      "training RMSE: 0.8505934876576436.\n",
      "training RMSE: 0.8494880935771318.\n",
      "training RMSE: 0.848720736941489.\n",
      "training RMSE: 0.8481214730423818.\n",
      "training RMSE: 0.8476289975565953.\n",
      "training RMSE: 0.8472131159422965.\n",
      "training RMSE: 0.8468558521185766.\n",
      "training RMSE: 0.8465452059838438.\n",
      "training RMSE: 0.8462725930422873.\n",
      "training RMSE: 0.84603159727241.\n",
      "training RMSE: 0.8458172731311977.\n",
      "training RMSE: 0.8456257147323165.\n",
      "training RMSE: 0.8454537725199528.\n",
      "training RMSE: 0.8452988593418155.\n",
      "training RMSE: 0.8451588147997082.\n",
      "training RMSE: 0.8450318094823973.\n",
      "training RMSE: 0.8449162768709559.\n",
      "training RMSE: 0.8448108642218617.\n",
      "training RMSE: 0.8447143962935917.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 1.0080531665669978.\n"
     ]
    }
   ],
   "source": [
    "u_features_005, i_features_005 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.05, lambda_i=.05, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9641700419931245.\n",
      "training RMSE: 0.9099997469053797.\n",
      "training RMSE: 0.8887662061230855.\n",
      "training RMSE: 0.8818756783694464.\n",
      "training RMSE: 0.88007661343303.\n",
      "training RMSE: 0.8791076687083005.\n",
      "training RMSE: 0.8784200166669982.\n",
      "training RMSE: 0.8778861494602218.\n",
      "training RMSE: 0.8774535573502504.\n",
      "training RMSE: 0.8770941181734742.\n",
      "training RMSE: 0.8767904125917425.\n",
      "training RMSE: 0.8765305723990653.\n",
      "training RMSE: 0.8763059833829042.\n",
      "training RMSE: 0.876110130288836.\n",
      "training RMSE: 0.8759379524566021.\n",
      "training RMSE: 0.8757854497925661.\n",
      "training RMSE: 0.875649422955707.\n",
      "training RMSE: 0.8755272919783184.\n",
      "training RMSE: 0.8754169642133055.\n",
      "training RMSE: 0.8753167350773614.\n",
      "training RMSE: 0.8752252115252053.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9924255556022445.\n"
     ]
    }
   ],
   "source": [
    "u_features_005b, i_features_005b = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.05, lambda_i=.1, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9471055893681931.\n",
      "training RMSE: 0.9017959436917476.\n",
      "training RMSE: 0.8862266980295657.\n",
      "training RMSE: 0.8813930763595111.\n",
      "training RMSE: 0.8799239460953054.\n",
      "training RMSE: 0.8790337780995346.\n",
      "training RMSE: 0.8783794213359105.\n",
      "training RMSE: 0.8778634555244905.\n",
      "training RMSE: 0.8774416351894632.\n",
      "training RMSE: 0.8770891129131911.\n",
      "training RMSE: 0.8767900401426214.\n",
      "training RMSE: 0.8765333996259413.\n",
      "training RMSE: 0.8763110782001142.\n",
      "training RMSE: 0.8761168723659957.\n",
      "training RMSE: 0.8759459220944804.\n",
      "training RMSE: 0.8757943586665514.\n",
      "training RMSE: 0.8756590691103389.\n",
      "training RMSE: 0.8755375297766786.\n",
      "training RMSE: 0.8754276839723851.\n",
      "training RMSE: 0.8753278492149178.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9924443605327473.\n"
     ]
    }
   ],
   "source": [
    "u_features_005c, i_features_005c = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.05, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9600277350424331.\n",
      "training RMSE: 0.9222327630355288.\n",
      "training RMSE: 0.9048966995498046.\n",
      "training RMSE: 0.8975922966242754.\n",
      "training RMSE: 0.8952037310593314.\n",
      "training RMSE: 0.8941106937455304.\n",
      "training RMSE: 0.893386142533172.\n",
      "training RMSE: 0.8928393495253608.\n",
      "training RMSE: 0.8924038884398758.\n",
      "training RMSE: 0.8920466804840125.\n",
      "training RMSE: 0.8917479838686498.\n",
      "training RMSE: 0.8914946655055959.\n",
      "training RMSE: 0.8912773492931217.\n",
      "training RMSE: 0.891089029117208.\n",
      "training RMSE: 0.8909243170385202.\n",
      "training RMSE: 0.8907789945585042.\n",
      "training RMSE: 0.8906497215479667.\n",
      "training RMSE: 0.8905338345742426.\n",
      "training RMSE: 0.8904292002274201.\n",
      "training RMSE: 0.8903341046975557.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9891936059402625.\n"
     ]
    }
   ],
   "source": [
    "u_features_01_part, i_features_01_part = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.08, lambda_i=.08, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1, max_iter=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9664856383101701.\n",
      "training RMSE: 0.9518009394617436.\n",
      "training RMSE: 0.940508321560623.\n",
      "training RMSE: 0.9344077573489507.\n",
      "training RMSE: 0.9309489496193133.\n",
      "training RMSE: 0.9289890089113081.\n",
      "training RMSE: 0.9278356364443863.\n",
      "training RMSE: 0.9270917039056633.\n",
      "training RMSE: 0.9265632906094825.\n",
      "training RMSE: 0.9261607690961122.\n",
      "training RMSE: 0.9258402188232969.\n",
      "training RMSE: 0.925577535108613.\n",
      "training RMSE: 0.9253579018294267.\n",
      "training RMSE: 0.9251713785840238.\n",
      "training RMSE: 0.925010887685777.\n",
      "training RMSE: 0.9248711931764648.\n",
      "training RMSE: 0.9247483248361132.\n",
      "training RMSE: 0.9246392221248814.\n",
      "training RMSE: 0.9245414982261824.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.987794646682212.\n"
     ]
    }
   ],
   "source": [
    "u_features_01, i_features_01 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9685997756660341.\n",
      "training RMSE: 0.9579673096160713.\n",
      "training RMSE: 0.9483720642486728.\n",
      "training RMSE: 0.9429259898308623.\n",
      "training RMSE: 0.9397650530051787.\n",
      "training RMSE: 0.9378346905308949.\n",
      "training RMSE: 0.936598493760148.\n",
      "training RMSE: 0.9357690859666724.\n",
      "training RMSE: 0.9351813018252785.\n",
      "training RMSE: 0.9347416198590953.\n",
      "training RMSE: 0.9343980199865941.\n",
      "training RMSE: 0.9341207075531481.\n",
      "training RMSE: 0.9338915344316967.\n",
      "training RMSE: 0.9336986709744584.\n",
      "training RMSE: 0.9335339451350378.\n",
      "training RMSE: 0.933391459874835.\n",
      "training RMSE: 0.933266826567983.\n",
      "training RMSE: 0.933156707949649.\n",
      "training RMSE: 0.9330585267874412.\n",
      "training RMSE: 0.9329702698626089.\n",
      "training RMSE: 0.9328903505724515.\n",
      "training RMSE: 0.9328175096246134.\n",
      "training RMSE: 0.9327507415365298.\n",
      "training RMSE: 0.9326892391761074.\n",
      "training RMSE: 0.9326323512388698.\n",
      "training RMSE: 0.9325795492082329.\n",
      "training RMSE: 0.9325304014114097.\n",
      "training RMSE: 0.9324845524889898.\n",
      "training RMSE: 0.9324417070714576.\n",
      "training RMSE: 0.9324016167798059.\n",
      "training RMSE: 0.9323640698903436.\n",
      "training RMSE: 0.9323288831583354.\n",
      "training RMSE: 0.9322958954033499.\n",
      "training RMSE: 0.9322649625358569.\n",
      "training RMSE: 0.932235953760267.\n",
      "training RMSE: 0.9322087487313306.\n",
      "training RMSE: 0.9321832354737335.\n",
      "training RMSE: 0.9321593089021317.\n",
      "training RMSE: 0.9321368698028796.\n",
      "training RMSE: 0.9321158241604691.\n",
      "training RMSE: 0.9320960827317859.\n",
      "training RMSE: 0.9320775607897416.\n",
      "training RMSE: 0.9320601779745894.\n",
      "training RMSE: 0.9320438582060977.\n",
      "training RMSE: 0.9320285296225618.\n",
      "training RMSE: 0.9320141245233182.\n",
      "training RMSE: 0.9320005793000506.\n",
      "training RMSE: 0.9319878343488235.\n",
      "training RMSE: 0.931975833959728.\n",
      "training RMSE: 0.9319645261844143.\n",
      "test RMSE after running ALS: 0.988206637815687.\n"
     ]
    }
   ],
   "source": [
    "u_features_01, i_features_01 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.11, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9506675376578096.\n",
      "training RMSE: 0.9670304223697375.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9954312095537533.\n"
     ]
    }
   ],
   "source": [
    "u_features_02, i_features_02 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.2, init_u_features=u_features_01,\n",
    "    init_i_features=i_features_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9791089571509457.\n",
      "training RMSE: 0.9841546392393206.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 1.0026210568848657.\n"
     ]
    }
   ],
   "source": [
    "u_features_02, i_features_02 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.2, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1, tol=1e-6, max_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.764079693732386.\n",
      "training RMSE: 0.7487370440355664.\n",
      "training RMSE: 0.7367960202706672.\n",
      "training RMSE: 0.7277611007095072.\n",
      "training RMSE: 0.7208526966769482.\n",
      "training RMSE: 0.7154968417729765.\n",
      "training RMSE: 0.7111754166452892.\n",
      "training RMSE: 0.7076865431408613.\n",
      "training RMSE: 0.704699234689281.\n",
      "training RMSE: 0.7021060690878047.\n",
      "training RMSE: 0.6998590462945328.\n",
      "training RMSE: 0.6979209453377244.\n",
      "training RMSE: 0.6961410618500861.\n",
      "training RMSE: 0.694559626538089.\n",
      "training RMSE: 0.6930833050041255.\n",
      "training RMSE: 0.6917270187892332.\n",
      "training RMSE: 0.6904966214487582.\n",
      "training RMSE: 0.6893671279734188.\n",
      "training RMSE: 0.6883076483311295.\n",
      "training RMSE: 0.6873107339225871.\n",
      "training RMSE: 0.6863843093995506.\n",
      "training RMSE: 0.685512344385891.\n",
      "training RMSE: 0.6846845872536762.\n",
      "training RMSE: 0.6838871751026376.\n",
      "training RMSE: 0.6831354320210377.\n",
      "training RMSE: 0.6824119004878529.\n",
      "training RMSE: 0.6817422747511901.\n",
      "training RMSE: 0.6810785577330692.\n",
      "training RMSE: 0.6804515713413287.\n",
      "training RMSE: 0.6798530103452062.\n",
      "training RMSE: 0.6792625150486901.\n",
      "training RMSE: 0.6787016912513951.\n",
      "training RMSE: 0.6781681661524621.\n",
      "training RMSE: 0.677644726329172.\n",
      "training RMSE: 0.6771517285760117.\n",
      "training RMSE: 0.6766766258435682.\n",
      "training RMSE: 0.6762093282518417.\n",
      "training RMSE: 0.6757644806524338.\n",
      "training RMSE: 0.6753337315342524.\n",
      "training RMSE: 0.6749172152175489.\n",
      "training RMSE: 0.6745148232000522.\n",
      "training RMSE: 0.6741222541431937.\n",
      "training RMSE: 0.6737423638336699.\n",
      "training RMSE: 0.6733838679244522.\n",
      "training RMSE: 0.6730263887715275.\n",
      "training RMSE: 0.6726856467858823.\n",
      "training RMSE: 0.6723495727601543.\n",
      "training RMSE: 0.672033669894543.\n",
      "training RMSE: 0.671710377795331.\n",
      "training RMSE: 0.6714037598579553.\n",
      "training RMSE: 0.6711021241869425.\n",
      "training RMSE: 0.6708066168972824.\n",
      "training RMSE: 0.6705200642916224.\n",
      "training RMSE: 0.6702460504328983.\n",
      "training RMSE: 0.6699792878991758.\n",
      "training RMSE: 0.6697136763948481.\n",
      "training RMSE: 0.6694542656526479.\n",
      "training RMSE: 0.6692074999289195.\n",
      "training RMSE: 0.6689611563957026.\n",
      "training RMSE: 0.6687265704965148.\n",
      "training RMSE: 0.6684936730405803.\n",
      "training RMSE: 0.6682644308287913.\n",
      "training RMSE: 0.6680466479979978.\n",
      "training RMSE: 0.6678294261719687.\n",
      "training RMSE: 0.6676100435534214.\n",
      "training RMSE: 0.6674043585680955.\n",
      "training RMSE: 0.6671974873418548.\n",
      "training RMSE: 0.6669910673242914.\n",
      "training RMSE: 0.6667872512441997.\n",
      "training RMSE: 0.6665891325166726.\n",
      "test RMSE after running ALS: 632.023283842641.\n"
     ]
    }
   ],
   "source": [
    "u_features_0_40, i_features_0_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=0, lambda_i=0, k=40, max_iter=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9502352027002542.\n",
      "training RMSE: 0.9442463019939583.\n",
      "training RMSE: 0.9317874269516543.\n",
      "training RMSE: 0.9240640706412115.\n",
      "training RMSE: 0.9191567410712959.\n",
      "training RMSE: 0.915690869857496.\n",
      "training RMSE: 0.9131276180328665.\n",
      "training RMSE: 0.9111635865378096.\n",
      "training RMSE: 0.909617128227158.\n",
      "training RMSE: 0.9083764596865558.\n",
      "training RMSE: 0.9073694332858585.\n",
      "training RMSE: 0.9065461682002757.\n",
      "training RMSE: 0.905869340049313.\n",
      "training RMSE: 0.9053093996256297.\n",
      "training RMSE: 0.9048425801177487.\n",
      "training RMSE: 0.9044499832850232.\n",
      "training RMSE: 0.904116844496979.\n",
      "training RMSE: 0.9038317577078886.\n",
      "training RMSE: 0.903585925110162.\n",
      "training RMSE: 0.9033725164600377.\n",
      "training RMSE: 0.9031861686892968.\n",
      "training RMSE: 0.9030226147108737.\n",
      "training RMSE: 0.9028784143793848.\n",
      "training RMSE: 0.9027507602676852.\n",
      "training RMSE: 0.9026373363786178.\n",
      "training RMSE: 0.9025362139785529.\n",
      "training RMSE: 0.9024457736207165.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.988198556651075.\n"
     ]
    }
   ],
   "source": [
    "u_features_01_40, i_features_01_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_0_40,\n",
    "    init_i_features=i_features_0_40, max_iter=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.902364645889504.\n",
      "training RMSE: 0.9022916657271381.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9881990898640572.\n"
     ]
    }
   ],
   "source": [
    "u_features_01_40, i_features_01_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9128528648114087.\n",
      "training RMSE: 0.9210937166399196.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9894435954115074.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.12, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9038697832853849.\n",
      "training RMSE: 0.9092718046081801.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9886557944140847.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.09, lambda_i=.12, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8933850328506282.\n",
      "training RMSE: 0.8954467443980143.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9882084317023953.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.08, lambda_i=.12, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8810657605174731.\n",
      "training RMSE: 0.8793935512980934.\n",
      "training RMSE: 0.8785809539198273.\n",
      "training RMSE: 0.8781562734253077.\n",
      "training RMSE: 0.8779093952847788.\n",
      "training RMSE: 0.8777493039843631.\n",
      "training RMSE: 0.877635216534865.\n",
      "training RMSE: 0.8775477675807446.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9885774655054526.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.07, lambda_i=.12, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8874785714241855.\n",
      "training RMSE: 0.8877111475985815.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9881828339262694.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.075, lambda_i=.12, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.07, lambda_i=.125, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8750818340461232.\n",
      "training RMSE: 0.8695922561249994.\n",
      "training RMSE: 0.867707582231519.\n",
      "training RMSE: 0.8669284630621088.\n",
      "training RMSE: 0.8665385416222589.\n",
      "training RMSE: 0.8663071990811342.\n",
      "training RMSE: 0.8661504826331785.\n",
      "training RMSE: 0.8660338957824613.\n",
      "training RMSE: 0.8659415335224563.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 0.9892708596057171.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.07, lambda_i=.11, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.08, lambda_i=.11, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.8985616420449941.\n",
      "training RMSE: 0.8999608562744604.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 0.9882211047909539.\n"
     ]
    }
   ],
   "source": [
    "u_features_011_40, i_features_011_40 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.09, lambda_i=.11, init_u_features=u_features_01_40,\n",
    "    init_i_features=i_features_01_40, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.6534586137476438.\n",
      "training RMSE: 0.6422907295119796.\n",
      "training RMSE: 0.6331741478288732.\n",
      "training RMSE: 0.6256363782162372.\n",
      "training RMSE: 0.6194738245373587.\n",
      "training RMSE: 0.6143144050570912.\n",
      "training RMSE: 0.6100458734836691.\n",
      "training RMSE: 0.6064691141476916.\n",
      "training RMSE: 0.6033600154625663.\n",
      "training RMSE: 0.600653562124729.\n",
      "training RMSE: 0.5982701005116559.\n",
      "training RMSE: 0.5961304061334717.\n",
      "training RMSE: 0.5942677051762565.\n",
      "training RMSE: 0.5925809042310873.\n",
      "training RMSE: 0.5910473468706384.\n",
      "training RMSE: 0.589617763857291.\n",
      "training RMSE: 0.5883066092943257.\n",
      "training RMSE: 0.5871116203645589.\n",
      "training RMSE: 0.5859757307322169.\n",
      "training RMSE: 0.5849288592007581.\n",
      "training RMSE: 0.5839240494548217.\n",
      "training RMSE: 0.5829927950056296.\n",
      "training RMSE: 0.5821408631750924.\n",
      "training RMSE: 0.5813338361280506.\n",
      "training RMSE: 0.5805419732371625.\n",
      "training RMSE: 0.5797926055089133.\n",
      "training RMSE: 0.579082452475022.\n",
      "training RMSE: 0.5783974363822718.\n",
      "training RMSE: 0.5777575266681354.\n",
      "training RMSE: 0.5771447372233818.\n",
      "training RMSE: 0.5765449595347527.\n",
      "training RMSE: 0.5759745871076891.\n",
      "training RMSE: 0.575417614965102.\n",
      "training RMSE: 0.5748624962670471.\n",
      "training RMSE: 0.5743637873765679.\n",
      "training RMSE: 0.5738573227775781.\n",
      "training RMSE: 0.5733660560822494.\n",
      "training RMSE: 0.5728945798600406.\n",
      "training RMSE: 0.5724378308385504.\n",
      "training RMSE: 0.5719916122839357.\n",
      "training RMSE: 0.5715547234992865.\n",
      "training RMSE: 0.5711204998316235.\n",
      "training RMSE: 0.5706918536978337.\n",
      "training RMSE: 0.570290546277285.\n",
      "training RMSE: 0.5699111832607192.\n",
      "training RMSE: 0.5695325472714257.\n",
      "training RMSE: 0.5691683269842212.\n",
      "training RMSE: 0.5688040057558433.\n",
      "training RMSE: 0.568451564663975.\n",
      "training RMSE: 0.5681015896270001.\n",
      "training RMSE: 0.5677755861107601.\n",
      "training RMSE: 0.5674445854684553.\n",
      "training RMSE: 0.5671167483164263.\n",
      "training RMSE: 0.5668015636326791.\n",
      "training RMSE: 0.5664938757888565.\n",
      "training RMSE: 0.5661837578923234.\n",
      "training RMSE: 0.5658817214635778.\n",
      "training RMSE: 0.5655910657408925.\n",
      "training RMSE: 0.5653090031355055.\n",
      "training RMSE: 0.5650187594178038.\n",
      "training RMSE: 0.5647391928461917.\n",
      "training RMSE: 0.5644605220068927.\n",
      "training RMSE: 0.564190393892023.\n",
      "training RMSE: 0.5639293089714716.\n",
      "training RMSE: 0.5636629740061475.\n",
      "training RMSE: 0.5634059461438556.\n",
      "training RMSE: 0.5631519804289872.\n",
      "training RMSE: 0.5629080166125939.\n",
      "training RMSE: 0.5626620111126902.\n",
      "training RMSE: 0.562424285520604.\n",
      "training RMSE: 0.5621842125176252.\n",
      "training RMSE: 0.5619533888533649.\n",
      "training RMSE: 0.5617276221255256.\n",
      "training RMSE: 0.5614972402131538.\n",
      "training RMSE: 0.5612666621132686.\n",
      "training RMSE: 0.5610455461127859.\n",
      "training RMSE: 0.5608274680323516.\n",
      "training RMSE: 0.5606149271586196.\n",
      "training RMSE: 0.5604059661197762.\n",
      "training RMSE: 0.5601959688542303.\n",
      "training RMSE: 0.5599915589665565.\n",
      "training RMSE: 0.5597857823304574.\n",
      "training RMSE: 0.559575055532093.\n",
      "training RMSE: 0.5593681672843219.\n",
      "training RMSE: 0.5591721266604476.\n",
      "training RMSE: 0.5589697947874087.\n",
      "training RMSE: 0.5587748669412099.\n",
      "training RMSE: 0.5585780785653081.\n",
      "training RMSE: 0.5583876638937337.\n",
      "training RMSE: 0.558199736140647.\n",
      "training RMSE: 0.5580152132009029.\n",
      "training RMSE: 0.5578321070911956.\n",
      "training RMSE: 0.5576568062327466.\n",
      "training RMSE: 0.557478288596173.\n",
      "training RMSE: 0.557301942079389.\n",
      "training RMSE: 0.5571280669040877.\n",
      "training RMSE: 0.5569511361785526.\n",
      "training RMSE: 0.5567820049121789.\n",
      "training RMSE: 0.5566087463696167.\n",
      "training RMSE: 0.5564371566206583.\n",
      "test RMSE after running ALS: 630.0353780229399.\n"
     ]
    }
   ],
   "source": [
    "u_features_0_60, i_features_0_60 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=0, lambda_i=0, k=60, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9197684680709619.\n",
      "training RMSE: 0.8291455686658581.\n",
      "training RMSE: 0.7833544267088788.\n",
      "training RMSE: 0.7651973293076858.\n",
      "training RMSE: 0.7557495001054115.\n",
      "training RMSE: 0.7498590990256256.\n",
      "training RMSE: 0.7460607826657366.\n",
      "training RMSE: 0.7433918339936172.\n",
      "training RMSE: 0.741384133054206.\n",
      "training RMSE: 0.739817099579396.\n",
      "training RMSE: 0.7385638616161235.\n",
      "training RMSE: 0.7375422613834153.\n",
      "training RMSE: 0.7366959292507607.\n",
      "training RMSE: 0.7359848981412597.\n",
      "training RMSE: 0.7353801917791133.\n",
      "training RMSE: 0.7348604224019104.\n",
      "training RMSE: 0.7344095423182087.\n",
      "training RMSE: 0.7340153156117378.\n",
      "training RMSE: 0.7336682625625537.\n",
      "training RMSE: 0.7333609227514835.\n",
      "training RMSE: 0.7330873357602062.\n",
      "training RMSE: 0.7328426715639716.\n",
      "training RMSE: 0.7326229646833201.\n",
      "training RMSE: 0.7324249209054203.\n",
      "training RMSE: 0.7322457752369247.\n",
      "training RMSE: 0.7320831863170385.\n",
      "training RMSE: 0.7319351569362869.\n",
      "training RMSE: 0.7317999733341011.\n",
      "training RMSE: 0.7316761580685354.\n",
      "training RMSE: 0.7315624327603649.\n",
      "training RMSE: 0.7314576880951342.\n",
      "training RMSE: 0.7313609592399486.\n",
      "Finished estimating features\n",
      "test RMSE after running ALS: 1.0100878461530176.\n"
     ]
    }
   ],
   "source": [
    "u_features_02_60, i_features_02_60 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.04, lambda_i=.09, init_u_features=u_features_0_60,\n",
    "    init_i_features=i_features_0_60, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.937124945573667.\n",
      "training RMSE: 0.938213384019861.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 1.0001283823013505.\n"
     ]
    }
   ],
   "source": [
    "u_features_02_60, i_features_02_60 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_0_60,\n",
    "    init_i_features=i_features_0_60, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9702491543325727.\n",
      "training RMSE: 0.9918356528055405.\n",
      "Whoops!\n",
      "test RMSE after running ALS: 1.0052750914530304.\n"
     ]
    }
   ],
   "source": [
    "u_features_02_60, i_features_02_60 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.2, lambda_i=.2, init_u_features=u_features_0_60,\n",
    "    init_i_features=i_features_0_60, max_iter=70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
