{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))\n",
    "#ratings = load_data('{dp}movielens100k.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1, verbose=False):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "        print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "        print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_ratings, train, test = split_data(ratings, num_items_per_user,\n",
    "    num_users_per_item, min_num_ratings=10, p_test=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from plots import plot_train_test_data\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read submission creation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using the global mean.\n",
    "    \"\"\"\n",
    "    # Compute global mean using training data\n",
    "    r_mean = data.sum() / data.count_nonzero()\n",
    "    return r_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean, set_num=0):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using either the user or the item mean,\n",
    "    as indicated in parameter mean.\n",
    "    \"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = max(set_num, data.shape[flag])\n",
    "    \n",
    "    # Obtain r_demeaned (ratings minus global avg)\n",
    "    global_mean = baseline_rating(data)\n",
    "    r_demeaned = data.copy()\n",
    "    r_demeaned.data = (1.0 * r_demeaned.data) - global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in r_demeaned\n",
    "    data_rcv = sp.find(r_demeaned)\n",
    "    # compute means\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first estimate the RMSE for our test set:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def model_baseline(data, test_data, test_flag, sub_flag=False,\n",
    "    sub_filename=\"new_submission\", verbose=False):\n",
    "\n",
    "    \"\"\"If 'test_flag' is True, then 'data' should be the training dataset\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "    \n",
    "    num_train_items, num_train_users = data.shape\n",
    "    num_test_items, num_test_users = test_data.shape\n",
    "    \n",
    "    num_i_max = max(num_train_items, num_test_items)\n",
    "    num_u_max = max(num_train_users, num_test_users)\n",
    "    \n",
    "    global_mean = baseline_rating(data)\n",
    "    item_means = baseline_user_item_specific(data, 'item', set_num=num_i_max)\n",
    "    user_means = baseline_user_item_specific(data, 'user', set_num=num_u_max)\n",
    "    \n",
    "    (rows, cols, vals) = sp.find(test_data)\n",
    "    \n",
    "    if test_flag:        \n",
    "        # Do predictions\n",
    "        pred_test = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "            for (i, u) in zip(rows, cols)])\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(vals))\n",
    "        if verbose:\n",
    "            print(\"Test RMSE of baseline using baseline: {e}\".format(e=test_rmse)) \n",
    "        return test_rmse, pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for (i, u) in zip(rows, cols):\n",
    "                pred_i_u = global_mean + user_means[u] + item_means[i]\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_rmse, pred_test = model_baseline(train, test, True, verbose=True)\n",
    "# Test RMSE of baseline using baseline: 1.0057078177840961"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the submission file training on all data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model_baseline(ratings_csr, sample_submission_csr, False, True, \"tmp\")\n",
    "# Achieves 1.00386 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with scipy's svds and baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_matrix(data, verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Compute global, user and item means    \n",
    "    global_mean = baseline_rating(data)\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals = train_vals - baselines\n",
    "    \n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    return r_demeaned, global_mean, user_means, item_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_matrix_factorization(data, test_data, test_flag, sub_flag=False,\n",
    "    k=20, int_vals=False, sub_filename=\"new_submission\", verbose=False):\n",
    "    \"\"\"Matrix factorization by (non-sparse) SVD.\n",
    "\n",
    "    If 'test_flag' is True, then 'data' should be the training dataset and\n",
    "    'test_data' the test dataset. In this case sub_flag is ignored.\n",
    "    \n",
    "    If 'test_flag' is False and 'sub_flag' is True, then 'data' should be\n",
    "    the entire ratings dataset and 'test_data' should be a sample submission.\n",
    "    \n",
    "    Both 'data' and 'test_data' should be csr sparse matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    assert test_flag or sub_flag, \"Specify a task\"\n",
    "    assert k <= min(data.shape), \"k must be smaller than the min dimension of 'data'\"\n",
    "    \n",
    "    # Substract baseline from data\n",
    "    r_demeaned, global_mean, user_means, item_means = demean_matrix(data, verbose=verbose)\n",
    "    # Use scipy's svds\n",
    "    U, sigma, Vt = svds(r_demeaned, k)\n",
    "    sigma = np.diag(sigma)\n",
    "    U_sigma = np.dot(U, sigma)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Finished fitting model')\n",
    "\n",
    "    # Get non-zero elements\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    (test_rows, test_cols, test_vals) = sp.find(test_data)\n",
    "\n",
    "    if test_flag:        \n",
    "        # Do predictions        \n",
    "        baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "            for (i, u) in zip(test_rows, test_cols)])\n",
    "        interactions = np.array([(U_sigma[i,:].dot(Vt[:,u]))\n",
    "            for (i, u) in zip(test_rows, test_cols)])\n",
    "        pred_test = baselines + interactions\n",
    "        if int_vals:\n",
    "            pred_test = np.rint(pred_test)\n",
    "      \n",
    "        if verbose:\n",
    "            print('Finished predicting')\n",
    "\n",
    "        # Compute and print test error\n",
    "        test_mse = calculate_mse(test_vals, pred_test)\n",
    "        test_rmse = np.sqrt(test_mse / len(test_vals))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Test RMSE using baseline and matrix factorization: {e}\".format(e=test_rmse)) \n",
    "            print()\n",
    "            print('-----------------------------------------------')\n",
    "            print(' Completed test in model_matrix_factorization! ')    \n",
    "            print('-----------------------------------------------')\n",
    "            \n",
    "        return test_rmse, pred_test\n",
    "\n",
    "    elif sub_flag:\n",
    "        # Directly write predictions to submission file\n",
    "        with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for (i, u) in zip(test_rows, test_cols):\n",
    "                interaction = U_sigma[i,:].dot(Vt[:,u])\n",
    "                baseline = global_mean + user_means[u] + item_means[i]\n",
    "                pred_i_u = interaction + baseline\n",
    "                writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "\n",
    "        if verbose:\n",
    "            print('-----------------------------------------------------')\n",
    "            print(' Completed submission in model_matrix_factorization! ')    \n",
    "            print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rmse, pred_test = model_matrix_factorization(train, test, True,\n",
    "    k=16, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=16: 0.9938483199616733\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=18, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=18: 0.9941246387415831\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=20, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=20: 0.9943127900237189\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=40, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=40: 0.9967949498593854\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=60, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=60: 0.9988456305848287\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train, test,\n",
    "    True, k=80, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=80: 1.0016261672370745\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train, test,\n",
    "    True, k=100, verbose=True)\n",
    "# Test RMSE of model_matrix_factorization with k=100: 1.0032905394645015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rmse, pred_test = model_matrix_factorization(train, test, True,\n",
    "    k=16, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=16: 0.9938483199616733\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=18, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=18: 0.9941246387415831\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=20, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=20: 0.9943127900237189\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=40, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=40: 0.9967949498593854\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train,\n",
    "    test, True, k=60, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=60: 0.9988456305848287\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train, test,\n",
    "    True, k=80, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=80: 1.0016261672370745\n",
    "\n",
    "test_rmse, pred_test = model_matrix_factorization(train, test,\n",
    "    True, k=100, verbose=True, int_vals=True)\n",
    "# Test RMSE of model_matrix_factorization with k=100: 1.0032905394645015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the submission file for Kaggle:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_matrix_factorization(ratings_csr, sample_submission_csr, False,\n",
    "    k=16, sub_flag=True, sub_filename=\"mf_svd_k16\", verbose=True)\n",
    "\n",
    "# Achieves 0.99063 in Kaggle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_matrix_factorization(ratings_csr, sample_submission_csr, False,\n",
    "    k=50, sub_flag=True, sub_filename=\"mf_svd_k50\", verbose=True)\n",
    "\n",
    "# Achieves 0.99536 in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this model is that the SVD function takes into account the missing values as if they had value 0, instead of ignoring them.\n",
    "\n",
    "This is the reason why if we increase the number of factors k, the predictions are worse - because they are being predicted as 0.\n",
    "\n",
    "Also, the predicted results are worse if they are rounded to an integer instead of directly considering the float number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
