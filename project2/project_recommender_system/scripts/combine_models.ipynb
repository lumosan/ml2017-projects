{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from datafile_methods.data_io import save_csv\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datafile_methods.data_io import load_datasets\n",
    "from datafile_methods.data_processing import load_data\n",
    "\n",
    "# Load datasets\n",
    "folds, ratings, sample_submission = load_datasets()\n",
    "\n",
    "k_fold = len(folds)\n",
    "\n",
    "models = ['baseline',\n",
    "          'knn_baseline_i',\n",
    "          'knn_baseline_u',\n",
    "          'mf_svd_sci',\n",
    "          'nmf',\n",
    "          'slope_one',\n",
    "          'sur_svd',\n",
    "          'mf_als_recommend',\n",
    "          'mf_als']\n",
    "\n",
    "# Load predictions for each fold and model\n",
    "predictions = [[load_data('{p}model_{m}_te_{i}.csv'.format(\n",
    "    p=PREDICTION_PATH, m=model, i=i)) for i in range(k_fold)] for model in models]\n",
    "\n",
    "predictions_dict = dict(zip(models, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the set into B disjoint subsets: Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support : The support of a data point (u, i) is the\n",
    "number of votes by user u. The blender can now base\n",
    "the weighting of predictors dependent on how many\n",
    "rating the user has given. RBMs are prone to receive\n",
    "high weight when the user has only a few votes in the\n",
    "data. SVDs are highly weighted when much information\n",
    "from a user is available.\n",
    "\n",
    "http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_raw_data\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine a separate blending for each subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Manually compute weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prediction_methods.create_ensemble import create_weighted_ensemble_submission\n",
    "\n",
    "# Load predictions of submission entries with each model\n",
    "predictions_sub = [load_data('{p}model_{m}_sub.csv'.format(p=PREDICTION_PATH, m=model))\n",
    "    for model in models]\n",
    "predictions_sub_dict = dict(zip(models, predictions_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_methods.create_ensemble import evaluate_manual_weighted_ensemble\n",
    "predictions_high_df, errors_comb_high, predictions_low_df, errors_comb_low = evaluate_manual_weighted_ensemble(ratings, folds, predictions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weighted_ensemble_submission(ratings, predictions_sub_dict,\n",
    "    prediction_path=PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Ridge Regression to find best weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use only the previous predictions, since the ensemble underfits.\n",
    "\n",
    "We get extra stuff from https://arxiv.org/pdf/0911.0460.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from prediction_methods.create_ensemble import evaluate_meta_features_ensemble\n",
    "predictors_high_df, observations_high_df, predictors_low_df, observations_low_df = evaluate_meta_features_ensemble(ratings, folds, predictions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "poly = PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_errors_h_tr = {}\n",
    "regularization_errors_h_te = {}\n",
    "\n",
    "for alpha in np.linspace(0,.001,9):\n",
    "    rmse_errors_tr = np.zeros(k_fold)\n",
    "    rmse_errors_te = np.zeros(k_fold)\n",
    "    for i in range(k_fold):\n",
    "        # Get training data\n",
    "        preds_train = predictors_high_df[i][::2]\n",
    "        train = poly.fit_transform(preds_train)\n",
    "        vals_train = observations_high_df[i][::2]\n",
    "        # Create and fit model\n",
    "        clf = Ridge(alpha=alpha, normalize=True)\n",
    "        clf.fit(train, vals_train)\n",
    "        # Obtain predictions for training set\n",
    "        pred_train = clf.predict(train)\n",
    "        pred_train = np.clip(pred_train, 1.0, 5.0)\n",
    "        # Compute train error\n",
    "        rmse_train = sqrt(mean_squared_error(vals_train, pred_train))\n",
    "\n",
    "        # Use remaining data for testing\n",
    "        preds_test = predictors_high_df[i][1::2]\n",
    "        test = poly.fit_transform(preds_test)\n",
    "        vals_test = observations_high_df[i][1::2]\n",
    "        # Obtain predictions for test setpred_high_sub\n",
    "        pred_test = clf.predict(test)\n",
    "        # Compute test error\n",
    "        rmse_test = sqrt(mean_squared_error(vals_test, pred_test))\n",
    "        \n",
    "        # Save errors in arrays\n",
    "        rmse_errors_tr[i] = rmse_train\n",
    "        rmse_errors_te[i] = rmse_test\n",
    "    # Save errors in dictionaries\n",
    "    regularization_errors_h_tr[alpha] = rmse_errors_tr\n",
    "    regularization_errors_h_te[alpha] = rmse_errors_te\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.DataFrame(regularization_errors_h_tr).boxplot()\n",
    "pd.DataFrame(regularization_errors_h_te).boxplot()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.DataFrame(regularization_errors_h_tr).boxplot()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.DataFrame(regularization_errors_h_te).boxplot()\n",
    "plt.show()\n",
    "\n",
    "# Best: alpha = .000125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_errors_l_tr = {}\n",
    "regularization_errors_l_te = {}\n",
    "\n",
    "for alpha in np.linspace(0,.001,9):\n",
    "    rmse_errors_tr = np.zeros(k_fold)\n",
    "    rmse_errors_te = np.zeros(k_fold)\n",
    "    for i in range(k_fold):\n",
    "        # Get training data\n",
    "        preds_train = predictors_low_df[i][::2]\n",
    "        train = poly.fit_transform(preds_train)\n",
    "        vals_train = observations_low_df[i][::2]\n",
    "        # Create and fit model\n",
    "        clf = Ridge(alpha=alpha, normalize=True)\n",
    "        clf.fit(train, vals_train)\n",
    "        # Obtain predictions for training set\n",
    "        pred_train = clf.predict(train)\n",
    "        pred_train = np.clip(pred_train, 1.0, 5.0)\n",
    "        # Compute train error\n",
    "        rmse_train = sqrt(mean_squared_error(vals_train, pred_train))\n",
    "\n",
    "        # Use remaining data for testing\n",
    "        preds_test = predictors_low_df[i][1::2]\n",
    "        test = poly.fit_transform(preds_test)\n",
    "        vals_test = observations_low_df[i][1::2]\n",
    "        # Obtain predictions for test set\n",
    "        pred_test = clf.predict(test)\n",
    "        # Compute test error\n",
    "        rmse_test = sqrt(mean_squared_error(vals_test, pred_test))\n",
    "        \n",
    "        # Save errors in arrays\n",
    "        rmse_errors_tr[i] = rmse_train\n",
    "        rmse_errors_te[i] = rmse_test\n",
    "    # Save errors in dictionaries\n",
    "    regularization_errors_l_tr[alpha] = rmse_errors_tr\n",
    "    regularization_errors_l_te[alpha] = rmse_errors_te\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.DataFrame(regularization_errors_l_tr).boxplot()\n",
    "pd.DataFrame(regularization_errors_l_te).boxplot()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.DataFrame(regularization_errors_l_tr).boxplot()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,35))\n",
    "pd.DataFrame(regularization_errors_l_te).boxplot()\n",
    "plt.show()\n",
    "\n",
    "# Best: alpha = .000375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load predictions of submission entries with each model\n",
    "predictions_sub = [load_data('{p}model_{m}_sub.csv'.format(p=PREDICTION_PATH, m=model))\n",
    "    for model in models]\n",
    "predictions_sub_dict = dict(zip(models, predictions_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_methods.create_ensemble import create_sklearn_ensemble_submission\n",
    "create_sklearn_ensemble_submission(ratings, predictions_sub_dict, predictions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here on I don't think it's useful\n",
    "But I can comment on the report that the voting system didn't work :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=0.01\n",
    "min_v=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get valid votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_vote_gen(tol):\n",
    "    def decide_vote(pred):\n",
    "        \"\"\"Defines if a model gets to vote or not\"\"\"\n",
    "        vote = round(pred)\n",
    "        if (tol > abs(vote - pred)):\n",
    "            return vote\n",
    "        else:\n",
    "            return np.nan\n",
    "    return decide_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the predictions are valid votes or not\n",
    "df_votes_tr = df_pred_tr.applymap(decide_vote_gen(tol))\n",
    "df_votes_sub = df_pred_sub.applymap(decide_vote_gen(tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have each model vote as many times as specified\n",
    "df_final_votes_tr = df_votes_tr[['nmf']]\n",
    "df_final_votes_sub = df_votes_sub[['nmf']]\n",
    "for m in df_votes_tr.columns:\n",
    "    for i in range(model_weights['n_votes'][m]):\n",
    "        df_final_votes_tr['{}_{}'.format(m,i)] = df_votes_tr[m]\n",
    "        df_final_votes_sub['{}_{}'.format(m,i)] = df_votes_sub[m]\n",
    "\n",
    "df_final_votes_tr = df_final_votes_tr.drop('nmf', axis=1)\n",
    "df_final_votes_sub = df_final_votes_sub.drop('nmf', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_tr['median_vote'] = df_final_votes_tr.median(axis=1)\n",
    "df_pred_sub['median_vote'] = df_final_votes_sub.median(axis=1)\n",
    "\n",
    "n_votes_tr = df_final_votes_tr.notnull().sum(axis=1)\n",
    "n_votes_sub = df_final_votes_sub.notnull().sum(axis=1)\n",
    "\n",
    "df_pred_tr['median_vote'][n_votes_tr < min_v] = np.nan\n",
    "df_pred_sub['median_vote'][n_votes_sub < min_v] = np.nan\n",
    "\n",
    "## Get weighted mean\n",
    "# Add column with mean values\n",
    "df_pred_tr['mean'] = pd.Series(sp.find(sp_mean_tr)[2])\n",
    "df_pred_sub['mean'] = pd.Series(sp.find(sp_mean_sub)[2])\n",
    "\n",
    "# Combine decisions and means\n",
    "df_pred_tr['final'] = df_pred_tr['median_vote'].combine_first(df_pred_tr['mean'])\n",
    "df_pred_sub['final'] = df_pred_sub['median_vote'].combine_first(df_pred_sub['mean'])\n",
    "\n",
    "## Look at the results\n",
    "e_round['{}_{}'.format(tol, min_v)] = np.sqrt(np.mean((df_obs_tr[0] - df_pred_tr['final'])**2))\n",
    "e_mean['{}_{}'.format(tol, min_v)] = np.sqrt(np.mean((df_obs_tr[0] - df_pred_tr['mean'])**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
