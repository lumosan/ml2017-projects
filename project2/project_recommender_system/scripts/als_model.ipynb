{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1, verbose=False):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "        print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "        print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratings, train, test = split_data(ratings, num_items_per_user,\n",
    "    num_users_per_item, min_num_ratings=10, p_test=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read submission creation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using the global mean.\n",
    "    \"\"\"\n",
    "    # Compute global mean using training data\n",
    "    r_mean = data.sum() / data.getnnz()\n",
    "    return r_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean, set_num=0):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using either the user or the item mean,\n",
    "    as indicated in parameter mean.\n",
    "    \"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = max(set_num, data.shape[flag])\n",
    "    \n",
    "    # Obtain r_demeaned (ratings minus global avg)\n",
    "    global_mean = baseline_rating(data)\n",
    "    r_demeaned = data.copy()\n",
    "    r_demeaned.data = (1.0 * r_demeaned.data) - global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in r_demeaned\n",
    "    data_rcv = sp.find(r_demeaned)\n",
    "    # compute means\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_matrix(data, verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Compute global, user and item means    \n",
    "    global_mean = baseline_rating(data)\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals = train_vals - baselines\n",
    "    \n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    return r_demeaned, global_mean, user_means, item_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_test_matrix(data, global_mean, item_means, user_means,\n",
    "    verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_items, num_users = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals -= baselines\n",
    "\n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_items, num_users))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    return r_demeaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(data, k):\n",
    "    \"\"\"Initializes parameters for Matrix Factorization.\n",
    "    Assumes 'data' matrix is already demeaned.\n",
    "    \"\"\"      \n",
    "    np.random.seed(988)\n",
    "    num_items, num_users = data.shape\n",
    "    u_features = np.random.rand(k, num_users)\n",
    "    i_features = np.random.rand(k, num_items)\n",
    "    return u_features, i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, u_features, i_features, nz):\n",
    "    \"\"\"Compute RMSE for prediction of nonzero elements.\"\"\"\n",
    "    preds = np.array([(u_features[:,u].dot(i_features[:,i]))\n",
    "        for (i, u) in nz])\n",
    "    vals = np.array([data[i,u] for (i,u) in nz])\n",
    "    mse = calculate_mse(vals, preds)  \n",
    "    rmse = np.sqrt(mse / len(vals))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_features(train, i_features, lambda_u,\n",
    "    n_i_per_user, nz_i_per_user):\n",
    "    \"\"\"Updates user feature matrix.\"\"\"\n",
    "    n_u = len(nz_i_per_user)\n",
    "    k = i_features.shape[0]\n",
    "    lambda_u_I = lambda_u * sp.eye(k)\n",
    "    new_u_features = np.zeros((k, n_u))\n",
    "    for u, i in nz_i_per_user:\n",
    "        M = i_features[:,i]\n",
    "        V = train[i,u].T.dot(M.T)\n",
    "        A = M.dot(M.T) + n_i_per_user[u] * lambda_u_I\n",
    "        new_u_features[:,u] = np.linalg.solve(A, V.T).T\n",
    "    return new_u_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_item_features(train, u_features, lambda_i,\n",
    "    n_u_per_item, nz_u_per_item):\n",
    "    \"\"\"Updates item feature matrix.\"\"\"\n",
    "    n_i = len(nz_u_per_item)\n",
    "    k = u_features.shape[0]\n",
    "    lambda_i_I = lambda_i * sp.eye(k)\n",
    "    new_i_features = np.zeros((k, n_i))\n",
    "    for i, u in nz_u_per_item:\n",
    "        M = u_features[:,u]\n",
    "        V = train[i,u].dot(M.T)\n",
    "        A = M.dot(M.T) + n_u_per_item[i] * lambda_i_I\n",
    "        new_i_features[:,i] = np.linalg.solve(A, V.T).T\n",
    "    return new_i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_ALS(data, test, k=20, lambda_u=.1, lambda_i=.7, tol=1e-6, max_iter=100,\n",
    "    init_u_features=None, init_i_features=None, sub_filename=\"new_submission\"):\n",
    "    \"\"\"Matrix factorization by ALS\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # Substract baseline from data\n",
    "    data_demeaned, global_mean, user_means, item_means = demean_matrix(data)\n",
    "    test_demeaned = demean_test_matrix(test, global_mean, item_means, user_means)\n",
    "    \n",
    "    # Get non-zero elements\n",
    "    (rows, cols, vals) = sp.find(data_demeaned)\n",
    "    (test_rows, test_cols, test_vals) = sp.find(test_demeaned)\n",
    "    \n",
    "    # Initialize feature vectors for users and items\n",
    "    rand_u_features, rand_i_features = init_MF(data_demeaned, k)\n",
    "    if init_u_features is None:\n",
    "        u_features = rand_u_features\n",
    "    else:\n",
    "        u_features = init_u_features\n",
    "\n",
    "    if init_i_features is None:\n",
    "        i_features = rand_i_features\n",
    "    else:\n",
    "        i_features = init_i_features\n",
    "\n",
    "    # Get number of non-zero ratings per user and item\n",
    "    n_i_per_user = data_demeaned.getnnz(axis=0)\n",
    "    n_u_per_item = data_demeaned.getnnz(axis=1)\n",
    "    \n",
    "    # Get non-zero ratings per user and item\n",
    "    nz_train, nz_u_per_item, nz_i_per_user = build_index_groups(data_demeaned)\n",
    "\n",
    "    e = 1000\n",
    "    \n",
    "    # ALS-WR algorithm\n",
    "    for it in range(max_iter):\n",
    "        u_features = update_user_features(data_demeaned, i_features, lambda_u,\n",
    "            n_i_per_user, nz_i_per_user)\n",
    "        i_features = update_item_features(data_demeaned, u_features, lambda_i,\n",
    "            n_u_per_item, nz_u_per_item)\n",
    "        # compute and print new training error\n",
    "        old_e = e\n",
    "        e = compute_error(data_demeaned, u_features, i_features, nz_train)\n",
    "        print(\"training RMSE: {}.\".format(e))\n",
    "        if(abs(old_e - e) < tol):\n",
    "            print('Finished estimating features')\n",
    "            break\n",
    "        if(old_e - e < -tol):\n",
    "            print('Whoops!')\n",
    "            break\n",
    "    # Do predictions        \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    interactions = np.array([u_features[:,u].dot(i_features[:,i].T)\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    pred_test = baselines + interactions\n",
    "\n",
    "    # Compute and print test error    \n",
    "    with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for (i, u) in zip(test_rows, test_cols):\n",
    "            interaction = u_features[:,u].dot(i_features[:,i].T)\n",
    "            baseline = global_mean + item_means[i] + user_means[u]\n",
    "            pred_i_u = interaction + baseline\n",
    "            writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_features_01, i_features_01 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_factorization_ALS(ratings_csr, sample_submission_csr, k=20, lambda_u=.1, lambda_i=.1, tol=1e-6, max_iter=100,\n",
    "    init_u_features=None, init_i_features=None, sub_filename=\"mf_als_k20_l01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
