{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import calculate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "`ratings` is a sparse matrix in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "PREDICTION_PATH = '../data/predictions/'\n",
    "ratings = load_data('{dp}data_train.csv'.format(dp=DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb40c40a2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1, verbose=False):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][:, valid_users]\n",
    "    \n",
    "    # LIL is a convenient format for constructing sparse matrices\n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    \n",
    "    valid_ratings_i, valid_ratings_u, valid_ratings_v = sp.find(valid_ratings)\n",
    "    valid_ratings_p_idx = np.random.permutation(range(len(valid_ratings_i)))\n",
    "    \n",
    "    n_test = int(p_test*len(valid_ratings_i))\n",
    "    \n",
    "    for idx in valid_ratings_p_idx[:n_test]:\n",
    "        test[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "        \n",
    "    for idx in valid_ratings_p_idx[n_test:]:\n",
    "        train[valid_ratings_i[idx], valid_ratings_u[idx]] = valid_ratings_v[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "        print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "        print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    \n",
    "    # convert to CSR for faster operations\n",
    "    return valid_ratings, train.tocsr(), test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in original data:1176952\n",
      "Total number of nonzero elements in train data:1059186\n",
      "Total number of nonzero elements in test data:117687\n"
     ]
    }
   ],
   "source": [
    "valid_ratings, train, test = split_data(ratings, num_items_per_user,\n",
    "    num_users_per_item, min_num_ratings=10, p_test=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read submission creation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "ratings_csr = ratings.tocsr()\n",
    "sample_submission = load_data('{dp}sample_submission.csv'.format(dp=DATA_PATH))\n",
    "sample_submission_csr = sample_submission.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_division(a, b):\n",
    "    \"\"\"Computes element by element division.\n",
    "    If x/0 returns 0.\n",
    "    \"\"\"\n",
    "    # Raises error if vectors have different lengths\n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    # Computes division\n",
    "    res = a.copy()\n",
    "    for i in range(len(a)):\n",
    "        if b[i] == 0:\n",
    "            res[i] = 0\n",
    "        else:\n",
    "            res[i] = a[i] / b[i]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline rating\n",
    "def baseline_rating(data):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using the global mean.\n",
    "    \"\"\"\n",
    "    # Compute global mean using training data\n",
    "    r_mean = data.sum() / data.getnnz()\n",
    "    return r_mean\n",
    "\n",
    "\n",
    "# User or item specific effect\n",
    "def baseline_user_item_specific(data, mean, set_num=0):\n",
    "    \"\"\"Implements baseline method for a ratings matrix\n",
    "    using either the user or the item mean,\n",
    "    as indicated in parameter mean.\n",
    "    \"\"\"\n",
    "    if mean==\"user\":\n",
    "        flag = 1\n",
    "        inv_flag = 0\n",
    "    else:\n",
    "        flag = 0\n",
    "        inv_flag = 1\n",
    "\n",
    "    num = max(set_num, data.shape[flag])\n",
    "    \n",
    "    # Obtain r_demeaned (ratings minus global avg)\n",
    "    global_mean = baseline_rating(data)\n",
    "    r_demeaned = data.copy()\n",
    "    r_demeaned.data = (1.0 * r_demeaned.data) - global_mean\n",
    "    \n",
    "    # Compute means using training data\n",
    "    # get rows, columns and values for elements in r_demeaned\n",
    "    data_rcv = sp.find(r_demeaned)\n",
    "    # compute means\n",
    "    counts = np.bincount(data_rcv[flag], minlength=num)\n",
    "    sums = np.bincount(data_rcv[flag], weights=data_rcv[2], minlength=num)\n",
    "    means = compute_division(sums, counts)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_matrix(data, verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Compute global, user and item means    \n",
    "    global_mean = baseline_rating(data)\n",
    "    item_means = baseline_user_item_specific(data, 'item')\n",
    "    user_means = baseline_user_item_specific(data, 'user')\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals = train_vals - baselines\n",
    "    \n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_rows, num_cols))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    return r_demeaned, global_mean, user_means, item_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_test_matrix(data, global_mean, item_means, user_means,\n",
    "    verbose=False):\n",
    "    \"\"\"Removes the global, user and item means from a matrix.\n",
    "    Returns the matrix and the computed means.\n",
    "    \"\"\"\n",
    "    num_items, num_users = data.shape\n",
    "    (rows, cols, vals) = sp.find(data)\n",
    "    \n",
    "    # Substract the baseline of each element in 'data'\n",
    "    train_vals = vals.copy()\n",
    "    train_vals = 1.0 * train_vals\n",
    "    \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(rows, cols)])\n",
    "    train_vals -= baselines\n",
    "\n",
    "    # Get matrix\n",
    "    r_demeaned = sp.csr_matrix((train_vals, (rows, cols)),\n",
    "        shape=(num_items, num_users))\n",
    "    \n",
    "    if verbose:\n",
    "        print('---------------------------------------------')\n",
    "        print('          Completed demean_matrix!           ')\n",
    "        print('---------------------------------------------')\n",
    "    return r_demeaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(data, k):\n",
    "    \"\"\"Initializes parameters for Matrix Factorization.\n",
    "    Assumes 'data' matrix is already demeaned.\n",
    "    \"\"\"      \n",
    "    np.random.seed(988)\n",
    "    num_items, num_users = data.shape\n",
    "    u_features = np.random.rand(k, num_users)\n",
    "    i_features = np.random.rand(k, num_items)\n",
    "    return u_features, i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, u_features, i_features, nz):\n",
    "    \"\"\"Compute RMSE for prediction of nonzero elements.\"\"\"\n",
    "    preds = np.array([(u_features[:,u].dot(i_features[:,i]))\n",
    "        for (i, u) in nz])\n",
    "    vals = np.array([data[i,u] for (i,u) in nz])\n",
    "    mse = calculate_mse(vals, preds)  \n",
    "    rmse = np.sqrt(mse / len(vals))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_features(train, i_features, lambda_u,\n",
    "    n_i_per_user, nz_i_per_user):\n",
    "    \"\"\"Updates user feature matrix.\"\"\"\n",
    "    n_u = len(nz_i_per_user)\n",
    "    k = i_features.shape[0]\n",
    "    lambda_u_I = lambda_u * sp.eye(k)\n",
    "    new_u_features = np.zeros((k, n_u))\n",
    "    for u, i in nz_i_per_user:\n",
    "        M = i_features[:,i]\n",
    "        V = train[i,u].T.dot(M.T)\n",
    "        A = M.dot(M.T) + n_i_per_user[u] * lambda_u_I\n",
    "        new_u_features[:,u] = np.linalg.solve(A, V.T).T\n",
    "    return new_u_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_item_features(train, u_features, lambda_i,\n",
    "    n_u_per_item, nz_u_per_item):\n",
    "    \"\"\"Updates item feature matrix.\"\"\"\n",
    "    n_i = len(nz_u_per_item)\n",
    "    k = u_features.shape[0]\n",
    "    lambda_i_I = lambda_i * sp.eye(k)\n",
    "    new_i_features = np.zeros((k, n_i))\n",
    "    for i, u in nz_u_per_item:\n",
    "        M = u_features[:,u]\n",
    "        V = train[i,u].dot(M.T)\n",
    "        A = M.dot(M.T) + n_u_per_item[i] * lambda_i_I\n",
    "        new_i_features[:,i] = np.linalg.solve(A, V.T).T\n",
    "    return new_i_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_ALS(data, test, k=20, lambda_u=.1, lambda_i=.7, tol=1e-6, max_iter=100,\n",
    "    init_u_features=None, init_i_features=None, sub_filename=\"new_submission\"):\n",
    "    \"\"\"Matrix factorization by ALS\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # Substract baseline from data\n",
    "    data_demeaned, global_mean, user_means, item_means = demean_matrix(data)\n",
    "    test_demeaned = demean_test_matrix(test, global_mean, item_means, user_means)\n",
    "    \n",
    "    # Get non-zero elements\n",
    "    (rows, cols, vals) = sp.find(data_demeaned)\n",
    "    (test_rows, test_cols, test_vals) = sp.find(test_demeaned)\n",
    "    \n",
    "    # Initialize feature vectors for users and items\n",
    "    rand_u_features, rand_i_features = init_MF(data_demeaned, k)\n",
    "    if init_u_features is None:\n",
    "        u_features = rand_u_features\n",
    "    else:\n",
    "        u_features = init_u_features\n",
    "\n",
    "    if init_i_features is None:\n",
    "        i_features = rand_i_features\n",
    "    else:\n",
    "        i_features = init_i_features\n",
    "\n",
    "    # Get number of non-zero ratings per user and item\n",
    "    n_i_per_user = data_demeaned.getnnz(axis=0)\n",
    "    n_u_per_item = data_demeaned.getnnz(axis=1)\n",
    "    \n",
    "    # Get non-zero ratings per user and item\n",
    "    nz_train, nz_u_per_item, nz_i_per_user = build_index_groups(data_demeaned)\n",
    "\n",
    "    e = 1000\n",
    "    \n",
    "    # ALS-WR algorithm\n",
    "    for it in range(max_iter):\n",
    "        u_features = update_user_features(data_demeaned, i_features, lambda_u,\n",
    "            n_i_per_user, nz_i_per_user)\n",
    "        i_features = update_item_features(data_demeaned, u_features, lambda_i,\n",
    "            n_u_per_item, nz_u_per_item)\n",
    "        # compute and print new training error\n",
    "        old_e = e\n",
    "        e = compute_error(data_demeaned, u_features, i_features, nz_train)\n",
    "        print(\"training RMSE: {}.\".format(e))\n",
    "        if(abs(old_e - e) < tol):\n",
    "            print('Finished estimating features')\n",
    "            break\n",
    "        if(old_e - e < -tol):\n",
    "            print('Whoops!')\n",
    "            break\n",
    "    # Do predictions        \n",
    "    baselines = np.array([(global_mean + item_means[i] + user_means[u])\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    interactions = np.array([u_features[:,u].dot(i_features[:,i].T)\n",
    "        for (i, u) in zip(test_rows, test_cols)])\n",
    "    pred_test = baselines + interactions\n",
    "\n",
    "    # Compute and print test error    \n",
    "    with open('{dp}{fn}.csv'.format(dp=PREDICTION_PATH, fn=sub_filename), 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for (i, u) in zip(test_rows, test_cols):\n",
    "            interaction = u_features[:,u].dot(i_features[:,i].T)\n",
    "            baseline = global_mean + item_means[i] + user_means[u]\n",
    "            pred_i_u = interaction + baseline\n",
    "            writer.writerow({'Id':'r{r}_c{c}'.format(r=i+1,c=u+1),'Prediction':pred_i_u})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_features_01, i_features_01 = matrix_factorization_ALS(r_demeaned, test_r_demeaned,\n",
    "    lambda_u=.1, lambda_i=.1, init_u_features=u_features_1,\n",
    "    init_i_features=i_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSE: 0.9914056724208424.\n",
      "training RMSE: 0.9900521783129816.\n",
      "training RMSE: 0.9831430660653337.\n",
      "training RMSE: 0.970112641575422.\n",
      "training RMSE: 0.9600324207716402.\n",
      "training RMSE: 0.9530952847293976.\n",
      "training RMSE: 0.9478842886307369.\n",
      "training RMSE: 0.9441252023170419.\n",
      "training RMSE: 0.9414708263854211.\n",
      "training RMSE: 0.939597186818672.\n",
      "training RMSE: 0.9382681029451635.\n",
      "training RMSE: 0.9373181705088184.\n",
      "training RMSE: 0.9366311177581605.\n",
      "training RMSE: 0.9361258650447625.\n",
      "training RMSE: 0.9357467744679271.\n",
      "training RMSE: 0.9354561360015584.\n",
      "training RMSE: 0.9352284844935719.\n",
      "training RMSE: 0.9350465373600463.\n",
      "training RMSE: 0.9348984263820911.\n",
      "training RMSE: 0.9347758672331428.\n",
      "training RMSE: 0.934672966811863.\n",
      "training RMSE: 0.934585448906412.\n",
      "training RMSE: 0.9345101492890515.\n",
      "training RMSE: 0.9344446832216614.\n",
      "training RMSE: 0.9343872234279536.\n",
      "training RMSE: 0.9343363493158452.\n",
      "training RMSE: 0.9342909426495482.\n",
      "training RMSE: 0.9342501139257511.\n",
      "training RMSE: 0.9342131493782772.\n",
      "training RMSE: 0.9341794720916302.\n",
      "training RMSE: 0.9341486129458348.\n",
      "training RMSE: 0.9341201885410054.\n",
      "training RMSE: 0.9340938841680172.\n",
      "training RMSE: 0.9340694404913292.\n",
      "training RMSE: 0.9340466430083068.\n",
      "training RMSE: 0.9340253136185366.\n",
      "training RMSE: 0.9340053038215271.\n",
      "training RMSE: 0.9339864891900723.\n",
      "training RMSE: 0.933968764857524.\n",
      "training RMSE: 0.9339520418220086.\n",
      "training RMSE: 0.9339362439170192.\n",
      "training RMSE: 0.9339213053313382.\n",
      "training RMSE: 0.9339071685854311.\n",
      "training RMSE: 0.9338937828891719.\n",
      "training RMSE: 0.9338811028188071.\n",
      "training RMSE: 0.9338690872609101.\n",
      "training RMSE: 0.9338576985787389.\n",
      "training RMSE: 0.9338469019625364.\n",
      "training RMSE: 0.9338366649305065.\n",
      "training RMSE: 0.9338269569516754.\n",
      "training RMSE: 0.9338177491658707.\n",
      "training RMSE: 0.9338090141796996.\n",
      "training RMSE: 0.9338007259207636.\n",
      "training RMSE: 0.9337928595353412.\n",
      "training RMSE: 0.933785391317539.\n",
      "training RMSE: 0.9337782986603026.\n",
      "training RMSE: 0.9337715600207909.\n",
      "training RMSE: 0.9337651548944096.\n",
      "training RMSE: 0.9337590637933105.\n",
      "training RMSE: 0.9337532682263722.\n",
      "training RMSE: 0.933747750678681.\n",
      "training RMSE: 0.933742494589276.\n",
      "training RMSE: 0.9337374843265062.\n",
      "training RMSE: 0.9337327051607683.\n",
      "training RMSE: 0.9337281432346956.\n",
      "training RMSE: 0.9337237855310514.\n",
      "training RMSE: 0.9337196198387129.\n",
      "training RMSE: 0.9337156347171786.\n",
      "training RMSE: 0.9337118194600622.\n",
      "training RMSE: 0.9337081640580159.\n",
      "training RMSE: 0.9337046591614913.\n",
      "training RMSE: 0.9337012960437251.\n",
      "training RMSE: 0.9336980665642542.\n",
      "training RMSE: 0.9336949631332465.\n",
      "training RMSE: 0.9336919786768686.\n",
      "training RMSE: 0.9336891066038543.\n",
      "training RMSE: 0.9336863407734339.\n",
      "training RMSE: 0.9336836754646911.\n",
      "training RMSE: 0.9336811053474346.\n",
      "training RMSE: 0.9336786254546047.\n",
      "training RMSE: 0.9336762311562293.\n",
      "training RMSE: 0.9336739181349282.\n",
      "training RMSE: 0.9336716823629215.\n",
      "training RMSE: 0.933669520080533.\n",
      "training RMSE: 0.933667427776109.\n",
      "training RMSE: 0.9336654021673224.\n",
      "training RMSE: 0.9336634401837888.\n",
      "training RMSE: 0.9336615389509348.\n",
      "training RMSE: 0.9336596957750541.\n",
      "training RMSE: 0.9336579081294809.\n",
      "training RMSE: 0.933656173641816.\n",
      "training RMSE: 0.9336544900821414.\n",
      "training RMSE: 0.9336528553521585.\n",
      "training RMSE: 0.9336512674751883.\n",
      "training RMSE: 0.9336497245869703.\n",
      "training RMSE: 0.9336482249272104.\n",
      "training RMSE: 0.9336467668318152.\n",
      "training RMSE: 0.9336453487257719.\n",
      "training RMSE: 0.9336439691166089.\n",
      "training RMSE: 0.9336426265884108.\n"
     ]
    }
   ],
   "source": [
    "matrix_factorization_ALS(ratings_csr, sample_submission_csr, k=20, lambda_u=.1, lambda_i=.1, tol=1e-6, max_iter=100,\n",
    "    init_u_features=None, init_i_features=None, sub_filename=\"new_submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
