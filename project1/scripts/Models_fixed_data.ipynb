{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "1. Process data: drop columns with many nans; normalize ignoring nan vals; fill in nans with zeros (mean); drop highly correlated variables; build polynomial (degree 2 or 3); normalize again; add offset\n",
    "2. Try ridge regression -> pretty good results, but not the best for classification\n",
    "3. Try logistic regression -> we might be overfitting\n",
    "4. Try an improved regularized logistic regression: doesnt penalize the mean component, and has an adaptive lambda. This is better for convergence (SGD), we dont worry about w0 being penalized (there are more labels of one kind than of the other, thus the mean of y is not 0, but we don't want to penalize it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from proj1_helpers import *\n",
    "from auxiliary_functions import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y, y_pred, zeros_ones=False):\n",
    "    if len(np.array(y).shape) > 1:\n",
    "        y=y.flatten()\n",
    "    if len(np.array(y_pred).shape) > 1:\n",
    "        y_pred=y_pred.flatten()\n",
    "    if zeros_ones:\n",
    "        incorrect = np.sum(np.abs(y - y_pred))\n",
    "    else:\n",
    "        incorrect = np.sum(np.abs(y - y_pred))/2\n",
    "    precision = 1 - (incorrect / y.shape[0])\n",
    "    return precision\n",
    "\n",
    "def predict_labels_bis(weights, data, return_zeros=False):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    if return_zeros:\n",
    "        y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "    else:\n",
    "        y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files and prepare data\n",
    "yb, input_data, ids = load_csv_data(DATA_PATH+'train.csv')\n",
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')\n",
    "\n",
    "# Y contains two arrays, one with labels -1 and 1, other with 0 and 1\n",
    "y = np.ones(len(yb))\n",
    "y[np.where(yb==-1)] = 0\n",
    "Y=np.array((yb, y)).T\n",
    "\n",
    "# keep fullest columns only\n",
    "nans=[]\n",
    "for c in input_data.T:\n",
    "    nans.append(-c[np.where(c==-999)].sum()/999)\n",
    "cols_full = np.array(range(input_data.shape[1]))[np.where(np.array(nans) < 40000)]\n",
    "full_data = input_data[:, cols_full]\n",
    "test_full_data = test_input_data[:, cols_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize over normal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_test(x, means, stds):\n",
    "    x_1 = x - means\n",
    "    x_2 = x_1 / stds\n",
    "    return x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ignoring_values(x, nan):\n",
    "    cs=[]\n",
    "    means=[]\n",
    "    stds=[]\n",
    "    for c in x.T:\n",
    "        mean = np.mean(c[np.where(c!=nan)])\n",
    "        c_1 = c - mean\n",
    "        std = np.std(c_1[np.where(c!=nan)])\n",
    "        c_2 = c_1 / std\n",
    "        c_2[np.where(c==nan)]=0\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "        cs.append(c_2)\n",
    "    return np.array(cs).T, np.array(means), np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_test_ignoring_values(x, nan, means, stds):\n",
    "    x_1 = x - means\n",
    "    x_2 = x_1 / stds\n",
    "    x_2[np.where(x==nan)]=0\n",
    "    return x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(x):\n",
    "    corrs=[]\n",
    "    for i in range(x.shape[1]):\n",
    "        corr_row = []\n",
    "        for j in range(x.shape[1]):\n",
    "            corr = np.corrcoef(x[:,i], x[:,j])[0,1]\n",
    "            corr_row.append(corr)\n",
    "        corrs.append(corr_row)\n",
    "    return np.array(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete([1,2,3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated(x, val_max):\n",
    "    x_corr = corr_matrix(x)\n",
    "    low_correlated = []\n",
    "    to_be_removed = []\n",
    "    for r in range(x_corr.shape[0]):\n",
    "        to_compare = abs(x_corr[r,:r])\n",
    "        to_compare_e = []\n",
    "        to_compare_c = []\n",
    "        low_corr = True\n",
    "        if len(to_compare) > 0:\n",
    "            for i in range(len(to_compare)):\n",
    "                if i not in to_be_removed:\n",
    "                    to_compare_c.append(to_compare[i])\n",
    "                    to_compare_e.append(i)\n",
    "            if np.array(to_compare_c).max() > val_max:\n",
    "                low_corr = False\n",
    "                to_be_removed.append(r)\n",
    "        low_correlated.append(low_corr)\n",
    "    return low_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize_ignoring_values(full_data, -999)\n",
    "test_x = standardize_test_ignoring_values(test_full_data, -999, mean_x, std_x)\n",
    "\n",
    "cols_useful = np.array(range(len(cols_full)))[np.where(remove_highly_correlated(x, .75))]\n",
    "small_x = x[:, cols_useful]\n",
    "test_small_x = test_x[:, cols_useful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa6a9ab7f98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6a76e9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(corr_matrix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build poly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "poly_tx = build_poly(small_x, 3)\n",
    "test_poly_tx = build_poly(test_small_x, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build poly, Standardize again, add offset (only used for the improved regularized logistic regression, which is currently the top submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_tx_1 = build_poly(small_x, 3, offset=False)\n",
    "test_poly_tx_1 = build_poly(test_small_x, 3, offset=False)\n",
    "\n",
    "x2, mean_x2, std_x2 = standardize(poly_tx_1)\n",
    "test_x2 = standardize_test(test_poly_tx_1, mean_x2, std_x2)\n",
    "\n",
    "yb, poly_tx = build_model_data(x2, yb)\n",
    "test_yb, test_poly_tx = build_model_data(test_x2, test_yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data: 80% for training and 20% for testing. Out of the training, 80% is for *training* and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_tx, te_tx, tv_Y, te_Y = split_data(poly_tx, Y, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tx, va_tx, tr_Y, va_Y = split_data(tv_tx, tv_Y, .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = tr_Y.T[0]\n",
    "va_y = va_Y.T[0]\n",
    "te_y = te_Y.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_errors = []\n",
    "va_errors = []\n",
    "ws = []\n",
    "\n",
    "lambdas = np.logspace(-2, 5, num=30)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = ridge_regression(tr_y, tr_tx, lambda_)\n",
    "    tr_y_pred = predict_labels(w, tr_tx)\n",
    "    va_y_pred = predict_labels(w, va_tx)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred)\n",
    "    ws.append(w)\n",
    "    tr_errors.append(tr_error)\n",
    "    va_errors.append(va_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(lambdas, tr_errors)\n",
    "plt.semilogx(lambdas, va_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose $\\lambda=10^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1e2\n",
    "w, loss = ridge_regression(tr_y, tr_tx, lambda_)\n",
    "te_y_pred = predict_labels(w, te_tx)\n",
    "calculate_precision(te_y_pred, te_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75263999999999998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'1_ridge_fd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 20\n",
    "ws = []\n",
    "losses = []\n",
    "tr_errors = []\n",
    "va_errors = []\n",
    "gammas = np.logspace(-8, -3.3, 12)\n",
    "for gamma in gammas:\n",
    "    w, loss = logistic_regression(tr_y, tr_tx, initial_w, max_iters, gamma)\n",
    "    tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "    va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "    tr_errors.append(tr_error)\n",
    "    print(tr_error)\n",
    "    va_errors.append(va_error)\n",
    "    print(va_error)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(gammas, tr_errors)\n",
    "plt.semilogx(gammas, va_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 10**-3.3\n",
    "max_iters = 80\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "w, loss = logistic_regression(tr_y, tr_tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.76563999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'2_lr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with adaptive lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_improved(y, tx, initial_w, max_iters, gamma,\n",
    "    batch_size=1000, return_all=False):\n",
    "    \"\"\"Logistic regression, with adaptive lambda\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "    \n",
    "    # init parameters    \n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = logistic_by_gd(y_n, tx_n, w, next(gamma))\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 40\n",
    "ws = []\n",
    "losses = []\n",
    "tr_errors = []\n",
    "va_errors = []\n",
    "\n",
    "gammas = [(np.random.uniform(.1,.9), np.random.randint(-4,-2)) for k in range(20)]\n",
    "\n",
    "for g in gammas:\n",
    "    gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "    w, loss = logistic_regression_improved(tr_y, tr_tx, initial_w, max_iters, gamma)\n",
    "    tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "    va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "    tr_errors.append(tr_error)\n",
    "    va_errors.append(va_error)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 100\n",
    "gamma = adaptive_gamma(kappa=0.1, eta0=1e-3)\n",
    "w, loss = logistic_regression_improved(tr_y, tr_tx, initial_w, max_iters, gamma, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.77025999999999994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'3_lri.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import reg_logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different gammas and lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 60\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "te_errors=[]\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-4, -3, 3)\n",
    "lambdas = np.logspace(-4, 0, num=5)\n",
    "\n",
    "for gamma in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        w, loss = reg_logistic_regression(tr_y, tr_tx, lambda_, initial_w, max_iters, gamma, batch_size=2000)        \n",
    "        tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "        va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "        tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "        va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "        tr_errors.append(tr_error)\n",
    "        va_errors.append(va_error)\n",
    "        print()\n",
    "        print(gamma)\n",
    "        print(lambda_)\n",
    "        print(va_error)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "gamma = .001\n",
    "lambda_ = .01\n",
    "max_iters = 100\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "w, loss = reg_logistic_regression(tr_y, tr_tx, lambda_, initial_w, max_iters,\n",
    "    gamma, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved regularized logistic regression with adaptive gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_improved(y, tx, lambda_, initial_w, max_iters,\n",
    "    gamma, batch_size=1000, return_all=False):\n",
    "    \"\"\"Regularized logistic regression using mini-batch gradient descent.\n",
    "    Uses adaptive lambda and does not penalize offset.\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "\n",
    "    # init parameters\n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = reg_logistic_by_gd(y_n, tx_n, lambda_, w, next(gamma), False)\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different gammas and lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.001\n",
      "(0.4044687039971879, -4)\n",
      "0.28295\n",
      "\n",
      "0.01\n",
      "(0.4044687039971879, -4)\n",
      "0.28315\n",
      "\n",
      "0.1\n",
      "(0.4044687039971879, -4)\n",
      "0.283175\n",
      "\n",
      "1.0\n",
      "(0.4044687039971879, -4)\n",
      "0.2835\n",
      "\n",
      "10.0\n",
      "(0.4044687039971879, -4)\n",
      "0.28315\n",
      "\n",
      "100.0\n",
      "(0.4044687039971879, -4)\n",
      "0.284775\n",
      "\n",
      "1000.0\n",
      "(0.4044687039971879, -4)\n",
      "0.323575\n",
      "\n",
      "0.001\n",
      "(0.84267375543786, -3)\n",
      "0.261375\n",
      "\n",
      "0.01\n",
      "(0.84267375543786, -3)\n",
      "0.2615\n",
      "\n",
      "0.1\n",
      "(0.84267375543786, -3)\n",
      "0.259425\n",
      "\n",
      "1.0\n",
      "(0.84267375543786, -3)\n",
      "0.261375\n",
      "\n",
      "10.0\n",
      "(0.84267375543786, -3)\n",
      "0.261425\n",
      "\n",
      "100.0\n",
      "(0.84267375543786, -3)\n",
      "0.268075\n",
      "\n",
      "1000.0\n",
      "(0.84267375543786, -3)\n",
      "0.317625\n",
      "\n",
      "0.001\n",
      "(0.3069826867370638, -4)\n",
      "0.2808\n",
      "\n",
      "0.01\n",
      "(0.3069826867370638, -4)\n",
      "0.280675\n",
      "\n",
      "0.1\n",
      "(0.3069826867370638, -4)\n",
      "0.279375\n",
      "\n",
      "1.0\n",
      "(0.3069826867370638, -4)\n",
      "0.280125\n",
      "\n",
      "10.0\n",
      "(0.3069826867370638, -4)\n",
      "0.280625\n",
      "\n",
      "100.0\n",
      "(0.3069826867370638, -4)\n",
      "0.280825\n",
      "\n",
      "1000.0\n",
      "(0.3069826867370638, -4)\n",
      "0.319775\n",
      "\n",
      "0.001\n",
      "(0.12848780555347813, -4)\n",
      "0.273375\n",
      "\n",
      "0.01\n",
      "(0.12848780555347813, -4)\n",
      "0.27335\n",
      "\n",
      "0.1\n",
      "(0.12848780555347813, -4)\n",
      "0.272225\n",
      "\n",
      "1.0\n",
      "(0.12848780555347813, -4)\n",
      "0.2741\n",
      "\n",
      "10.0\n",
      "(0.12848780555347813, -4)\n",
      "0.272875\n",
      "\n",
      "100.0\n",
      "(0.12848780555347813, -4)\n",
      "0.2751\n",
      "\n",
      "1000.0\n",
      "(0.12848780555347813, -4)\n",
      "0.31805\n",
      "\n",
      "0.001\n",
      "(0.37972412998770627, -4)\n",
      "0.282375\n",
      "\n",
      "0.01\n",
      "(0.37972412998770627, -4)\n",
      "0.283025\n",
      "\n",
      "0.1\n",
      "(0.37972412998770627, -4)\n",
      "0.282525\n",
      "\n",
      "1.0\n",
      "(0.37972412998770627, -4)\n",
      "0.2827\n",
      "\n",
      "10.0\n",
      "(0.37972412998770627, -4)\n",
      "0.28195\n",
      "\n",
      "100.0\n",
      "(0.37972412998770627, -4)\n",
      "0.283725\n",
      "\n",
      "1000.0\n",
      "(0.37972412998770627, -4)\n",
      "0.321775\n",
      "\n",
      "0.001\n",
      "(0.3872523530960694, -2)\n",
      "0.229675\n",
      "\n",
      "0.01\n",
      "(0.3872523530960694, -2)\n",
      "0.236125\n",
      "\n",
      "0.1\n",
      "(0.3872523530960694, -2)\n",
      "0.235125\n",
      "\n",
      "1.0\n",
      "(0.3872523530960694, -2)\n",
      "0.249775\n",
      "\n",
      "10.0\n",
      "(0.3872523530960694, -2)\n",
      "0.24145\n",
      "\n",
      "100.0\n",
      "(0.3872523530960694, -2)\n",
      "0.318425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:106: RuntimeWarning: overflow encountered in exp\n",
      "  loss = (np.sum(np.log(1 + np.exp(tx_w))) - np.sum(y * tx_w) +\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  t_exp = np.exp(t)\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return t_exp / (t_exp + 1)\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in less_equal\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000.0\n",
      "(0.3872523530960694, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.7693642791831146, -3)\n",
      "0.2586\n",
      "\n",
      "0.01\n",
      "(0.7693642791831146, -3)\n",
      "0.25895\n",
      "\n",
      "0.1\n",
      "(0.7693642791831146, -3)\n",
      "0.258775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a6edc260e46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtr_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_bis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_zeros\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mva_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_bis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_zeros\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-a65ff4029d35>\u001b[0m in \u001b[0;36mreg_logistic_regression_improved\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma, batch_size, return_all)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# get mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0my_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# get loss and update w by gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_by_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(y, tx, batch_size)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 50\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "va_errors=[]\n",
    "losses = []\n",
    "\n",
    "gammas = [(np.random.uniform(.05,.9), np.random.randint(-4,-1)) for k in range(10)]\n",
    "lambdas = np.logspace(-3, 3, num=7)\n",
    "\n",
    "for g in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "        w, loss = reg_logistic_regression_improved(tr_y, tr_tx, lambda_, initial_w, max_iters, gamma)        \n",
    "        tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "        va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "        tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "        va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "        tr_errors.append(tr_error)\n",
    "        va_errors.append(va_error)\n",
    "        print()\n",
    "        print(lambda_)\n",
    "        print(g)\n",
    "        print(va_error)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 100\n",
    "gamma = adaptive_gamma(kappa=0.4, eta0=1e-2)\n",
    "lambda_= .001\n",
    "w, loss = reg_logistic_regression_improved(tr_y, tr_tx, lambda_, initial_w,\n",
    "                                           max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76863999999999999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.76863999999999999 (Using 3 degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'5_irlr_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression using k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    '''\n",
    "    Build k indices for k-fold.\n",
    "    '''\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def split_data_k_indices(y, x, k_indices, k):\n",
    "    '''\n",
    "    Splits the data into test and training data.\n",
    "    Samples get randomized through 'k_indices' and\n",
    "    selected through 'k'.\n",
    "    '''\n",
    "    mask = np.ones(k_indices.shape, dtype=bool)\n",
    "    mask[k] = False\n",
    "    \n",
    "    # Test data\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    # Train data\n",
    "    x_train = x[k_indices[mask]]\n",
    "    y_train = y[k_indices[mask]]\n",
    "    \n",
    "    return x_test, y_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    # tx_test = build_poly(x_test, degree)\n",
    "    # tx_train = build_poly(x_train, degree)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = ridge_regression(y_train, x_train, lambda_)\n",
    "    \n",
    "    prediction = {\n",
    "        'train': predict_labels(w_opt, x_train),\n",
    "        'test': predict_labels(w_opt, x_test)\n",
    "    } \n",
    "    \n",
    "    precision = {\n",
    "        'train': calculate_precision(prediction['train'], y_train),\n",
    "        'test': calculate_precision(prediction['test'], y_test)\n",
    "    }\n",
    "    \n",
    "    # Return loss for train and test data\n",
    "    return precision['train'], precision['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te, mse_dif):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.semilogx(lambds, mse_dif, marker=\".\", color='y', label='error delta')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 7\n",
    "k_fold = 3\n",
    "seed = 13\n",
    "\n",
    "lambdas = np.logspace(-40, 40, 30)\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "lambda_avg_rmse_trs = []\n",
    "lambda_avg_rmse_tes = []\n",
    "lambda_avg_rmse_difs = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    rmse_trs = []\n",
    "    rmse_tes = []\n",
    "    rmse_difs = []\n",
    "\n",
    "    # K-fold cross validation and pick the rmse 'test' and 'train'\n",
    "    # errors that represent the least absolute difference between\n",
    "    # them (for a given lambda).\n",
    "    for k in range(k_fold):\n",
    "        rmse_cur_tr, rmse_cur_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        \n",
    "        rmse_trs.append(rmse_cur_tr)\n",
    "        rmse_tes.append(rmse_cur_te)\n",
    "        rmse_difs.append(abs(rmse_cur_tr - rmse_cur_te))\n",
    "        \n",
    "    lambda_avg_rmse_trs.append(np.mean(rmse_trs))\n",
    "    lambda_avg_rmse_tes.append(np.mean(rmse_tes))\n",
    "    lambda_avg_rmse_difs.append(np.mean(rmse_difs))\n",
    "\n",
    "cross_validation_visualization(lambdas, lambda_avg_rmse_trs, lambda_avg_rmse_tes, lambda_avg_rmse_difs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try cross validation in reg logistic regression (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_rlr(y, x, k_indices, k, lambda_, gamma):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = w, loss = reg_logistic_regression_improved(y_train, x_train, lambda_, initial_w, 100, gamma)\n",
    "\n",
    "    prediction = {\n",
    "        'train': predict_labels_bis(w_opt, x_train, return_zeros=True),\n",
    "        'test': predict_labels_bis(w_opt, x_test, return_zeros=True)\n",
    "    } \n",
    "    \n",
    "    precision = {\n",
    "        'train': 1 - calculate_precision(prediction['train'], y_train, zeros_ones=True),\n",
    "        'test': 1 - calculate_precision(prediction['test'], y_test, zeros_ones=True),\n",
    "    }\n",
    "    \n",
    "    # Return loss for train and test data\n",
    "    return precision['train'], precision['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return z[0][0] / tx.shape[0]\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:10: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 100\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "va_errors=[]\n",
    "losses = []\n",
    "k_fold=3\n",
    "seed=1\n",
    "\n",
    "yc=Y[1]\n",
    "xc=poly_tx\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(yc, k_fold, seed)\n",
    "\n",
    "lambda_gamma_avg_rmse_trs = []\n",
    "lambda_gamma_avg_rmse_tes = []\n",
    "lambda_gamma_avg_rmse_difs = []\n",
    "\n",
    "np.random.seed(1)\n",
    "gammas = [(np.random.uniform(.05,.9), np.random.randint(-4,-1)) for k in range(20)]\n",
    "lambdas = np.logspace(-4, 4, num=9)\n",
    "\n",
    "for g in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_trs = []\n",
    "        rmse_tes = []\n",
    "        rmse_difs = []\n",
    "        for k in range(k_fold):\n",
    "            gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "            rmse_cur_tr, rmse_cur_te = cross_validation_rlr(yc, xc, k_indices, k, lambda_, gamma)\n",
    "            rmse_trs.append(rmse_cur_tr)\n",
    "            rmse_tes.append(rmse_cur_te)\n",
    "            rmse_difs.append(abs(rmse_cur_tr - rmse_cur_te))\n",
    "        lambda_gamma_avg_rmse_trs.append(np.mean(rmse_trs))\n",
    "        lambda_gamma_avg_rmse_tes.append(np.mean(rmse_tes))\n",
    "        lambda_gamma_avg_rmse_difs.append(np.mean(rmse_difs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_gamma_avg_rmse_trs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w_opt, test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
