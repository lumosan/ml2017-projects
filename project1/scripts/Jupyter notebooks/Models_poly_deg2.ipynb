{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from proj1_helpers import *\n",
    "from auxiliary_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y, y_pred, zeros_ones=False):\n",
    "    if len(np.array(y).shape) > 1:\n",
    "        y=y.flatten()\n",
    "    if len(np.array(y_pred).shape) > 1:\n",
    "        y_pred=y_pred.flatten()\n",
    "    if zeros_ones:\n",
    "        incorrect = np.sum(np.abs(y - y_pred))\n",
    "    else:\n",
    "        incorrect = np.sum(np.abs(y - y_pred))/2\n",
    "    precision = 1 - (incorrect / y.shape[0])\n",
    "    return precision\n",
    "\n",
    "def predict_labels_bis(weights, data, return_zeros=False):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    if return_zeros:\n",
    "        y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "    else:\n",
    "        y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(DATA_PATH+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we need to change the labels (we now need to do it for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones(len(yb))\n",
    "y[np.where(yb==-1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array((yb, y)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y.T[0] has labels {-1, 1}\n",
    "\n",
    "Y.T[1] has labels {0, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix weird values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = input_data\n",
    "clean_data[np.where(input_data==-999)] = 0\n",
    "\n",
    "test_clean_data = test_input_data\n",
    "test_clean_data[np.where(input_data==-999)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(clean_data)\n",
    "test_x, test_mean_x, test_std_x = standardize(test_clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Keep some columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_x = x[:, [0,1,2,10,11,13]]\n",
    "test_small_x = test_x[:, [0,1,2,10,11,13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_tx = build_poly(small_x, 2)\n",
    "test_poly_tx = build_poly(test_small_x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data: 80% for training and 20% for testing. Out of the training, 80% is for *training* and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_tx, te_tx, tv_Y, te_Y = split_data(poly_tx, Y, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tx, va_tx, tr_Y, va_Y = split_data(tv_tx, tv_Y, .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = tr_Y.T[0]\n",
    "va_y = va_Y.T[0]\n",
    "te_y = te_Y.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_errors = []\n",
    "va_errors = []\n",
    "ws = []\n",
    "\n",
    "lambdas = np.logspace(-2, 5, num=30)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = ridge_regression(tr_y, tr_tx, lambda_)\n",
    "    tr_y_pred = predict_labels(w, tr_tx)\n",
    "    va_y_pred = predict_labels(w, va_tx)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred)\n",
    "    ws.append(w)\n",
    "    tr_errors.append(tr_error)\n",
    "    va_errors.append(va_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU0IQERFACUFERHxKCCIiAighiIiI738Bv15sjAv3uMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52d5719ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(lambdas, tr_errors)\n",
    "plt.semilogx(lambdas, va_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose $\\lambda=10^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73304000000000002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ = 1e2\n",
    "w, loss = ridge_regression(tr_y, tr_tx, lambda_)\n",
    "te_y_pred = predict_labels(w, te_tx)\n",
    "calculate_precision(te_y_pred, te_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.73304000000000002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'ridge_submission_upd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 40\n",
    "ws = []\n",
    "losses = []\n",
    "tr_errors = []\n",
    "va_errors = []\n",
    "gammas = np.logspace(-5, -2.5, 10)\n",
    "for gamma in gammas:\n",
    "    w, loss = logistic_regression(tr_y, tr_tx, initial_w, max_iters, gamma)\n",
    "    tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "    va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "    tr_errors.append(tr_error)\n",
    "    va_errors.append(va_error)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52862bb080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(gammas, tr_errors)\n",
    "plt.semilogx(gammas, va_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e-3\n",
    "max_iters = 100\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "w, loss = logistic_regression(tr_y, tr_tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74981999999999993"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.74981999999999993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'lr_submission_upd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with adaptive lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = tr_Y.T[1]\n",
    "va_y = va_Y.T[1]\n",
    "te_y = te_Y.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_improved(y, tx, initial_w, max_iters, gamma,\n",
    "    batch_size=1000, return_all=False):\n",
    "    \"\"\"Logistic regression, with adaptive lambda\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "    \n",
    "    # init parameters    \n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = logistic_by_gd(y_n, tx_n, w, next(gamma))\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 40\n",
    "ws = []\n",
    "losses = []\n",
    "tr_errors = []\n",
    "va_errors = []\n",
    "\n",
    "gammas = [(np.random.uniform(.1,.9), np.random.randint(-4,-2)) for k in range(20)]\n",
    "\n",
    "for g in gammas:\n",
    "    gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "    w, loss = logistic_regression_improved(tr_y, tr_tx, initial_w, max_iters, gamma)\n",
    "    tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "    va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "    tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "    va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "    tr_errors.append(tr_error)\n",
    "    va_errors.append(va_error)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.43361760376205927, -4),\n",
       " (0.846045887470927, -3),\n",
       " (0.3418660581054718, -3),\n",
       " (0.28887118156158087, -3),\n",
       " (0.24900816910213674, -3),\n",
       " (0.635796829442784, -3),\n",
       " (0.5310533872026856, -4),\n",
       " (0.35061881354582014, -4),\n",
       " (0.263561799785214, -4),\n",
       " (0.2836617709838605, -4),\n",
       " (0.6363740081427218, -3),\n",
       " (0.4657638463895907, -3),\n",
       " (0.21230955087618703, -4),\n",
       " (0.7227113890692268, -3),\n",
       " (0.874609260575518, -3),\n",
       " (0.17424064691259034, -3),\n",
       " (0.8011113218368306, -4),\n",
       " (0.7633175258937133, -4),\n",
       " (0.1312438265863059, -3),\n",
       " (0.1473945610412508, -4)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33744375000000004,\n",
       " 0.28513125000000006,\n",
       " 0.25940625000000006,\n",
       " 0.25688749999999994,\n",
       " 0.2623875,\n",
       " 0.27257500000000001,\n",
       " 0.33876250000000008,\n",
       " 0.33413749999999998,\n",
       " 0.32946249999999999,\n",
       " 0.33160000000000001,\n",
       " 0.26643749999999999,\n",
       " 0.26458749999999998,\n",
       " 0.32883125000000002,\n",
       " 0.27803124999999995,\n",
       " 0.27839999999999998,\n",
       " 0.26129999999999998,\n",
       " 0.34065624999999999,\n",
       " 0.34056874999999998,\n",
       " 0.25358124999999998,\n",
       " 0.31857500000000005]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33804999999999996,\n",
       " 0.28687499999999999,\n",
       " 0.26255000000000006,\n",
       " 0.26037500000000002,\n",
       " 0.26642500000000002,\n",
       " 0.27564999999999995,\n",
       " 0.33945000000000003,\n",
       " 0.33472499999999994,\n",
       " 0.32994999999999997,\n",
       " 0.33240000000000003,\n",
       " 0.26897499999999996,\n",
       " 0.26772499999999999,\n",
       " 0.32942500000000008,\n",
       " 0.28092499999999998,\n",
       " 0.28167500000000001,\n",
       " 0.26507499999999995,\n",
       " 0.34182500000000005,\n",
       " 0.34172500000000006,\n",
       " 0.25705,\n",
       " 0.319025]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 100\n",
    "gamma = adaptive_gamma(kappa=0.1, eta0=1e-3)\n",
    "w, loss = logistic_regression_improved(tr_y, tr_tx, initial_w, max_iters, gamma, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74371999999999994"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.74371999999999994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'lri_submission_upd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import reg_logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different gammas and lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0001\n",
      "0.001\n",
      "0.2858\n",
      "\n",
      "0.001\n",
      "0.001\n",
      "0.261375\n",
      "\n",
      "0.01\n",
      "0.001\n",
      "0.254475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:96: RuntimeWarning: overflow encountered in exp\n",
      "  # compute the cost\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  # update w\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  w_1 = w\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in less_equal\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0001\n",
      "0.01\n",
      "nan\n",
      "\n",
      "0.001\n",
      "0.01\n",
      "nan\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "nan\n",
      "\n",
      "0.0001\n",
      "0.1\n",
      "nan\n",
      "\n",
      "0.001\n",
      "0.1\n",
      "nan\n",
      "\n",
      "0.01\n",
      "0.1\n",
      "nan\n",
      "\n",
      "0.0001\n",
      "1.0\n",
      "nan\n",
      "\n",
      "0.001\n",
      "1.0\n",
      "nan\n",
      "\n",
      "0.01\n",
      "1.0\n",
      "nan\n",
      "\n",
      "0.0001\n",
      "10.0\n",
      "nan\n",
      "\n",
      "0.001\n",
      "10.0\n",
      "nan\n",
      "\n",
      "0.01\n",
      "10.0\n",
      "nan\n",
      "\n",
      "0.0001\n",
      "100.0\n",
      "nan\n",
      "\n",
      "0.001\n",
      "100.0\n",
      "nan\n",
      "\n",
      "0.01\n",
      "100.0\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 20\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "te_errors=[]\n",
    "losses = []\n",
    "\n",
    "gammas = np.logspace(-5, 5, 11)\n",
    "lambdas = np.logspace(-3, 3, num=7)\n",
    "\n",
    "for gamma in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        w, loss = reg_logistic_regression(tr_y, tr_tx, lambda_, initial_w, max_iters, gamma, batch_size=2000)        \n",
    "        tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "        va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "        tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "        va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "        tr_errors.append(tr_error)\n",
    "        va_errors.append(va_error)\n",
    "        print()\n",
    "        print(lambda_)\n",
    "        print(gamma)\n",
    "        print(va_error)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "gamma = .001\n",
    "lambda_ = .01\n",
    "max_iters = 100\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "w, loss = reg_logistic_regression(tr_y, tr_tx, lambda_, initial_w, max_iters,\n",
    "    gamma, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75375999999999999"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved regularized logistic regression with adaptive gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_improved(y, tx, lambda_, initial_w, max_iters,\n",
    "    gamma, batch_size=1000, return_all=False):\n",
    "    \"\"\"Regularized logistic regression using mini-batch gradient descent.\n",
    "    Uses adaptive lambda and does not penalize offset.\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "\n",
    "    # init parameters\n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = reg_logistic_by_gd(y_n, tx_n, lambda_, w, next(gamma), False)\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different gammas and lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.001\n",
      "(0.4144666096098798, -3)\n",
      "0.268275\n",
      "\n",
      "0.01\n",
      "(0.4144666096098798, -3)\n",
      "0.26645\n",
      "\n",
      "0.1\n",
      "(0.4144666096098798, -3)\n",
      "0.262625\n",
      "\n",
      "1.0\n",
      "(0.4144666096098798, -3)\n",
      "0.2603\n",
      "\n",
      "10.0\n",
      "(0.4144666096098798, -3)\n",
      "0.2692\n",
      "\n",
      "100.0\n",
      "(0.4144666096098798, -3)\n",
      "0.280125\n",
      "\n",
      "1000.0\n",
      "(0.4144666096098798, -3)\n",
      "0.34275\n",
      "\n",
      "0.001\n",
      "(0.6779804887602191, -2)\n",
      "0.21955\n",
      "\n",
      "0.01\n",
      "(0.6779804887602191, -2)\n",
      "0.22105\n",
      "\n",
      "0.1\n",
      "(0.6779804887602191, -2)\n",
      "0.2247\n",
      "\n",
      "1.0\n",
      "(0.6779804887602191, -2)\n",
      "0.224975\n",
      "\n",
      "10.0\n",
      "(0.6779804887602191, -2)\n",
      "0.237125\n",
      "\n",
      "100.0\n",
      "(0.6779804887602191, -2)\n",
      "0.25385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:15: RuntimeWarning: overflow encountered in exp\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  # update w\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  w_1 = w\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in less_equal\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000.0\n",
      "(0.6779804887602191, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.6298227205333596, -2)\n",
      "0.217975\n",
      "\n",
      "0.01\n",
      "(0.6298227205333596, -2)\n",
      "0.2279\n",
      "\n",
      "0.1\n",
      "(0.6298227205333596, -2)\n",
      "0.224025\n",
      "\n",
      "1.0\n",
      "(0.6298227205333596, -2)\n",
      "0.2456\n",
      "\n",
      "10.0\n",
      "(0.6298227205333596, -2)\n",
      "0.2233\n",
      "\n",
      "100.0\n",
      "(0.6298227205333596, -2)\n",
      "0.26695\n",
      "\n",
      "1000.0\n",
      "(0.6298227205333596, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.548120153411696, -3)\n",
      "0.270675\n",
      "\n",
      "0.01\n",
      "(0.548120153411696, -3)\n",
      "0.2721\n",
      "\n",
      "0.1\n",
      "(0.548120153411696, -3)\n",
      "0.2683\n",
      "\n",
      "1.0\n",
      "(0.548120153411696, -3)\n",
      "0.270675\n",
      "\n",
      "10.0\n",
      "(0.548120153411696, -3)\n",
      "0.2693\n",
      "\n",
      "100.0\n",
      "(0.548120153411696, -3)\n",
      "0.287725\n",
      "\n",
      "1000.0\n",
      "(0.548120153411696, -3)\n",
      "0.341525\n",
      "\n",
      "0.001\n",
      "(0.40848324300629346, -2)\n",
      "0.27885\n",
      "\n",
      "0.01\n",
      "(0.40848324300629346, -2)\n",
      "0.268725\n",
      "\n",
      "0.1\n",
      "(0.40848324300629346, -2)\n",
      "0.2666\n",
      "\n",
      "1.0\n",
      "(0.40848324300629346, -2)\n",
      "0.24485\n",
      "\n",
      "10.0\n",
      "(0.40848324300629346, -2)\n",
      "0.275975\n",
      "\n",
      "100.0\n",
      "(0.40848324300629346, -2)\n",
      "0.338675\n",
      "\n",
      "1000.0\n",
      "(0.40848324300629346, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.6510415341636413, -2)\n",
      "0.239025\n",
      "\n",
      "0.01\n",
      "(0.6510415341636413, -2)\n",
      "0.22145\n",
      "\n",
      "0.1\n",
      "(0.6510415341636413, -2)\n",
      "0.223\n",
      "\n",
      "1.0\n",
      "(0.6510415341636413, -2)\n",
      "0.230975\n",
      "\n",
      "10.0\n",
      "(0.6510415341636413, -2)\n",
      "0.227175\n",
      "\n",
      "100.0\n",
      "(0.6510415341636413, -2)\n",
      "0.317475\n",
      "\n",
      "1000.0\n",
      "(0.6510415341636413, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.70915007011796, -2)\n",
      "0.221525\n",
      "\n",
      "0.01\n",
      "(0.70915007011796, -2)\n",
      "0.224725\n",
      "\n",
      "0.1\n",
      "(0.70915007011796, -2)\n",
      "0.21995\n",
      "\n",
      "1.0\n",
      "(0.70915007011796, -2)\n",
      "0.2184\n",
      "\n",
      "10.0\n",
      "(0.70915007011796, -2)\n",
      "0.225375\n",
      "\n",
      "100.0\n",
      "(0.70915007011796, -2)\n",
      "0.251675\n",
      "\n",
      "1000.0\n",
      "(0.70915007011796, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.06817964428719923, -2)\n",
      "nan\n",
      "\n",
      "0.01\n",
      "(0.06817964428719923, -2)\n",
      "nan\n",
      "\n",
      "0.1\n",
      "(0.06817964428719923, -2)\n",
      "0.29675\n",
      "\n",
      "1.0\n",
      "(0.06817964428719923, -2)\n",
      "0.333125\n",
      "\n",
      "10.0\n",
      "(0.06817964428719923, -2)\n",
      "0.30165\n",
      "\n",
      "100.0\n",
      "(0.06817964428719923, -2)\n",
      "nan\n",
      "\n",
      "1000.0\n",
      "(0.06817964428719923, -2)\n",
      "nan\n",
      "\n",
      "0.001\n",
      "(0.6063805305703175, -3)\n",
      "0.270675\n",
      "\n",
      "0.01\n",
      "(0.6063805305703175, -3)\n",
      "0.277225\n",
      "\n",
      "0.1\n",
      "(0.6063805305703175, -3)\n",
      "0.263425\n",
      "\n",
      "1.0\n",
      "(0.6063805305703175, -3)\n",
      "0.27295\n",
      "\n",
      "10.0\n",
      "(0.6063805305703175, -3)\n",
      "0.275625\n",
      "\n",
      "100.0\n",
      "(0.6063805305703175, -3)\n",
      "0.2835\n",
      "\n",
      "1000.0\n",
      "(0.6063805305703175, -3)\n",
      "0.33955\n",
      "\n",
      "0.001\n",
      "(0.149331727567769, -3)\n",
      "0.26065\n",
      "\n",
      "0.01\n",
      "(0.149331727567769, -3)\n",
      "0.258725\n",
      "\n",
      "0.1\n",
      "(0.149331727567769, -3)\n",
      "0.2532\n",
      "\n",
      "1.0\n",
      "(0.149331727567769, -3)\n",
      "0.2602\n",
      "\n",
      "10.0\n",
      "(0.149331727567769, -3)\n",
      "0.2515\n",
      "\n",
      "100.0\n",
      "(0.149331727567769, -3)\n",
      "0.287075\n",
      "\n",
      "1000.0\n",
      "(0.149331727567769, -3)\n",
      "0.31225\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 20\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "te_errors=[]\n",
    "losses = []\n",
    "\n",
    "gammas = [(np.random.uniform(.05,.9), np.random.randint(-3,-1)) for k in range(10)]\n",
    "lambdas = np.logspace(-3, 3, num=7)\n",
    "\n",
    "for g in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "        w, loss = reg_logistic_regression_improved(tr_y, tr_tx, lambda_, initial_w, max_iters, gamma)        \n",
    "        tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "        va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "        tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "        va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "        tr_errors.append(tr_error)\n",
    "        va_errors.append(va_error)\n",
    "        print()\n",
    "        print(lambda_)\n",
    "        print(g)\n",
    "        print(va_error)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat in a smaller range of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.001\n",
      "(0.7811277484042665, -2)\n",
      "0.2201\n",
      "\n",
      "0.0215443469003\n",
      "(0.7811277484042665, -2)\n",
      "0.21995\n",
      "\n",
      "0.464158883361\n",
      "(0.7811277484042665, -2)\n",
      "0.22475\n",
      "\n",
      "10.0\n",
      "(0.7811277484042665, -2)\n",
      "0.233375\n",
      "\n",
      "0.001\n",
      "(0.6805780148797523, -2)\n",
      "0.23775\n",
      "\n",
      "0.0215443469003\n",
      "(0.6805780148797523, -2)\n",
      "0.22325\n",
      "\n",
      "0.464158883361\n",
      "(0.6805780148797523, -2)\n",
      "0.237525\n",
      "\n",
      "10.0\n",
      "(0.6805780148797523, -2)\n",
      "0.245475\n",
      "\n",
      "0.001\n",
      "(0.7382012836849072, -2)\n",
      "0.220675\n",
      "\n",
      "0.0215443469003\n",
      "(0.7382012836849072, -2)\n",
      "0.224975\n",
      "\n",
      "0.464158883361\n",
      "(0.7382012836849072, -2)\n",
      "0.215625\n",
      "\n",
      "10.0\n",
      "(0.7382012836849072, -2)\n",
      "0.253975\n",
      "\n",
      "0.001\n",
      "(0.7547460441047553, -2)\n",
      "0.215825\n",
      "\n",
      "0.0215443469003\n",
      "(0.7547460441047553, -2)\n",
      "0.22\n",
      "\n",
      "0.464158883361\n",
      "(0.7547460441047553, -2)\n",
      "0.22135\n",
      "\n",
      "10.0\n",
      "(0.7547460441047553, -2)\n",
      "0.238425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:15: RuntimeWarning: overflow encountered in exp\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:108: RuntimeWarning: overflow encountered in exp\n",
      "  # update w\n",
      "/home/lmon/repositories/03-ML/ml2017-proj/project1/scripts/auxiliary_functions.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  w_1 = w\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in less_equal\n",
      "/home/lmon/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.001\n",
      "(0.7024348800263576, -2)\n",
      "nan\n",
      "\n",
      "0.0215443469003\n",
      "(0.7024348800263576, -2)\n",
      "0.229125\n",
      "\n",
      "0.464158883361\n",
      "(0.7024348800263576, -2)\n",
      "0.2271\n",
      "\n",
      "10.0\n",
      "(0.7024348800263576, -2)\n",
      "0.242975\n",
      "\n",
      "0.001\n",
      "(0.6326542261454162, -2)\n",
      "0.220125\n",
      "\n",
      "0.0215443469003\n",
      "(0.6326542261454162, -2)\n",
      "0.2179\n",
      "\n",
      "0.464158883361\n",
      "(0.6326542261454162, -2)\n",
      "0.227325\n",
      "\n",
      "10.0\n",
      "(0.6326542261454162, -2)\n",
      "0.247\n",
      "\n",
      "0.001\n",
      "(0.6789481107794665, -2)\n",
      "0.222325\n",
      "\n",
      "0.0215443469003\n",
      "(0.6789481107794665, -2)\n",
      "0.2295\n",
      "\n",
      "0.464158883361\n",
      "(0.6789481107794665, -2)\n",
      "0.218\n",
      "\n",
      "10.0\n",
      "(0.6789481107794665, -2)\n",
      "0.24185\n",
      "\n",
      "0.001\n",
      "(0.7433874619669785, -2)\n",
      "nan\n",
      "\n",
      "0.0215443469003\n",
      "(0.7433874619669785, -2)\n",
      "0.222475\n",
      "\n",
      "0.464158883361\n",
      "(0.7433874619669785, -2)\n",
      "0.2296\n",
      "\n",
      "10.0\n",
      "(0.7433874619669785, -2)\n",
      "0.2443\n",
      "\n",
      "0.001\n",
      "(0.6991490849407433, -2)\n",
      "0.22505\n",
      "\n",
      "0.0215443469003\n",
      "(0.6991490849407433, -2)\n",
      "0.23825\n",
      "\n",
      "0.464158883361\n",
      "(0.6991490849407433, -2)\n",
      "0.22805\n",
      "\n",
      "10.0\n",
      "(0.6991490849407433, -2)\n",
      "0.257125\n",
      "\n",
      "0.001\n",
      "(0.5424729465111501, -2)\n",
      "0.22085\n",
      "\n",
      "0.0215443469003\n",
      "(0.5424729465111501, -2)\n",
      "0.228125\n",
      "\n",
      "0.464158883361\n",
      "(0.5424729465111501, -2)\n",
      "0.24155\n",
      "\n",
      "10.0\n",
      "(0.5424729465111501, -2)\n",
      "0.254025\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 40\n",
    "ws=[]\n",
    "tr_errors=[]\n",
    "te_errors=[]\n",
    "losses = []\n",
    "\n",
    "gammas = [(np.random.uniform(.5,.8), -2) for k in range(10)]\n",
    "lambdas = np.logspace(-3, 1, num=4)\n",
    "\n",
    "for g in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        gamma = adaptive_gamma(kappa=g[0], eta0=10**g[1])\n",
    "        w, loss = reg_logistic_regression_improved(tr_y, tr_tx, lambda_, initial_w, max_iters, gamma)        \n",
    "        tr_y_pred = predict_labels_bis(w, tr_tx, return_zeros=True)\n",
    "        va_y_pred = predict_labels_bis(w, va_tx, return_zeros=True)\n",
    "        tr_error = 1 - calculate_precision(tr_y, tr_y_pred, zeros_ones=True)\n",
    "        va_error = 1 - calculate_precision(va_y, va_y_pred, zeros_ones=True)\n",
    "        tr_errors.append(tr_error)\n",
    "        va_errors.append(va_error)\n",
    "        print()\n",
    "        print(lambda_)\n",
    "        print(g)\n",
    "        print(va_error)\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "initial_w = np.zeros((tr_tx.shape[1], 1))\n",
    "max_iters = 200\n",
    "gamma = adaptive_gamma(kappa=0.7, eta0=1e-2)\n",
    "lambda_= 0.1\n",
    "w, loss = reg_logistic_regression_improved(tr_y, tr_tx, lambda_, initial_w,\n",
    "                                           max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75394000000000005"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision(predict_labels_bis(w, te_tx, return_zeros=True), te_y, zeros_ones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75394000000000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w, test_poly_tx)\n",
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'rlri_submission_upd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression using k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    '''\n",
    "    Build k indices for k-fold.\n",
    "    '''\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def split_data_k_indices(y, x, k_indices, k):\n",
    "    '''\n",
    "    Splits the data into test and training data.\n",
    "    Samples get randomized through 'k_indices' and\n",
    "    selected through 'k'.\n",
    "    '''\n",
    "    mask = np.ones(k_indices.shape, dtype=bool)\n",
    "    mask[k] = False\n",
    "    \n",
    "    # Test data\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    # Train data\n",
    "    x_train = x[k_indices[mask]]\n",
    "    y_train = y[k_indices[mask]]\n",
    "    \n",
    "    return x_test, y_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    # tx_test = build_poly(x_test, degree)\n",
    "    # tx_train = build_poly(x_train, degree)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = ridge_regression(y_train, x_train, lambda_)\n",
    "    \n",
    "    prediction = {\n",
    "        'train': predict_labels(w_opt, x_train),\n",
    "        'test': predict_labels(w_opt, x_test)\n",
    "    } \n",
    "    \n",
    "    precision = {\n",
    "        'train': calculate_precision(prediction['train'], y_train),\n",
    "        'test': calculate_precision(prediction['test'], y_test)\n",
    "    }\n",
    "    \n",
    "    # Return loss for train and test data\n",
    "    return precision['train'], precision['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te, mse_dif):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.semilogx(lambds, mse_dif, marker=\".\", color='y', label='error delta')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 7\n",
    "k_fold = 3\n",
    "seed = 13\n",
    "\n",
    "lambdas = np.logspace(-40, 40, 30)\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "lambda_avg_rmse_trs = []\n",
    "lambda_avg_rmse_tes = []\n",
    "lambda_avg_rmse_difs = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    rmse_trs = []\n",
    "    rmse_tes = []\n",
    "    rmse_difs = []\n",
    "\n",
    "    # K-fold cross validation and pick the rmse 'test' and 'train'\n",
    "    # errors that represent the least absolute difference between\n",
    "    # them (for a given lambda).\n",
    "    for k in range(k_fold):\n",
    "        rmse_cur_tr, rmse_cur_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        \n",
    "        rmse_trs.append(rmse_cur_tr)\n",
    "        rmse_tes.append(rmse_cur_te)\n",
    "        rmse_difs.append(abs(rmse_cur_tr - rmse_cur_te))\n",
    "        \n",
    "    lambda_avg_rmse_trs.append(np.mean(rmse_trs))\n",
    "    lambda_avg_rmse_tes.append(np.mean(rmse_tes))\n",
    "    lambda_avg_rmse_difs.append(np.mean(rmse_difs))\n",
    "\n",
    "cross_validation_visualization(lambdas, lambda_avg_rmse_trs, lambda_avg_rmse_tes, lambda_avg_rmse_difs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w_opt, test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
