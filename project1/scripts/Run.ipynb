{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis = 0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis = 0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def de_standardize(x, mean_x, std_x):\n",
    "    \"\"\"Reverse the procedure of standardization.\"\"\"\n",
    "    x = x * std_x\n",
    "    x = x + mean_x\n",
    "    return x\n",
    "\n",
    "def build_poly(x, degree, offset=True):\n",
    "    \"\"\"Polynomial basis functions for input data x,\n",
    "    for up to a certain degree.\"\"\"\n",
    "    if offset:\n",
    "        rows, cols = np.indices((x.shape[0], degree+1))\n",
    "        tx = np.power(x[rows], cols)\n",
    "    else:\n",
    "        rows, cols = np.indices((x.shape[0], degree))\n",
    "        tx = np.power(x[rows], cols+1)\n",
    "    return tx\n",
    "\n",
    "def build_model_data(x, y):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form. Uses build_poly.\"\"\"\n",
    "    tx = np.c_[np.ones(len(y)), x]\n",
    "    return y, tx\n",
    "\n",
    "def split_data(x, y, ratio):\n",
    "    \"\"\"Split the dataset based on the split ratio.\n",
    "\n",
    "    E.g. if the ratio is 0.8 then 80% of the data set is dedicated\n",
    "    to training (and the rest dedicated to testing)\n",
    "    \"\"\"\n",
    "    size = int(ratio * x.shape[0])\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    training_idx, test_idx = indices[:size], indices[size:]\n",
    "\n",
    "    x_training, x_test = x[training_idx], x[test_idx]\n",
    "    y_training, y_test = y[training_idx], y[test_idx]\n",
    "\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculates the loss using MSE.\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    w = np.array(w).reshape(tx.shape[1], 1)\n",
    "    z = y - tx.dot(w)\n",
    "    z = z.T.dot(z)\n",
    "    return z[0][0] / tx.shape[0]\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t, with t being a one dim vector\"\"\"\n",
    "    t_exp = np.exp(t)\n",
    "    return t_exp / (t_exp + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y, y_pred):\n",
    "    correct = np.sum(np.abs(y + y_pred))/2\n",
    "    precision = correct / y.shape[0]\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(DATA_PATH+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we need to change the labels (we now need to do it for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones(len(yb))\n",
    "y[np.where(yb==-1)] = 0\n",
    "#y = yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize and add offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(input_data)\n",
    "y, tx = build_model_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_mean_x, test_std_x = standardize(test_input_data)\n",
    "test_y, test_tx = build_model_data(test_x, test_yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70% split and Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training, tx_val, y_training, y_val = split_data(tx, y, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Implements ridge regression using normal equations.\"\"\"\n",
    "    # create identity matrix\n",
    "    I = np.eye(tx.shape[1])\n",
    "    # compute w opt using normal equations\n",
    "    inv = np.linalg.inv(tx.T.dot(tx) + 2 * tx.shape[1] * lambda_ * I)\n",
    "    w_opt = inv.dot(tx.T).dot(y)\n",
    "    # compute ridge regression loss\n",
    "    loss = compute_mse(y, tx, w_opt) + lambda_ / tx.shape[1] * w_opt.T.dot(w_opt)\n",
    "    return w_opt, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-3, 2, num=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_tr = []\n",
    "precisions_te = []\n",
    "ws = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    w_opt, loss = ridge_regression(y, tx, lambda_)\n",
    "    y_pred_tr = predict_labels(w_opt, tx_training)\n",
    "    y_pred_te = predict_labels(w_opt, tx_val)\n",
    "    precision_tr = calculate_precision(y_training, y_pred_tr)\n",
    "    precision_te = calculate_precision(y_val, y_pred_te)\n",
    "    ws.append(w_opt)\n",
    "    precisions_tr.append(precision_tr)\n",
    "    precisions_te.append(precision_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(lambdas, precisions_te)\n",
    "plt.semilogx(lambdas, precisions_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose $\\lambda=10^{-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt, loss = ridge_regression(y, tx, 10**(-2))\n",
    "test_y_pred = predict_labels(w_opt, test_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'ridge_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 0.74454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70% split and SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training, tx_val, y_training, y_val = split_data(tx, y, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(y, tx, batch_size):\n",
    "    m = tx.shape[0]\n",
    "    p = np.random.permutation(np.arange(m))\n",
    "    tx_p, y_p = tx[p], y[p]\n",
    "    return y_p[:batch_size], tx_p[:batch_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_regr_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    tx_w = tx.dot(w)\n",
    "    # compute the cost\n",
    "    loss = np.sum(np.log(1 + np.exp(tx_w))) - np.sum(y * tx_w)\n",
    "    # compute the gradient\n",
    "    gradient = tx.T.dot(sigmoid(tx_w) - y)\n",
    "    # update w\n",
    "    w_1 = w\n",
    "    w = w_1 - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, return_all=False):\n",
    "    \"\"\"Logistic regression using mini-batch gradient descent\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "\n",
    "    batch_size = int(.1 * tx.shape[0])\n",
    "    \n",
    "    # init parameters    \n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        gamma_iter = gamma / ((n_iter + 1)**2)\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = log_regr_by_gradient_descent(y_n, tx_n, w, gamma_iter)\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tx_training.shape[1], 1))\n",
    "max_iters = 10000\n",
    "gamma = .00001\n",
    "w, loss = logistic_regression(y_training, tx_training, initial_w, max_iters, gamma, return_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_bis(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_te = predict_labels_bis(w, tx_val)\n",
    "y_aux = np.ones(len(y_pred_te))\n",
    "y_aux[np.where(y_pred_te==0)[0]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_te = calculate_precision(y_aux, y_pred_te.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017453333333333335"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w_opt, test_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "initial_w = np.zeros((x_training.shape[1], 1))\n",
    "max_iter = 10000\n",
    "gamma = .001\n",
    "lambda_ = .5\n",
    "w, loss = reg_logistic_regression(y_training, x_training, lambda_, initial_w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression using k-fold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "    rmse_test = np.sqrt(compute_mse(y_test, tx_test, w_opt))\n",
    "\n",
    "    # Return loss for train and test data\n",
    "    return rmse_train, rmse_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "degree = 7\n",
    "k_fold = 3\n",
    "lambda_ = 0\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, )\n",
    "\n",
    "# Ridge regression\n",
    "rmse_min_dif = float(\"inf\")\n",
    "rmse_min_dif_tr = 0\n",
    "rmse_min_dif_te = 0\n",
    "\n",
    "# K-fold cross validation and pick the rmse 'test' and 'train'\n",
    "# errors that represent the least absolute difference between\n",
    "# them (for a given lambda).\n",
    "for k in range(k_fold):\n",
    "    rmse_cur_tr, rmse_cur_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "    rmse_cur_dif = abs(rmse_cur_tr - rmse_cur_te)\n",
    "    if (rmse_cur_dif < rmse_min_dif):\n",
    "        rmse_min_dif = rmse_cur_dif\n",
    "        rmse_min_dif_tr = rmse_cur_tr\n",
    "        rmse_min_dif_te = rmse_cur_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w_opt, test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
