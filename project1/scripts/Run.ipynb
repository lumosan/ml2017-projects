{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis = 0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis = 0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def de_standardize(x, mean_x, std_x):\n",
    "    \"\"\"Reverse the procedure of standardization.\"\"\"\n",
    "    x = x * std_x\n",
    "    x = x + mean_x\n",
    "    return x\n",
    "\n",
    "def build_poly(x, degree, offset=True):\n",
    "    \"\"\"Polynomial basis functions for input data x,\n",
    "    for up to a certain degree.\"\"\"\n",
    "    if offset:\n",
    "        rows, cols = np.indices((x.shape[0], degree+1))\n",
    "        tx = np.power(x[rows], cols)\n",
    "    else:\n",
    "        rows, cols = np.indices((x.shape[0], degree))\n",
    "        tx = np.power(x[rows], cols+1)\n",
    "    return tx\n",
    "\n",
    "def build_model_data(x, y):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form. Uses build_poly.\"\"\"\n",
    "    tx = np.c_[np.ones(len(y)), x]\n",
    "    return y, tx\n",
    "\n",
    "def split_data(x, y, ratio):\n",
    "    \"\"\"Split the dataset based on the split ratio.\n",
    "\n",
    "    E.g. if the ratio is 0.8 then 80% of the data set is dedicated\n",
    "    to training (and the rest dedicated to testing)\n",
    "    \"\"\"\n",
    "    size = int(ratio * x.shape[0])\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    training_idx, test_idx = indices[:size], indices[size:]\n",
    "\n",
    "    x_training, x_test = x[training_idx], x[test_idx]\n",
    "    y_training, y_test = y[training_idx], y[test_idx]\n",
    "\n",
    "    return x_training, x_test, y_training, y_test\n",
    "\n",
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculates the loss using MSE.\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    w = np.array(w).reshape(tx.shape[1], 1)\n",
    "    z = y - tx.dot(w)\n",
    "    z = z.T.dot(z)\n",
    "    return z[0][0] / tx.shape[0]\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Apply sigmoid function on t, with t being a one dim vector\"\"\"\n",
    "    t_exp = np.exp(t)\n",
    "    return t_exp / (t_exp + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(y, y_pred):\n",
    "    correct = np.sum(np.abs(y + y_pred))/2\n",
    "    precision = correct / y.shape[0]\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(DATA_PATH+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we need to change the labels (we now need to do it for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y = np.ones(len(yb))\n",
    "#y[np.where(yb==-1)] = 0\n",
    "y = yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize and add offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(input_data)\n",
    "y, tx = build_model_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x, test_mean_x, test_std_x = standardize(test_input_data)\n",
    "test_y, test_tx = build_model_data(test_x, test_yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70% split and Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_training, tx_val, y_training, y_val = split_data(tx, y, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Implements ridge regression using normal equations.\"\"\"\n",
    "    # create identity matrix\n",
    "    I = np.eye(tx.shape[1])\n",
    "    # compute w opt using normal equations\n",
    "    inv = np.linalg.inv(tx.T.dot(tx) + 2 * tx.shape[1] * lambda_ * I)\n",
    "    w_opt = inv.dot(tx.T).dot(y)\n",
    "    # compute ridge regression loss\n",
    "    loss = compute_mse(y, tx, w_opt) + lambda_ / tx.shape[1] * w_opt.T.dot(w_opt)\n",
    "    return w_opt, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-3, 2, num=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precisions_tr = []\n",
    "precisions_te = []\n",
    "ws = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    w_opt, loss = ridge_regression(y, tx, lambda_)\n",
    "    y_pred_tr = predict_labels(w_opt, tx_training)\n",
    "    y_pred_te = predict_labels(w_opt, tx_val)\n",
    "    precision_tr = calculate_precision(y_training, y_pred_tr)\n",
    "    precision_te = calculate_precision(y_val, y_pred_te)\n",
    "    ws.append(w_opt)\n",
    "    precisions_tr.append(precision_tr)\n",
    "    precisions_te.append(precision_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(lambdas, precisions_te)\n",
    "plt.semilogx(lambdas, precisions_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose $\\lambda=10^{-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_opt, loss = ridge_regression(y, tx, 10**(-2))\n",
    "test_y_pred = predict_labels(w_opt, test_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'ridge_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 0.74454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70% split and SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_training, tx_val, y_training, y_val = split_data(tx, y, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(y, tx, batch_size):\n",
    "    m = tx.shape[0]\n",
    "    p = np.random.permutation(np.arange(m))\n",
    "    tx_p, y_p = tx[p], y[p]\n",
    "    return y_p[:batch_size], tx_p[:batch_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_regr_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    tx_w = tx.dot(w)\n",
    "    # compute the cost\n",
    "    loss = np.sum(np.log(1 + np.exp(tx_w))) - np.sum(y * tx_w)\n",
    "    # compute the gradient\n",
    "    gradient = tx.T.dot(sigmoid(tx_w) - y)\n",
    "    # update w\n",
    "    w_1 = w\n",
    "    w = w_1 - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, return_all=False):\n",
    "    \"\"\"Logistic regression using mini-batch gradient descent\"\"\"\n",
    "    if len(tx.shape) == 1:\n",
    "        tx = tx.reshape(-1, 1)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    initial_w = np.array(initial_w).reshape(tx.shape[1], 1)\n",
    "\n",
    "    batch_size = int(.1 * tx.shape[0])\n",
    "    \n",
    "    # init parameters    \n",
    "    w = initial_w\n",
    "    if return_all:\n",
    "        ws = [w]\n",
    "        losses = []\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        gamma_iter = gamma / ((n_iter + 1)**2)\n",
    "        # get mini-batch\n",
    "        y_n, tx_n = get_batch(y, tx, batch_size)\n",
    "        # get loss and update w by gradient\n",
    "        loss, w = log_regr_by_gradient_descent(y_n, tx_n, w, gamma_iter)\n",
    "        if return_all:\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            if n_iter % 100 == 0:\n",
    "                print(\"Current iteration={i}, loss={l}\".format(i=n_iter, l=loss))\n",
    "    # return w and loss, either all or only last ones\n",
    "    if return_all:\n",
    "        return ws, losses\n",
    "    else:\n",
    "        return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tx_training.shape[1], 1))\n",
    "max_iters = 10000\n",
    "gamma = .00001\n",
    "w, loss = logistic_regression(y_training, tx_training, initial_w, max_iters, gamma, return_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels_bis(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_te = predict_labels_bis(w, tx_val)\n",
    "y_aux = np.ones(len(y_pred_te))\n",
    "y_aux[np.where(y_pred_te==0)[0]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision_te = calculate_precision(y_aux, y_pred_te.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017453333333333335"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels_bis(w_opt, test_tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "initial_w = np.zeros((x_training.shape[1], 1))\n",
    "max_iter = 10000\n",
    "gamma = .001\n",
    "lambda_ = .5\n",
    "w, loss = reg_logistic_regression(y_training, x_training, lambda_, initial_w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression using k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    '''\n",
    "    Build k indices for k-fold.\n",
    "    '''\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def split_data_k_indices(y, x, k_indices, k):\n",
    "    '''\n",
    "    Splits the data into test and training data.\n",
    "    Samples get randomized through 'k_indices' and\n",
    "    selected through 'k'.\n",
    "    '''\n",
    "    mask = np.ones(k_indices.shape, dtype=bool)\n",
    "    mask[k] = False\n",
    "    \n",
    "    # Test data\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    # Train data\n",
    "    x_train = x[k_indices[mask]]\n",
    "    y_train = y[k_indices[mask]]\n",
    "    \n",
    "    return x_test, y_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    '''\n",
    "    Return the loss of ridge regression.\n",
    "    '''\n",
    "    # Split data according to 'k_indices' and 'k'\n",
    "    x_test, y_test, x_train, y_train = split_data_k_indices(y, x, k_indices, k)\n",
    "\n",
    "    # Form data with polynomial degree\n",
    "    # tx_test = build_poly(x_test, degree)\n",
    "    # tx_train = build_poly(x_train, degree)\n",
    "\n",
    "    # Apply ridge regression\n",
    "    w_opt, rmse_train = ridge_regression(y_train, x_train, lambda_)\n",
    "    \n",
    "    prediction = {\n",
    "        'train': predict_labels(w_opt, x_train),\n",
    "        'test': predict_labels(w_opt, x_test)\n",
    "    } \n",
    "    \n",
    "    precision = {\n",
    "        'train': calculate_precision(prediction['train'], y_train),\n",
    "        'test': calculate_precision(prediction['test'], y_test)\n",
    "    }\n",
    "    \n",
    "    # Return loss for train and test data\n",
    "    return precision['train'], precision['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te, mse_dif):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.semilogx(lambds, mse_dif, marker=\".\", color='y', label='error delta')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71904287617150464, 0.71858087432349727]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71903087612350447, 0.71858687434749735]\n",
      "[0.720374881499526, 0.71703886815547258, 0.71826287305149217]\n",
      "[0.71779487117948471, 0.71903087612350447, 0.71859287437149744]\n",
      "[0.72036288145152583, 0.71703886815547258, 0.718250873003492]\n",
      "[0.71775287101148399, 0.7190188760755043, 0.71862887451549806]\n",
      "[0.72038688154752617, 0.71706286825147303, 0.71827487309949245]\n",
      "[0.71781287125148496, 0.71911487645950589, 0.71866487465949869]\n",
      "[0.720374881499526, 0.7170748682994732, 0.71817887271549086]\n",
      "[0.71718286873147497, 0.71853287413149658, 0.71830487321949288]\n",
      "[0.72005088020352082, 0.71606686426745703, 0.71787887151548602]\n",
      "[0.71622886491545967, 0.71771687086748348, 0.71727286909147636]\n",
      "[0.71958287833151335, 0.71546686186744746, 0.7173028692114769]\n",
      "[0.66081864327457307, 0.66172464689858757, 0.66096264385057535]\n",
      "[0.66223464893859574, 0.65949863799455199, 0.6613466453865815]\n",
      "[0.63838455353821411, 0.63900855603422413, 0.639248556994228]\n",
      "[0.63962655850623407, 0.63833055332221333, 0.63866655466621869]\n",
      "[0.63842655370621482, 0.63903855615422467, 0.63927855711422843]\n",
      "[0.63969855879423521, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n",
      "[0.63842655370621482, 0.63904455617822475, 0.63927855711422843]\n",
      "[0.63968655874623503, 0.63835455341821368, 0.63871455485821949]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrYssDNxR1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1314b943cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree = 7\n",
    "k_fold = 3\n",
    "seed = 13\n",
    "\n",
    "lambdas = np.logspace(-40, 40, 30)\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "lambda_avg_rmse_trs = []\n",
    "lambda_avg_rmse_tes = []\n",
    "lambda_avg_rmse_difs = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    rmse_trs = []\n",
    "    rmse_tes = []\n",
    "    rmse_difs = []\n",
    "\n",
    "    # K-fold cross validation and pick the rmse 'test' and 'train'\n",
    "    # errors that represent the least absolute difference between\n",
    "    # them (for a given lambda).\n",
    "    for k in range(k_fold):\n",
    "        rmse_cur_tr, rmse_cur_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "        \n",
    "        rmse_trs.append(rmse_cur_tr)\n",
    "        rmse_tes.append(rmse_cur_te)\n",
    "        rmse_difs.append(abs(rmse_cur_tr - rmse_cur_te))\n",
    "        \n",
    "    lambda_avg_rmse_trs.append(np.mean(rmse_trs))\n",
    "    lambda_avg_rmse_tes.append(np.mean(rmse_tes))\n",
    "    lambda_avg_rmse_difs.append(np.mean(rmse_difs))\n",
    "\n",
    "cross_validation_visualization(lambdas, lambda_avg_rmse_trs, lambda_avg_rmse_tes, lambda_avg_rmse_difs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_yb, test_input_data, test_ids = load_csv_data(DATA_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_pred = predict_labels(w_opt, test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, test_y_pred, DATA_PATH+'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
