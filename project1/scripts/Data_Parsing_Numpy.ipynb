{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IN_TEST_DATA_PATH = '../data/test.csv'\n",
    "IN_TRAIN_DATA_PATH = '../data/train.csv'\n",
    "\n",
    "OUT_TEST_DATA_PATH = '../data/test_fixed.csv'\n",
    "OUT_TRAIN_DATA_PATH = '../data/train_fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrame:\n",
    "    '''\n",
    "    This class is used as a data-container,\n",
    "    representeing column-organized information\n",
    "    read from csv files.\n",
    "    '''\n",
    "    column_labels = {}\n",
    "    data = None\n",
    "    \n",
    "    def __init__(self, csv_path=None):\n",
    "        \n",
    "        # Leave the labels and data empty if the path is None\n",
    "        if csv_path is None:\n",
    "            return\n",
    "        \n",
    "        temp_data = None\n",
    "        \n",
    "        with open(csv_path) as csv_file:\n",
    "            csv_reader =  csv.reader(csv_file)\n",
    "            n_rows = sum(1 for row in csv_reader)\n",
    "        \n",
    "            # Reset reader's head pointer\n",
    "            csv_file.seek(0)\n",
    "            \n",
    "            for row_idx, row in enumerate(csv_reader):\n",
    "                if (row_idx == 0):\n",
    "                    \n",
    "                    # Fill in dictionary with (column_name:column_index)\n",
    "                    for column_idx, column_label in enumerate(row):\n",
    "                        self.column_labels[column_label] = column_idx\n",
    "                    temp_data = [[0 for x in range(n_rows-1)] for y in range(len(self.column_labels))]\n",
    "                else:\n",
    "                    \n",
    "                    # Fill data in a column-oriented fashion\n",
    "                    for column_idx, column_value in enumerate(row):\n",
    "                        temp_data[column_idx][row_idx-1] = column_value\n",
    "        \n",
    "        # Store all the data into an 'ndarray'\n",
    "        self.data = np.array(temp_data)\n",
    "    \n",
    "    # targets have to be labels, not indices\n",
    "    def get_columns(self, targets=None):\n",
    "        '''\n",
    "        Returns a copy the desired columns' data as a \n",
    "        list of Column objects.\n",
    "        '''\n",
    "        columns = []\n",
    "        \n",
    "        if targets is not None:\n",
    "            if all(isinstance(label, str) for label in targets):\n",
    "                columns = [Column(label, self.data[self.column_labels[label],:]) for label in targets]\n",
    "        else:\n",
    "            columns = [Column(label, self.data[self.column_labels[label],:]) for label in self.column_labels]\n",
    "            \n",
    "        return columns\n",
    "    \n",
    "    # 'at' cannot have repeated values\n",
    "    def insert(self, columns, at):\n",
    "        '''\n",
    "        Returns a new DataFrame with newly\n",
    "        inserted columns, at a designated index.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        \n",
    "        # Zip columns and their desired indexes together for iteration\n",
    "        col_idx_zipped = zip(columns, at)\n",
    "        col_idx_zipped.sort(key = lambda t: t[1])\n",
    "        \n",
    "        # Create a new dictionary of labels to append\n",
    "        new_labels = dict(col_idx_zipped)\n",
    "        \n",
    "        # Insert the columns' data\n",
    "        for column, idx in col_idx_zipped:\n",
    "            dataframe_clone.data = np.insert(dataframe_clone.data, idx, column.values, 0)\n",
    "        \n",
    "        # Apply index offset where needed\n",
    "        for column, idx in col_idx_zipped:\n",
    "            offset_column_labels = {}\n",
    "            for k, v in dataframe_clone.column_labels.items():\n",
    "                offset_column_labels[k] = (v + 1) if (v >= idx) else v\n",
    "            dataframe_clone.column_labels = offset_column_labels\n",
    "           \n",
    "        # Append the new labels to previously existing ones\n",
    "        dataframe_clone.column_labels = dict(dataframe_clone.column_labels, new_labels)\n",
    "        \n",
    "        return dataframe_clone\n",
    "    \n",
    "    # target_axix=0 is columns, target_axix=1 is rows\n",
    "    def drop(self, targets, target_axis=0):\n",
    "        '''\n",
    "        Returns a new DataFrame without the\n",
    "        dropped columns/rows.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        dropable_indexes = []\n",
    "        \n",
    "        # All elements in the list are indexes\n",
    "        if all(isinstance(index, int) for index in targets):\n",
    "            dropable_indexes = targets\n",
    "            dropable_keys = [key for key in dataframe_clone.column_labels if dataframe_clone.column_labels[key] in targets]\n",
    "            dataframe_clone.data = np.delete(dataframe_clone.data, [dataframe_clone.column_labels.pop(label) for label in dropable_keys], axis=target_axis)\n",
    "        \n",
    "        # All elements in the list are labels\n",
    "        elif all(isinstance(label, str) for label in targets) and target_axis == 0:\n",
    "            dropable_indexes = [self.column_labels[label] for label in targets]\n",
    "            dataframe_clone.data = np.delete(dataframe_clone.data, [dataframe_clone.column_labels.pop(label) for label in targets], axis=target_axis)\n",
    "        \n",
    "        # Non-expected parameters, just return the whole DataFrame\n",
    "        else:\n",
    "            return dataframe_clone\n",
    "        \n",
    "        # Apply index offset where needed\n",
    "        dropable_indexes.sort(reverse=True)\n",
    "        for idx in dropable_indexes:\n",
    "            offset_column_labels = {}\n",
    "            for k, v in dataframe_clone.column_labels.items():\n",
    "                offset_column_labels[k] = (v - 1) if (v >= idx) else v\n",
    "            dataframe_clone.column_labels = copy.deepcopy(offset_column_labels)\n",
    "        \n",
    "        return dataframe_clone\n",
    "    \n",
    "    def replace(self, existing_value, new_value):\n",
    "        '''\n",
    "        Replaces all occurrences of a given value,\n",
    "        in the DataFrame, witha new specified value.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        if np.isnan(existing_value):\n",
    "            dataframe_clone.data[np.isnan(dataframe_clone.data)] = new_value\n",
    "        else:\n",
    "            dataframe_clone.data[dataframe_clone.data == existing_value] = new_value\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def mean(self):\n",
    "        '''\n",
    "        Returns a new DataFrame with the columns'\n",
    "        mean values.\n",
    "        '''\n",
    "        mean_df = DataFrame()\n",
    "        mean_df.column_labels = copy.deepcopy(self.column_labels)\n",
    "        \n",
    "        columns = self.get_columns()\n",
    "        \n",
    "        temp_data = [[0 for x in range(1)] for y in range(len(self.column_labels))]\n",
    "        for column in columns:\n",
    "            temp_data[self.column_labels[column.label]][0] = column.mean()\n",
    "            \n",
    "        mean_df.data = np.array(temp_data)\n",
    "        \n",
    "        return mean_df\n",
    "    \n",
    "    def std(self):\n",
    "        '''\n",
    "        Returns a new DataFrame with the columns'\n",
    "        stadard deviation values.\n",
    "        '''\n",
    "        std_df = DataFrame()\n",
    "        std_df.column_labels = copy.deepcopy(self.column_labels)\n",
    "        \n",
    "        columns = self.get_columns()\n",
    "        \n",
    "        temp_data = [[0 for x in range(1)] for y in range(len(self.column_labels))]\n",
    "        for column in columns:\n",
    "            temp_data[self.column_labels[column.label]][0] = column.std()\n",
    "            \n",
    "        std_df.data = np.array(temp_data)\n",
    "        \n",
    "        return std_df\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Returns a DataFrame with column-based\n",
    "        normalization.\n",
    "        '''\n",
    "        normalized_df = DataFrame()\n",
    "        normalized_df.column_labels = copy.deepcopy(self.column_labels)\n",
    "        \n",
    "        temp_data = [[0 for x in range(len(self.data[0,:]))] for y in range(len(self.column_labels))]\n",
    "        for column in self.get_columns():\n",
    "            temp_data[normalized_df.column_labels[column.label]] = column.normalize().values\n",
    "        \n",
    "        normalized_df.data = np.array(temp_data)\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def corr(self):\n",
    "        '''\n",
    "        Returns a DataFrame with the correlation\n",
    "        coeficients between all of the columns.\n",
    "        '''\n",
    "        corr_df = DataFrame()\n",
    "        corr_df.column_labels = copy.deepcopy(self.column_labels)\n",
    "        corr_df.data = np.corrcoef(self.data)\n",
    "        \n",
    "        return corr_df\n",
    "    \n",
    "    def set_type(self, target_type):\n",
    "        '''\n",
    "        Attempts to change the DataFrame's data\n",
    "        type to a single type (target_type). The\n",
    "        returned DataFrame is a copy of 'self'.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        dataframe_clone.data = dataframe_clone.data.astype(target_type)\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def __clone(self):\n",
    "        '''\n",
    "        Creates and returns a clone of the current\n",
    "        DataFrame object (creating a deep copy of\n",
    "        all its components).\n",
    "        '''\n",
    "        dataframe_clone = copy.deepcopy(self)\n",
    "        dataframe_clone.column_labels = copy.deepcopy(self.column_labels)\n",
    "        dataframe_clone.data = copy.deepcopy(self.data)\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        Default class' representation.\n",
    "        '''\n",
    "        return str(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Default class' string representation.\n",
    "        '''\n",
    "        max_columns = min(6, len(self.column_labels.keys()))\n",
    "        max_rows = min(8, len(self.data[0,:]))\n",
    "        max_string_size = 13\n",
    "        final_string = '| '\n",
    "        \n",
    "        # Add the schema to the top\n",
    "        for idx, label in enumerate(self.column_labels.keys()):\n",
    "            if (idx < max_columns):\n",
    "                label_rep = label if (len(label) <= max_string_size) else label[:max_string_size-3] + '...'\n",
    "                final_string += label_rep.rjust(max_string_size) + ' | '\n",
    "            elif (idx == max_columns):\n",
    "                final_string += '...'\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        final_string += '\\n'\n",
    "        final_string += '-' * (max_string_size * max_columns + (max_columns + 1) * 3)\n",
    "        \n",
    "        # Add the first rows as preview\n",
    "        for i in range(max_rows):\n",
    "            final_string += '\\n| '\n",
    "            for idx, value in enumerate(self.data[:,i]):\n",
    "                if (idx < max_columns):\n",
    "                    value_rep = str(value) if (len(str(value)) <= max_string_size) else str(value)[:max_string_size-3] + '...'\n",
    "                    final_string += value_rep.rjust(max_string_size) + ' | '\n",
    "                elif (idx == max_columns):\n",
    "                    final_string += '...'\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        if (len(self.data[0,:]) > max_rows):\n",
    "            final_string += '\\n(...)\\n'\n",
    "        else:\n",
    "            final_string += '\\n'\n",
    "        \n",
    "        return final_string\n",
    "    \n",
    "class Column:\n",
    "    '''\n",
    "    This class is meant as single column's\n",
    "    data representation.\n",
    "    '''\n",
    "    label = None\n",
    "    values = None\n",
    "    \n",
    "    def __init__(self, label, values):\n",
    "        self.label = label\n",
    "        self.values = values\n",
    "        \n",
    "    def mean(self):\n",
    "        '''\n",
    "        Calculates the column's values mean,\n",
    "        while ignoring NaNs.\n",
    "        '''\n",
    "        return np.nanmean(self.values)\n",
    "    \n",
    "    def std(self):\n",
    "        '''\n",
    "        Calculates the column's values stadrad\n",
    "        deviation, while ignoring NaNs.\n",
    "        '''\n",
    "        return np.nanstd(self.values)\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Returns a column with normalized values.\n",
    "        '''\n",
    "        column_clone = self.__clone()\n",
    "        column_clone.values = column_clone.values - column_clone.mean()\n",
    "        column_clone.values = column_clone.values / column_clone.std()\n",
    "        return column_clone\n",
    "    \n",
    "    def nonan(self):\n",
    "        '''\n",
    "        Returns an Column with all the non NaN\n",
    "        values.\n",
    "        '''\n",
    "        column_clone = self.__clone()\n",
    "        column_clone.values = column_clone.values[~np.isnan(column_clone.values)]\n",
    "        return column_clone\n",
    "    \n",
    "    def __clone(self):\n",
    "        column_clone = copy.deepcopy(self)\n",
    "        column_clone.label = self.label\n",
    "        column_clone.values = copy.deepcopy(self.values)\n",
    "        return column_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = DataFrame(IN_TEST_DATA_PATH)\n",
    "train_df = DataFrame(IN_TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping indexes: [1, 0]\n",
      "Adjusting 1...\n",
      "Result: {'DER_mass_MMC': 1, 'DER_mass_transverse_met_lep': 2, 'DER_mass_vis': 3, 'DER_pt_h': 4, 'DER_deltaeta_jet_jet': 5, 'DER_mass_jet_jet': 6, 'DER_prodeta_jet_jet': 7, 'DER_deltar_tau_lep': 8, 'DER_pt_tot': 9, 'DER_sum_pt': 10, 'DER_pt_ratio_lep_tau': 11, 'DER_met_phi_centrality': 12, 'DER_lep_eta_centrality': 13, 'PRI_tau_pt': 14, 'PRI_tau_eta': 15, 'PRI_tau_phi': 16, 'PRI_lep_pt': 17, 'PRI_lep_eta': 18, 'PRI_lep_phi': 19, 'PRI_met': 20, 'PRI_met_phi': 21, 'PRI_met_sumet': 22, 'PRI_jet_num': 23, 'PRI_jet_leading_pt': 24, 'PRI_jet_leading_eta': 25, 'PRI_jet_leading_phi': 26, 'PRI_jet_subleading_pt': 27, 'PRI_jet_subleading_eta': 28, 'PRI_jet_subleading_phi': 29, 'PRI_jet_all_pt': 30}\n",
      "Adjusting 0...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltaeta_jet_jet': 4, 'DER_mass_jet_jet': 5, 'DER_prodeta_jet_jet': 6, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'DER_lep_eta_centrality': 12, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_leading_pt': 23, 'PRI_jet_leading_eta': 24, 'PRI_jet_leading_phi': 25, 'PRI_jet_subleading_pt': 26, 'PRI_jet_subleading_eta': 27, 'PRI_jet_subleading_phi': 28, 'PRI_jet_all_pt': 29}\n",
      "Dropping indexes: [1, 0]\n",
      "Adjusting 1...\n",
      "Result: {'DER_mass_MMC': 1, 'DER_mass_transverse_met_lep': 2, 'DER_mass_vis': 3, 'DER_pt_h': 4, 'DER_deltaeta_jet_jet': 5, 'DER_mass_jet_jet': 6, 'DER_prodeta_jet_jet': 7, 'DER_deltar_tau_lep': 8, 'DER_pt_tot': 9, 'DER_sum_pt': 10, 'DER_pt_ratio_lep_tau': 11, 'DER_met_phi_centrality': 12, 'DER_lep_eta_centrality': 13, 'PRI_tau_pt': 14, 'PRI_tau_eta': 15, 'PRI_tau_phi': 16, 'PRI_lep_pt': 17, 'PRI_lep_eta': 18, 'PRI_lep_phi': 19, 'PRI_met': 20, 'PRI_met_phi': 21, 'PRI_met_sumet': 22, 'PRI_jet_num': 23, 'PRI_jet_leading_pt': 24, 'PRI_jet_leading_eta': 25, 'PRI_jet_leading_phi': 26, 'PRI_jet_subleading_pt': 27, 'PRI_jet_subleading_eta': 28, 'PRI_jet_subleading_phi': 29, 'PRI_jet_all_pt': 30}\n",
      "Adjusting 0...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltaeta_jet_jet': 4, 'DER_mass_jet_jet': 5, 'DER_prodeta_jet_jet': 6, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'DER_lep_eta_centrality': 12, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_leading_pt': 23, 'PRI_jet_leading_eta': 24, 'PRI_jet_leading_phi': 25, 'PRI_jet_subleading_pt': 26, 'PRI_jet_subleading_eta': 27, 'PRI_jet_subleading_phi': 28, 'PRI_jet_all_pt': 29}\n"
     ]
    }
   ],
   "source": [
    "# Store 'Id' and 'Prediction' columns\n",
    "test_id_column, test_prediction_column = test_df.get_columns(['Id', 'Prediction'])\n",
    "trrain_id_column, train_prediction_column = train_df.get_columns(['Id', 'Prediction'])\n",
    "\n",
    "# Drop 'Id' and 'Prediction' columns and replace '-999' with 'NaN'\n",
    "test_df = test_df.drop(['Id', 'Prediction']).set_type(float).replace(-999.0, float('NaN'))\n",
    "train_df = train_df.drop(['Id', 'Prediction']).set_type(float).replace(-999.0, float('NaN'))\n",
    "\n",
    "# Create correlation matrices\n",
    "test_corr_df = test_df.replace(float('NaN'), 0.0).corr()\n",
    "train_corr_df = train_df.replace(float('NaN'), 0.0).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.replace(float('NaN'), 0.0).corr().get_columns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|  DER_mass_MMC | DER_mass_t... |  DER_mass_vis |      DER_pt_h | DER_deltae... | DER_mass_j... | ...\n",
       "---------------------------------------------------------------------------------------------------\n",
       "| 121.871729343 | 49.2583872444 | 81.1223376772 | 57.8290937019 | 2.40501628365 | 372.355428652 | ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|  DER_mass_MMC | DER_mass_t... |  DER_mass_vis |      DER_pt_h | DER_deltae... | DER_mass_j... | ...\n",
       "---------------------------------------------------------------------------------------------------\n",
       "| 56.7853497841 |  35.393433862 | 40.4739995151 | 63.3043943928 |  1.7426863849 | 398.470091756 | ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping indexes: [28, 27, 26, 25, 24, 23, 12, 6, 5, 4]\n",
      "Adjusting 28...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 28}\n",
      "Adjusting 27...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 27}\n",
      "Adjusting 26...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 26}\n",
      "Adjusting 25...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 25}\n",
      "Adjusting 24...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 24}\n",
      "Adjusting 23...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 23}\n",
      "Adjusting 12...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 12, 'PRI_tau_eta': 13, 'PRI_tau_phi': 14, 'PRI_lep_pt': 15, 'PRI_lep_eta': 16, 'PRI_lep_phi': 17, 'PRI_met': 18, 'PRI_met_phi': 19, 'PRI_met_sumet': 20, 'PRI_jet_num': 21, 'PRI_jet_all_pt': 22}\n",
      "Adjusting 6...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 6, 'DER_pt_tot': 7, 'DER_sum_pt': 8, 'DER_pt_ratio_lep_tau': 9, 'DER_met_phi_centrality': 10, 'PRI_tau_pt': 11, 'PRI_tau_eta': 12, 'PRI_tau_phi': 13, 'PRI_lep_pt': 14, 'PRI_lep_eta': 15, 'PRI_lep_phi': 16, 'PRI_met': 17, 'PRI_met_phi': 18, 'PRI_met_sumet': 19, 'PRI_jet_num': 20, 'PRI_jet_all_pt': 21}\n",
      "Adjusting 5...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 5, 'DER_pt_tot': 6, 'DER_sum_pt': 7, 'DER_pt_ratio_lep_tau': 8, 'DER_met_phi_centrality': 9, 'PRI_tau_pt': 10, 'PRI_tau_eta': 11, 'PRI_tau_phi': 12, 'PRI_lep_pt': 13, 'PRI_lep_eta': 14, 'PRI_lep_phi': 15, 'PRI_met': 16, 'PRI_met_phi': 17, 'PRI_met_sumet': 18, 'PRI_jet_num': 19, 'PRI_jet_all_pt': 20}\n",
      "Adjusting 4...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 4, 'DER_pt_tot': 5, 'DER_sum_pt': 6, 'DER_pt_ratio_lep_tau': 7, 'DER_met_phi_centrality': 8, 'PRI_tau_pt': 9, 'PRI_tau_eta': 10, 'PRI_tau_phi': 11, 'PRI_lep_pt': 12, 'PRI_lep_eta': 13, 'PRI_lep_phi': 14, 'PRI_met': 15, 'PRI_met_phi': 16, 'PRI_met_sumet': 17, 'PRI_jet_num': 18, 'PRI_jet_all_pt': 19}\n",
      "Dropping indexes: [28, 27, 26, 25, 24, 23, 12, 6, 5, 4]\n",
      "Adjusting 28...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 28}\n",
      "Adjusting 27...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 27}\n",
      "Adjusting 26...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 26}\n",
      "Adjusting 25...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 25}\n",
      "Adjusting 24...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 24}\n",
      "Adjusting 23...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 13, 'PRI_tau_eta': 14, 'PRI_tau_phi': 15, 'PRI_lep_pt': 16, 'PRI_lep_eta': 17, 'PRI_lep_phi': 18, 'PRI_met': 19, 'PRI_met_phi': 20, 'PRI_met_sumet': 21, 'PRI_jet_num': 22, 'PRI_jet_all_pt': 23}\n",
      "Adjusting 12...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 7, 'DER_pt_tot': 8, 'DER_sum_pt': 9, 'DER_pt_ratio_lep_tau': 10, 'DER_met_phi_centrality': 11, 'PRI_tau_pt': 12, 'PRI_tau_eta': 13, 'PRI_tau_phi': 14, 'PRI_lep_pt': 15, 'PRI_lep_eta': 16, 'PRI_lep_phi': 17, 'PRI_met': 18, 'PRI_met_phi': 19, 'PRI_met_sumet': 20, 'PRI_jet_num': 21, 'PRI_jet_all_pt': 22}\n",
      "Adjusting 6...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 6, 'DER_pt_tot': 7, 'DER_sum_pt': 8, 'DER_pt_ratio_lep_tau': 9, 'DER_met_phi_centrality': 10, 'PRI_tau_pt': 11, 'PRI_tau_eta': 12, 'PRI_tau_phi': 13, 'PRI_lep_pt': 14, 'PRI_lep_eta': 15, 'PRI_lep_phi': 16, 'PRI_met': 17, 'PRI_met_phi': 18, 'PRI_met_sumet': 19, 'PRI_jet_num': 20, 'PRI_jet_all_pt': 21}\n",
      "Adjusting 5...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 5, 'DER_pt_tot': 6, 'DER_sum_pt': 7, 'DER_pt_ratio_lep_tau': 8, 'DER_met_phi_centrality': 9, 'PRI_tau_pt': 10, 'PRI_tau_eta': 11, 'PRI_tau_phi': 12, 'PRI_lep_pt': 13, 'PRI_lep_eta': 14, 'PRI_lep_phi': 15, 'PRI_met': 16, 'PRI_met_phi': 17, 'PRI_met_sumet': 18, 'PRI_jet_num': 19, 'PRI_jet_all_pt': 20}\n",
      "Adjusting 4...\n",
      "Result: {'DER_mass_MMC': 0, 'DER_mass_transverse_met_lep': 1, 'DER_mass_vis': 2, 'DER_pt_h': 3, 'DER_deltar_tau_lep': 4, 'DER_pt_tot': 5, 'DER_sum_pt': 6, 'DER_pt_ratio_lep_tau': 7, 'DER_met_phi_centrality': 8, 'PRI_tau_pt': 9, 'PRI_tau_eta': 10, 'PRI_tau_phi': 11, 'PRI_lep_pt': 12, 'PRI_lep_eta': 13, 'PRI_lep_phi': 14, 'PRI_met': 15, 'PRI_met_phi': 16, 'PRI_met_sumet': 17, 'PRI_jet_num': 18, 'PRI_jet_all_pt': 19}\n"
     ]
    }
   ],
   "source": [
    "# Get NaN count for each column\n",
    "nan_count = {}\n",
    "for column in test_df.get_columns():\n",
    "    nan_count[column.label] = len(column.values) - len(column.nonan().values)\n",
    "    \n",
    "# Get columns sorted by NaN count\n",
    "sorted_nan_count = sorted(nan_count, key=nan_count.get)\n",
    "\n",
    "# Pick 1/3 worst columns to remove\n",
    "target_columns = sorted_nan_count[int(2 * len(sorted_nan_count) / 3):]\n",
    "\n",
    "# Drop columns from DataFrames\n",
    "test_df = test_df.drop(target_columns)\n",
    "train_df = train_df.drop(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(test_df.column_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing dataframes\n",
    "test_df = test_df.normalize()\n",
    "train_df = train_df.normalize()\n",
    "\n",
    "# Replacing NaNs with 0s\n",
    "test_df = test_df.replace(float('NaN'), 0.0)\n",
    "train_df = train_df.replace(float('NaN'), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix re-indexing. Use accumulating array...\n",
    "\n",
    "dropping [0, 1, 4] means [1, 2, 2, 2, 3, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "    1. Remove 1/3 of the columns (the ones with the most NaNs)\n",
    "    2. Fill in the data from the most correlated columns (above a certain coeficient)\n",
    "    3. Normalize the data (subtract the mean and divide by the stddev)\n",
    "    4. Fill in the rest with 0s (since that is the mean of the normalized data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
