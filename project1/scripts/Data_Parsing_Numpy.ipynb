{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IN_TEST_DATA_PATH = '../data/test.csv'\n",
    "IN_TRAIN_DATA_PATH = '../data/train.csv'\n",
    "\n",
    "OUT_TEST_DATA_PATH = '../data/test_fixed.csv'\n",
    "OUT_TRAIN_DATA_PATH = '../data/train_fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrame:\n",
    "    '''\n",
    "    This class is used as a data-container,\n",
    "    representeing column-organized information\n",
    "    read from csv files.\n",
    "    '''\n",
    "    column_labels = {}\n",
    "    data = None\n",
    "    \n",
    "    def __init__(self, csv_path=None):\n",
    "        \n",
    "        # Leave the labels and data empty if the path is None\n",
    "        if csv_path is None:\n",
    "            return\n",
    "        \n",
    "        temp_data = None\n",
    "        \n",
    "        with open(csv_path) as csv_file:\n",
    "            csv_reader =  csv.reader(csv_file)\n",
    "            n_rows = sum(1 for row in csv_reader)\n",
    "        \n",
    "            # Reset reader's head pointer\n",
    "            csv_file.seek(0)\n",
    "            \n",
    "            for row_idx, row in enumerate(csv_reader):\n",
    "                if (row_idx == 0):\n",
    "                    \n",
    "                    # Fill in dictionary with (column_name:column_index)\n",
    "                    for column_idx, column_label in enumerate(row):\n",
    "                        self.column_labels[column_label] = column_idx\n",
    "                    temp_data = [[0 for x in range(n_rows-1)] for y in range(len(self.column_labels))]\n",
    "                else:\n",
    "                    \n",
    "                    # Fill data in a column-oriented fashion\n",
    "                    for column_idx, column_value in enumerate(row):\n",
    "                        temp_data[column_idx][row_idx-1] = column_value\n",
    "        \n",
    "        # Store all the data into an 'ndarray'\n",
    "        self.data = np.array(temp_data)\n",
    "    \n",
    "    # targets have to be labels, not indices\n",
    "    def get_columns(self, targets):\n",
    "        '''\n",
    "        Returns a copy the desired columns' data as a \n",
    "        list of Column objects.\n",
    "        '''\n",
    "        columns = []\n",
    "        \n",
    "        if all(isinstance(label, str) for label in targets):\n",
    "            columns = [Column(label, self.data[self.column_labels[label],:]) for label in targets]\n",
    "            \n",
    "        return columns\n",
    "    \n",
    "    # target_axix=0 is columns, target_axix=1 is rows\n",
    "    def drop(self, targets, target_axis=0):\n",
    "        '''\n",
    "        Returns a new DataFrame without the\n",
    "        dropped columns/rows.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        dataframe_offset = len(targets)\n",
    "        \n",
    "        # All elements in the list are indexes\n",
    "        if all(isinstance(index, int) for index in targets):\n",
    "            dropable_keys = [key for key in dataframe_clone.column_labels if dataframe_clone.column_labels[key] in targets]\n",
    "            dataframe_clone.data = np.delete(dataframe_clone.data, [dataframe_clone.column_labels.pop(label) for label in dropable_keys], axis=target_axis)\n",
    "            dataframe_clone.column_labels = {k: v-dataframe_offset for k, v in dataframe_clone.items()}\n",
    "        \n",
    "        # All elements in the list are labels\n",
    "        elif all(isinstance(label, str) for label in targets) and target_axis == 0:\n",
    "            dataframe_clone.data = np.delete(dataframe_clone.data, [dataframe_clone.column_labels.pop(label) for label in targets], axis=target_axis)\n",
    "            dataframe_clone.column_labels = {k: v-dataframe_offset for k, v in dataframe_clone.column_labels.items()}\n",
    "        \n",
    "        return dataframe_clone\n",
    "    \n",
    "    def replace(self, existing_value, new_value):\n",
    "        '''\n",
    "        Replaces all occurrences of a given value,\n",
    "        in the DataFrame, witha new specified value.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        if np.isnan(existing_value):\n",
    "            dataframe_clone.data[np.isnan(dataframe_clone.data)] = new_value\n",
    "        else:\n",
    "            dataframe_clone.data[dataframe_clone.data == existing_value] = new_value\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def corr(self):\n",
    "        '''\n",
    "        Returns a 2D matrix with the correlation\n",
    "        coeficients between all of the columns.\n",
    "        '''\n",
    "        corr_df = DataFrame()\n",
    "        corr_df.column_labels = copy.deepcopy(self.column_labels)\n",
    "        corr_df.data = np.corrcoef(self.data)\n",
    "        \n",
    "        return corr_df\n",
    "    \n",
    "    def set_type(self, target_type):\n",
    "        '''\n",
    "        Attempts to change the DataFrame's data\n",
    "        type to a single type (target_type). The\n",
    "        returned DataFrame is a copy of 'self'.\n",
    "        '''\n",
    "        dataframe_clone = self.__clone()\n",
    "        dataframe_clone.data = dataframe_clone.data.astype(target_type)\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def __clone(self):\n",
    "        '''\n",
    "        Creates and returns a clone of the current\n",
    "        DataFrame object (creating a deep copy of\n",
    "        all its components).\n",
    "        '''\n",
    "        dataframe_clone = copy.deepcopy(self)\n",
    "        dataframe_clone.column_labels = copy.deepcopy(self.column_labels)\n",
    "        dataframe_clone.data = copy.deepcopy(self.data)\n",
    "        return dataframe_clone\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        Default class' representation.\n",
    "        '''\n",
    "        return str(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Default class' string representation.\n",
    "        '''\n",
    "        max_columns = min(6, len(self.column_labels.keys()))\n",
    "        max_rows = min(8, len(self.data[0,:]))\n",
    "        max_string_size = 13\n",
    "        final_string = '| '\n",
    "        \n",
    "        # Add the schema to the top\n",
    "        for idx, label in enumerate(self.column_labels.keys()):\n",
    "            if (idx < max_columns):\n",
    "                label_rep = label if (len(label) <= max_string_size) else label[:max_string_size-3] + '...'\n",
    "                final_string += label_rep.rjust(max_string_size) + ' | '\n",
    "            elif (idx == max_columns):\n",
    "                final_string += '...'\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        final_string += '\\n'\n",
    "        final_string += '-' * (max_string_size * max_columns + (max_columns + 1) * 3)\n",
    "        \n",
    "        # Add the first rows as preview\n",
    "        for i in range(max_rows):\n",
    "            final_string += '\\n| '\n",
    "            for idx, value in enumerate(self.data[:,i]):\n",
    "                if (idx < max_columns):\n",
    "                    value_rep = str(value) if (len(str(value)) <= max_string_size) else str(value)[:max_string_size-3] + '...'\n",
    "                    final_string += value_rep.rjust(max_string_size) + ' | '\n",
    "                elif (idx == max_columns):\n",
    "                    final_string += '...'\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        final_string += '\\n(...)\\n'\n",
    "        \n",
    "        return final_string\n",
    "    \n",
    "class Column:\n",
    "    '''\n",
    "    This class is meant as single column's\n",
    "    data representation.\n",
    "    '''\n",
    "    label = None\n",
    "    values = None\n",
    "    \n",
    "    def __init__(self, label, values):\n",
    "        self.label = label\n",
    "        self.values = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = DataFrame(IN_TEST_DATA_PATH)\n",
    "train_df = DataFrame(IN_TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store 'Id' and 'Prediction' columns\n",
    "test_id_column, test_prediction_column = test_df.get_columns(['Id', 'Prediction'])\n",
    "trrain_id_column, train_prediction_column = train_df.get_columns(['Id', 'Prediction'])\n",
    "\n",
    "# Drop 'Id' and 'Prediction' columns and replace '-999' with 'NaN'\n",
    "test_df = test_df.drop(['Id', 'Prediction']).set_type(float).replace(-999.0, float('NaN'))\n",
    "train_df = train_df.drop(['Id', 'Prediction']).set_type(float).replace(-999.0, float('NaN'))\n",
    "\n",
    "# Create correlation matrices\n",
    "test_corr_df = test_df.replace(float('NaN'), 0.0).corr()\n",
    "train_corr_df = train_df.replace(float('NaN'), 0.0).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|  DER_mass_MMC | DER_mass_t... |  DER_mass_vis |      DER_pt_h | DER_deltae... | DER_mass_j... | ...\n",
       "---------------------------------------------------------------------------------------------------\n",
       "|           1.0 | -0.1715815... | 0.66487572... | 0.14314338... | 0.08945169... | 0.07617855... | ...\n",
       "| -0.1715815... |           1.0 | 0.19026079871 | -0.2523138... | -0.1836568... | -0.1667020825 | ...\n",
       "| 0.66487572... | 0.19026079871 |           1.0 | -0.0621338... | -0.0355031... | -0.0408823... | ...\n",
       "| 0.14314338... | -0.2523138... | -0.0621338... |           1.0 | 0.37758454... | 0.41496683... | ...\n",
       "| 0.08945169... | -0.1836568... | -0.0355031... | 0.37758454... |           1.0 | 0.87713594... | ...\n",
       "| 0.07617855... | -0.1667020825 | -0.0408823... | 0.41496683... | 0.87713594... |           1.0 | ...\n",
       "| -0.0317553... | 0.10011035... | 0.02149630... | -0.1126845... | -0.6810896... | -0.7081717... | ...\n",
       "| 0.45474738... | 0.03906426... | 0.57613211... | -0.5433957... | -0.2136287... | -0.2327591... | ...\n",
       "(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.replace(float('NaN'), 0.0).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "    1. Remove 1/3 of the columns (the ones with the most NaNs)\n",
    "    2. Fill in the data from the most correlated columns (above a certain coeficient)\n",
    "    3. Normalize the data (subtract the mean and divide by the stddev)\n",
    "    4. Fill in the rest with 0s (since that is the mean of the normalized data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
